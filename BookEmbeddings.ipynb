{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "books = pd.read_pickle('Pickle/books.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['combined_features'] = books.apply(\n",
    "    lambda row: f\"{row['title']} by {row['authors']}, \" +\n",
    "                f\"Description: {row['description']}, \" +\n",
    "                f\"Shelves: {row['expanded_shelves']}\",\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maddy\\anaconda3\\envs\\goodreads\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize your model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def save_embeddings_incrementally(books_df, model, interval=100):\n",
    "    embeddings_file = 'Pickle/embeddings.pkl'\n",
    "    \n",
    "    # Load existing embeddings if they exist\n",
    "    if os.path.exists(embeddings_file):\n",
    "        embeddings_df = pd.read_pickle(embeddings_file)\n",
    "    else:\n",
    "        embeddings_df = pd.DataFrame(columns=['index', 'book_id', 'embeddings'])\n",
    "    \n",
    "    # Ensure combined_features are non-null\n",
    "    books_df = books_df.dropna(subset=['combined_features']).reset_index(drop=True)\n",
    "\n",
    "    new_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(len(books_df)), desc=\"Generating embeddings\"):\n",
    "        if i in embeddings_df['index'].values:\n",
    "            continue  # Skip if already processed\n",
    "        \n",
    "        embedding = model.encode(books_df.at[i, 'combined_features'])\n",
    "        new_row = {'index': i, 'book_id': books_df.at[i, 'book_id'], 'embeddings': embedding}\n",
    "        new_embeddings.append(new_row)\n",
    "        \n",
    "        # Save periodically\n",
    "        if len(new_embeddings) % interval == 0:\n",
    "            new_embeddings_df = pd.DataFrame(new_embeddings)\n",
    "            embeddings_df = pd.concat([embeddings_df, new_embeddings_df], ignore_index=True)\n",
    "            embeddings_df.to_pickle(embeddings_file)\n",
    "            new_embeddings = []  # Reset the list\n",
    "    \n",
    "    # Save any remaining new embeddings\n",
    "    if new_embeddings:\n",
    "        new_embeddings_df = pd.DataFrame(new_embeddings)\n",
    "        embeddings_df = pd.concat([embeddings_df, new_embeddings_df], ignore_index=True)\n",
    "        embeddings_df.to_pickle(embeddings_file)\n",
    "\n",
    "    print(f\"Embeddings saved to {embeddings_file} successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 120421/120421 [2:56:20<00:00, 11.38it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to Pickle/embeddings.pkl successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120421/120421 [22:31<00:00, 89.12it/s] \n"
     ]
    }
   ],
   "source": [
    "save_embeddings_incrementally(books, model, interval=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.read_pickle('Pickle/embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df=embeddings_df.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging embeddings into the 'books' DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120421 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120421/120421 [00:01<00:00, 94554.65it/s] \n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "embeddings_df.set_index('book_id', inplace=True)\n",
    "\n",
    "def get_embedding(book_id):\n",
    "    try:\n",
    "        return embeddings_df.at[book_id, 'embeddings']\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "books['embeddings'] = books['book_id'].progress_apply(get_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.to_pickle('Pickle/books.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
