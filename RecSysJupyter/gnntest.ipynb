{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_data():\n",
    "    reviews = pd.read_pickle('../Pickle/reviews.pkl')\n",
    "    books = pd.read_pickle('../Pickle/books.pkl')\n",
    "    read = pd.read_pickle('../Pickle/read.pkl')\n",
    "    user_genres = pd.read_pickle('../Pickle/user_most_common_genres.pkl')\n",
    "    return reviews, books, read, user_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ID mappings for users and books\n",
    "def initialize_id_mappings(combined_data):\n",
    "    unique_user_ids = set(combined_data['user_id'])\n",
    "    unique_book_ids = set(combined_data['book_id'])\n",
    "\n",
    "    user_id_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "    book_id_to_index = {book_id: idx for idx, book_id in enumerate(unique_book_ids)}\n",
    "\n",
    "    return user_id_to_index, book_id_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_split_data(ratings_data, user_genres, test_size=0.2, random_state=42):\n",
    "    # Filter out books that have been rated fewer than twice\n",
    "    book_user_counts = ratings_data['book_id'].value_counts()\n",
    "    eligible_books = book_user_counts[book_user_counts > 5].index  \n",
    "    ratings_data = ratings_data[ratings_data['book_id'].isin(eligible_books)]\n",
    "\n",
    "    # Filter out users who don't have enough ratings\n",
    "    user_book_counts = ratings_data['user_id'].value_counts()\n",
    "    eligible_users = user_book_counts[user_book_counts > 5].index  \n",
    "    ratings_data = ratings_data[ratings_data['user_id'].isin(eligible_users)]    \n",
    "\n",
    "    # Ensure users are in both ratings_data and user_genres\n",
    "    eligible_users_in_genres = user_genres['user_id'].isin(eligible_users)\n",
    "    user_genres = user_genres[eligible_users_in_genres]\n",
    "    \n",
    "    # Merge the ratings_data and user_genres on user_id to get the most common genres for users\n",
    "    filtered_data = ratings_data.merge(user_genres[['user_id', 'most_common_genres']], on='user_id', how='inner')\n",
    "\n",
    "    # Now proceed with train-test split\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "\n",
    "    # Create a mask to check for users with enough rated books (>= 5)\n",
    "    user_data_valid = filtered_data.groupby('user_id').filter(lambda x: len(x) > 5)\n",
    "\n",
    "    # Split train-test for each user\n",
    "    for user_id, user_data in user_data_valid.groupby('user_id'):\n",
    "        books = user_data['book_id'].unique()  # All books rated by the user\n",
    "        \n",
    "        # Split the books into train and test sets\n",
    "        train_books, test_books = train_test_split(books, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        # Get the train and test data for the user\n",
    "        user_train_data = user_data[user_data['book_id'].isin(train_books)]\n",
    "        user_test_data = user_data[user_data['book_id'].isin(test_books)]\n",
    "        \n",
    "        # Append to train and test lists\n",
    "        train_dfs.append(user_train_data)\n",
    "        test_dfs.append(user_test_data)\n",
    "    \n",
    "    # Combine all the train and test data into single dataframes\n",
    "    train_data = pd.concat(train_dfs)\n",
    "    test_data = pd.concat(test_dfs)\n",
    "\n",
    "    return train_data, test_data, user_genres, filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize ratings\n",
    "def normalize_ratings(train_data, test_data):\n",
    "    mean_rating = train_data['rating'].mean()\n",
    "    std_rating = train_data['rating'].std()\n",
    "\n",
    "    train_data['rating'] = (train_data['rating'] - mean_rating) / std_rating\n",
    "    test_data['rating'] = (test_data['rating'] - mean_rating) / std_rating\n",
    "\n",
    "    return train_data, test_data, mean_rating, std_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare edge index, ratings, and edge attributes (review embeddings removed)\n",
    "def prepare_edge_index_ratings_attributes(df, user_id_to_index, book_id_to_index):\n",
    "    edge_index = []\n",
    "    ratings = []\n",
    "\n",
    "    # Get user and book indices as lists or numpy arrays\n",
    "    user_indices = df['user_id'].map(user_id_to_index).values\n",
    "    book_indices = df['book_id'].map(book_id_to_index).values\n",
    "\n",
    "    # Filter out invalid entries (those with no matching user or book)\n",
    "    valid_rows = df[(user_indices != -1) & (book_indices != -1)]  # Filter out invalid entries\n",
    "    user_indices = valid_rows['user_id'].map(user_id_to_index).values\n",
    "    book_indices = valid_rows['book_id'].map(book_id_to_index).values\n",
    "\n",
    "    # Create edge index and ratings tensors\n",
    "    edge_index = torch.stack([torch.tensor(user_indices), torch.tensor(book_indices)], dim=0)\n",
    "    ratings_tensor = torch.tensor(valid_rows['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    return edge_index, ratings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_user_and_book_features(filtered_data, user_id_to_index, book_id_to_index):\n",
    "    # Create a dictionary for all possible genres (book genres from filtered data)\n",
    "    unique_book_genres = sorted(set(filtered_data['filtered_genres'].str.split(',').explode()))\n",
    "    book_genre_dict = {genre: idx for idx, genre in enumerate(unique_book_genres)}\n",
    "\n",
    "    # Prepare user genre features\n",
    "    user_genre_features = {}\n",
    "    \n",
    "    # Group by user_id and process all genres at once\n",
    "    for user_id, group in filtered_data.groupby('user_id'):\n",
    "        genres = group['most_common_genres'].iloc[0]  # All rows for this user should have the same genres\n",
    "        genre_vector = np.zeros(len(book_genre_dict))  # Size based on unique book genres\n",
    "        for genre in genres:\n",
    "            if genre in book_genre_dict:\n",
    "                genre_vector[book_genre_dict[genre]] = 1\n",
    "        user_genre_features[user_id_to_index[user_id]] = torch.tensor(genre_vector, dtype=torch.float32)\n",
    "\n",
    "    # Prepare book genre features\n",
    "    book_genre_features = {}\n",
    "    for book_id, group in filtered_data.groupby('book_id'):\n",
    "        genres = group['filtered_genres'].iloc[0].split(',')  # Assuming all rows for this book have the same genres\n",
    "        genre_vector = np.zeros(len(book_genre_dict))  # Size based on unique book genres\n",
    "        for genre in genres:\n",
    "            if genre in book_genre_dict:\n",
    "                genre_vector[book_genre_dict[genre]] = 1\n",
    "        book_genre_features[book_id_to_index[book_id]] = torch.tensor(genre_vector, dtype=torch.float32)\n",
    "\n",
    "    return user_genre_features, book_genre_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_pca_on_features(user_genre_features, book_genre_features, n_components=20):\n",
    "    # Combine user and book features into one array for PCA\n",
    "    all_user_features = torch.stack(list(user_genre_features.values()))\n",
    "    all_book_features = torch.stack(list(book_genre_features.values()))\n",
    "\n",
    "    all_features = torch.cat([all_user_features, all_book_features], dim=0)  # Combine user and book features\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_features = pca.fit_transform(all_features)\n",
    "\n",
    "    # Split back the reduced features into user and book features\n",
    "    reduced_user_features = reduced_features[:len(user_genre_features)]\n",
    "    reduced_book_features = reduced_features[len(user_genre_features):]\n",
    "\n",
    "    # Update the user and book genre features dictionaries with the reduced features\n",
    "    updated_user_genre_features = {key: torch.tensor(val) for key, val in zip(user_genre_features.keys(), reduced_user_features)}\n",
    "    updated_book_genre_features = {key: torch.tensor(val) for key, val in zip(book_genre_features.keys(), reduced_book_features)}\n",
    "\n",
    "    return updated_user_genre_features, updated_book_genre_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data objects (train/test)\n",
    "def prepare_data_objects(train_data, test_data, user_genre_features, book_genre_features, user_id_to_index, book_id_to_index):\n",
    "    train_edge_index, train_ratings_tensor = prepare_edge_index_ratings_attributes(\n",
    "        train_data, user_id_to_index, book_id_to_index\n",
    "    )\n",
    "    test_edge_index, test_ratings_tensor = prepare_edge_index_ratings_attributes(\n",
    "        test_data, user_id_to_index, book_id_to_index\n",
    "    )\n",
    "\n",
    "    num_users = len(user_id_to_index)\n",
    "    num_books = len(book_id_to_index)\n",
    "\n",
    "    user_embeddings = torch.zeros(num_users, len(user_genre_features[0]))\n",
    "    book_embeddings = torch.zeros(num_books, len(book_genre_features[0]))\n",
    "\n",
    "    for user_idx, user_feature in user_genre_features.items():\n",
    "        user_embeddings[user_idx] = user_feature\n",
    "\n",
    "    for book_idx, book_feature in book_genre_features.items():\n",
    "        book_embeddings[book_idx] = book_feature\n",
    "\n",
    "    node_embeddings = torch.cat([user_embeddings, book_embeddings], dim=0)\n",
    "\n",
    "    train_data_obj = Data(\n",
    "        x=node_embeddings,  \n",
    "        edge_index=train_edge_index,\n",
    "        y=train_ratings_tensor\n",
    "    )\n",
    "    test_data_obj = Data(\n",
    "        x=node_embeddings,  \n",
    "        edge_index=test_edge_index,\n",
    "        y=test_ratings_tensor\n",
    "    )\n",
    "\n",
    "    return train_data_obj, test_data_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "reviews, books, read, user_genres = load_data()\n",
    "\n",
    "# Merge data with books information\n",
    "books = books[['book_id', 'title', 'authors', 'filtered_genres']]\n",
    "data = pd.merge(read, books, on='book_id')\n",
    "data = data[['user_id', 'book_id', 'rating', 'filtered_genres']]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Reset index for user_genres\n",
    "user_genres = user_genres.reset_index()\n",
    "user_genres = user_genres[user_genres['most_common_genres'].apply(lambda x: len(x) > 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, user_genres, filtered_data = filter_and_split_data(data, user_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_to_index, book_id_to_index = initialize_id_mappings(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the ratings\n",
    "train_data, test_data, mean_rating, std_rating = normalize_ratings(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare user and book genre features\n",
    "user_genre_features, book_genre_features = align_user_and_book_features(filtered_data, user_id_to_index, book_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA on the features\n",
    "user_genre_features, book_genre_features = apply_pca_on_features(user_genre_features, book_genre_features, n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data objects for train and test\n",
    "train_data_obj, test_data_obj = prepare_data_objects(\n",
    "    train_data, test_data, user_genre_features, book_genre_features, user_id_to_index, book_id_to_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5801050752 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Initialize and train the model\u001b[39;00m\n\u001b[0;32m     74\u001b[0m model \u001b[38;5;241m=\u001b[39m GATModel(in_channels, hidden_channels, out_channels\u001b[38;5;241m=\u001b[39mout_channels, num_heads\u001b[38;5;241m=\u001b[39mnum_heads)\n\u001b[1;32m---> 75\u001b[0m \u001b[43mtrain_gat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 51\u001b[0m, in \u001b[0;36mtrain_gat\u001b[1;34m(model, train_loader, test_loader, num_epochs, lr, log_freq)\u001b[0m\n\u001b[0;32m     49\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, batch\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Log the training loss at the specified frequency\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maddy\\anaconda3\\envs\\goodreads\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maddy\\anaconda3\\envs\\goodreads\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maddy\\anaconda3\\envs\\goodreads\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5801050752 bytes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels=1, num_heads=1):\n",
    "        super(GATModel, self).__init__()\n",
    "        # Define GAT layers\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels, heads=num_heads, concat=True)\n",
    "        self.gat2 = GATConv(hidden_channels * num_heads, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        # Apply GAT layers\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        # Return predictions for edges only\n",
    "        src, dst = edge_index  # Get node pairs for each edge\n",
    "        edge_predictions = (x[src] * x[dst]).sum(dim=-1)  # Inner product for edge regression\n",
    "        return edge_predictions\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Assuming train_data_obj and test_data_obj are instances of torch_geometric.data.Data\n",
    "train_data_list = [train_data_obj]  \n",
    "test_data_list = [test_data_obj]   \n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_data_list, batch_size=382, shuffle=True)  # Adjust batch_size as needed\n",
    "test_loader = DataLoader(test_data_list, batch_size=8, shuffle=False)\n",
    "\n",
    "def train_gat(model, train_loader, test_loader, num_epochs=20, lr=0.0001, log_freq=1):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Iterate over batches\n",
    "        for batch in train_loader:\n",
    "            # Move the batch to the correct device (CPU/GPU)\n",
    "            # Forward pass for the current batch\n",
    "            out = model(batch).squeeze()\n",
    "            # Calculate loss (MSE) for this batch\n",
    "            loss = criterion(out, batch.y.squeeze())\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Log the training loss at the specified frequency\n",
    "        if (epoch + 1) % log_freq == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    # Evaluate on the test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            test_out = model(batch).squeeze()\n",
    "            test_loss = criterion(test_out, batch.y.squeeze())\n",
    "        print(f'Test Loss: {test_loss.item()}')\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "in_channels = train_data_obj.x.shape[1]  # Number of input features per node\n",
    "hidden_channels = 14\n",
    "num_heads = 8 \n",
    "out_channels = 1\n",
    "\n",
    "# Initialize and train the model\n",
    "model = GATModel(in_channels, hidden_channels, out_channels=out_channels, num_heads=num_heads)\n",
    "train_gat(model, train_loader, test_loader, num_epochs=50, lr=0.0001, log_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(data):\n",
    "    assert data.edge_index.ndim == 2 and data.edge_index.shape[0] == 2, \"Invalid edge_index shape\"\n",
    "    assert data.x.ndim == 2, \"Node features should be a 2D tensor\"\n",
    "    assert data.x.shape[1] > 0, \"Node features are missing\"\n",
    "    print(f\"Data validation passed. Num nodes: {data.x.shape[0]}, Num features: {data.x.shape[1]}, Num edges: {data.edge_index.shape[1]}\")\n",
    "\n",
    "# Validate training data before training\n",
    "validate_data(train_data_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
