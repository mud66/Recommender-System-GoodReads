{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threadpoolctl\n",
    "threadpoolctl.threadpool_limits(1, \"blas\")\n",
    "from surprise import Reader, Dataset, KNNBasic\n",
    "from surprise import accuracy\n",
    "import pandas as pd\n",
    "from surprise import accuracy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from implicit.als import AlternatingLeastSquares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = pd.read_pickle('../Pickle/read.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = read.sample(frac = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = read[read['is_read']== 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = read.drop(columns=[\"rating\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = read.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.model_selection import GroupShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import implicit\n",
    "\n",
    "read = read.copy()  \n",
    "\n",
    "# Step 1: Apply GroupShuffleSplit based on 'user_id' to split the interactions\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(read, groups=read[\"user_id\"]))\n",
    "\n",
    "train_df = read.iloc[train_idx].copy()  # Copy to avoid SettingWithCopyWarning\n",
    "test_df = read.iloc[test_idx].copy()\n",
    "\n",
    "# Step 2: Assign categorical indices to users and books\n",
    "user_cat = pd.Categorical(read[\"user_id\"])\n",
    "book_cat = pd.Categorical(read[\"book_id\"])\n",
    "\n",
    "read[\"user_idx\"] = user_cat.codes\n",
    "read[\"book_idx\"] = book_cat.codes\n",
    "\n",
    "# Modify train and test DataFrames\n",
    "train_df.loc[:, \"user_idx\"] = pd.Categorical(train_df[\"user_id\"], categories=user_cat.categories).codes\n",
    "train_df.loc[:, \"book_idx\"] = pd.Categorical(train_df[\"book_id\"], categories=book_cat.categories).codes\n",
    "\n",
    "test_df.loc[:, \"user_idx\"] = pd.Categorical(test_df[\"user_id\"], categories=user_cat.categories).codes\n",
    "test_df.loc[:, \"book_idx\"] = pd.Categorical(test_df[\"book_id\"], categories=book_cat.categories).codes\n",
    "\n",
    "# Step 3: Create the train and test matrices (binary data: 1 for read, 0 for not read)\n",
    "train_matrix = sp.csr_matrix((train_df[\"is_read\"], (train_df[\"user_idx\"], train_df[\"book_idx\"])))\n",
    "test_matrix = sp.csr_matrix((test_df[\"is_read\"], (test_df[\"user_idx\"], test_df[\"book_idx\"])))\n",
    "\n",
    "# Ensure the test set is not empty\n",
    "if test_df.empty:\n",
    "    print(\"Warning: The test set is empty after splitting. Try adjusting your split parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    }
   ],
   "source": [
    "class ALSWrapper:\n",
    "    def __init__(self, factors=10, regularization=0.1, iterations=20):\n",
    "        self.factors = factors\n",
    "        self.regularization = regularization\n",
    "        self.iterations = iterations\n",
    "        self.model = AlternatingLeastSquares(factors=self.factors,\n",
    "                                             regularization=self.regularization,\n",
    "                                             iterations=self.iterations)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'factors': self.factors,\n",
    "            'regularization': self.regularization,\n",
    "            'iterations': self.iterations\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        self.model = AlternatingLeastSquares(factors=self.factors,\n",
    "                                             regularization=self.regularization,\n",
    "                                             iterations=self.iterations)\n",
    "        return self\n",
    "\n",
    "\n",
    "# Step 5: Define Precision@K scorer\n",
    "def precision_at_k_scorer(model, X, y, k=10):\n",
    "    precisions = []\n",
    "    for user_idx in range(X.shape[0]):\n",
    "        recommendations = model.predict(user_idx)\n",
    "        actual_books = set(y[user_idx].nonzero()[1])\n",
    "        recommended_books = set(recommendations)\n",
    "        precision = len(recommended_books & actual_books) / k\n",
    "        precisions.append(precision)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "# Step 6: Define parameter grid for ALS model\n",
    "param_grid = {\n",
    "    'factors': [10, 20, 50],             # Number of latent factors\n",
    "    'regularization': [0.01, 0.1, 1.0],  # Regularization strength\n",
    "    'iterations': [10, 20, 50],          # Number of iterations\n",
    "    'alpha': [1.0, 2.0]                  # Confidence in the observed interactions\n",
    "}\n",
    "\n",
    "# Step 7: Initialize ALS model and GridSearchCV\n",
    "als_model = ALSWrapper()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=als_model,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=make_scorer(precision_at_k_scorer, k=10),  # Custom Precision@K scorer\n",
    "                           cv=3,  # Cross-validation splits\n",
    "                           n_jobs=-1,  # Use all processors\n",
    "                           verbose=2)  # Show progress\n",
    "\n",
    "# Step 8: Fit GridSearchCV\n",
    "grid_search.fit(train_matrix, train_matrix)  # Using train_matrix for both X and y\n",
    "\n",
    "# Step 9: Output best hyperparameters and best score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Precision@10:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Matrix Summary:\")\n",
    "print(f\"Shape: {train_matrix.shape}\")\n",
    "print(f\"Non-zero values: {train_matrix.nnz}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
