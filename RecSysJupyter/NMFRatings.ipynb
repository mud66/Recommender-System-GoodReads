{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise import Reader, Dataset\n",
    "from tqdm import tqdm\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "from sklearn.utils import resample\n",
    "tqdm.pandas()\n",
    "from surprise import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and filter data\n",
    "interactions = pd.read_pickle('../Pickle/interactions.pkl')\n",
    "books = pd.read_pickle('../Pickle/books.pkl')\n",
    "interactions = interactions[['user_id', 'book_id', 'rating', 'is_read']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "4    178755\n",
       "5    143523\n",
       "3    111691\n",
       "0     29983\n",
       "2     29237\n",
       "1      8360\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions[interactions['rating'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter user IDs that appear more than 5 times\n",
    "# Filter book IDs that appear more than 5 times\n",
    "book_counts = interactions['book_id'].value_counts()\n",
    "interactions = interactions[interactions['book_id'].isin(book_counts[book_counts >= 5].index)]\n",
    "user_counts = interactions['user_id'].value_counts()\n",
    "interactions = interactions[interactions['user_id'].isin(user_counts[user_counts >= 5].index)]\n",
    "\n",
    "# Shuffle user interactions\n",
    "# Initialize empty lists for train and test splits\n",
    "train_list, test_list = [], []\n",
    "\n",
    "for user_id, user_data in interactions.groupby('user_id'):\n",
    "    # Shuffle the ratings for the user\n",
    "    user_data_shuffled = user_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split the data into train and test (80% for train, 20% for test)\n",
    "    train, test = train_test_split(user_data_shuffled, test_size=0.2, random_state=42, stratify=user_data_shuffled['user_id'])\n",
    "    \n",
    "    # Add the train and test data for this user to the overall list\n",
    "    train_list.append(train)\n",
    "    test_list.append(test)\n",
    "\n",
    "# Concatenate all train and test data\n",
    "train_df = pd.concat(train_list, ignore_index=True)\n",
    "test_df = pd.concat(test_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rating counts\n",
    "rating_counts = train_df['rating'].value_counts()\n",
    "\n",
    "# Find the majority class size (highest count)\n",
    "majority_count = rating_counts.max()\n",
    "\n",
    "# Dictionary to store modified dataframes\n",
    "modified_dfs = []\n",
    "\n",
    "# Perform upsampling with slight noise addition for each class\n",
    "for rating, count in rating_counts.items():\n",
    "    class_df = train_df[train_df['rating'] == rating]\n",
    "    \n",
    "    # Duplicate rows based on the majority class size, ensuring the size of the class is adjusted\n",
    "    num_duplicates = int(majority_count / count)  # Number of duplications required\n",
    "    \n",
    "    # Duplicate the class_df rows and add small random noise to the ratings\n",
    "    duplicated_df = pd.concat([class_df] * num_duplicates, ignore_index=True)\n",
    "    \n",
    "    # Add random noise to the ratings\n",
    "    duplicated_df['rating'] = duplicated_df['rating'] + np.random.uniform(-0.1, 0.1, size=duplicated_df.shape[0])\n",
    "    # Ensure ratings are within the valid range (e.g., between 1 and 5)\n",
    "    duplicated_df['rating'] = duplicated_df['rating'].clip(1, 5)\n",
    "    modified_dfs.append(duplicated_df)\n",
    "\n",
    "# Combine all modified data\n",
    "balanced_train = pd.concat(modified_dfs)\n",
    "\n",
    "# Shuffle dataset\n",
    "train_df = balanced_train.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(train_df['rating'].min(), train_df['rating'].max()))\n",
    "train_data = Dataset.load_from_df(train_df[['user_id', 'book_id', 'rating']], reader)\n",
    "test_data = Dataset.load_from_df(test_df[['user_id', 'book_id', 'rating']], reader)\n",
    "trainset = train_data.build_full_trainset()\n",
    "testset = test_data.construct_testset([(uid, iid, r, {}) for uid, iid, r in test_df[['user_id', 'book_id', 'rating']].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = { \n",
    "    'n_factors': [400,300],  \n",
    "    'n_epochs': [250, 150],  \n",
    "    'reg_pu': [0.1, 0.01],  # Regularization for user factors\n",
    "    'reg_qi': [0.1, 0.01],  # Regularization for item factors\n",
    "    'lr_bu': [0.01, 0.1],  # Learning rate for user bias\n",
    "    'lr_bi': [0.01, 0.1],  # Learning rate for item bias\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Define the grid search with GridSearchCV\n",
    "gs = GridSearchCV(NMF, param_grid, measures=['rmse'], cv=2)\n",
    "\n",
    "# Fit the grid search model\n",
    "gs.fit(train_data)\n",
    "\n",
    "# Get the best parameters and model\n",
    "best_params = gs.best_params['rmse']\n",
    "best_nmf = NMF(**best_params)\n",
    "best_nmf.fit(trainset)\n",
    "\n",
    "# Test the model\n",
    "predictions = best_nmf.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# joblib.dump(best_nmf, '../Pickle/best_nmf_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics: Precision, Recall, and nDCG at k\n",
    "def precision_recall_ndcg_at_k(predictions, k, threshold):\n",
    "    \"\"\"Return precision, recall, and nDCG at k metrics for each user.\"\"\"\n",
    "    \n",
    "    def dcg_at_k(scores, k):\n",
    "        return sum([rel / log2(idx + 2) for idx, rel in enumerate(scores[:k])])\n",
    "\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    ndcgs = dict()\n",
    "    \n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        \n",
    "        # Precision@K\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        \n",
    "        # Recall@K\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "        \n",
    "        # nDCG@K\n",
    "        actual = [true_r for (_, true_r) in user_ratings]\n",
    "        ideal = sorted(actual, reverse=True)\n",
    "        \n",
    "        idcg = dcg_at_k(ideal, k)\n",
    "        dcg = dcg_at_k([rel for (est, rel) in user_ratings], k)\n",
    "        ndcgs[uid] = dcg / idcg if idcg > 0 else 0\n",
    "    \n",
    "    precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "    recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "    ndcg = sum(ndcg for ndcg in ndcgs.values()) / len(ndcgs)\n",
    "    \n",
    "    return precision, recall, ndcg\n",
    "\n",
    "# Compute the evaluation metrics\n",
    "precision, recall, ndcg = precision_recall_ndcg_at_k(predictions, k=10, threshold=4)\n",
    "print(f'Adjusted Precision: {precision}, Adjusted Recall: {recall}, Adjusted nDCG: {ndcg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the adjusted predictions\n",
    "rating_values = [pred[3] for pred in predictions] \n",
    "\n",
    "# Plot the distribution of adjusted ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist([pred[3] for pred in predictions], bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Predictions')\n",
    "plt.xlabel('Adjusted Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from surprise import Reader, Dataset, NMF\n",
    "# from surprise.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from collections import defaultdict\n",
    "# from math import log2\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.utils import resample\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "\n",
    "# # Function to load and filter data\n",
    "# def load_and_filter_data():\n",
    "#     interactions = pd.read_pickle('../Pickle/interactions.pkl')\n",
    "#     books = pd.read_pickle('../Pickle/books.pkl')\n",
    "#     interactions = interactions[['user_id', 'book_id', 'rating', 'is_read']]\n",
    "    \n",
    "#     # Step 1: Compute the average rating per user\n",
    "#     user_avg_rating = interactions[interactions['rating'] > 0].groupby('user_id')['rating'].mean().round().astype(int)\n",
    "\n",
    "#     # Step 2: Round the book's average rating column\n",
    "#     books['average_rating'] = pd.to_numeric(books['average_rating'], errors='coerce').round().astype('Int64')\n",
    "#     book_avg = books[['average_rating', 'book_id']]\n",
    "#     interactions = pd.merge(interactions, book_avg, on='book_id')\n",
    "\n",
    "#     # Step 3: Impute missing ratings\n",
    "#     def impute_rating(row):\n",
    "#         if row['rating'] == 0:\n",
    "#             return user_avg_rating.get(row['user_id'], row['average_rating'])  # Use user avg or book avg\n",
    "#         return row['rating']\n",
    "    \n",
    "#     interactions['rating'] = interactions.apply(impute_rating, axis=1)\n",
    "    \n",
    "#     # Filter user IDs and book IDs with fewer than 5 occurrences\n",
    "#     book_counts = interactions['book_id'].value_counts()\n",
    "#     interactions = interactions[interactions['book_id'].isin(book_counts[book_counts >= 5].index)]\n",
    "    \n",
    "#     user_counts = interactions['user_id'].value_counts()\n",
    "#     interactions = interactions[interactions['user_id'].isin(user_counts[user_counts >= 5].index)]\n",
    "    \n",
    "#     return interactions\n",
    "\n",
    "# # Function to split data into train and test\n",
    "# def split_data(interactions):\n",
    "#     train_list, test_list = [], []\n",
    "#     for user_id, user_data in interactions.groupby('user_id'):\n",
    "#         user_data_shuffled = user_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "#         train, test = train_test_split(user_data_shuffled, test_size=0.2, random_state=42, stratify=user_data_shuffled['user_id'])\n",
    "#         train_list.append(train)\n",
    "#         test_list.append(test)\n",
    "    \n",
    "#     train_df = pd.concat(train_list, ignore_index=True)\n",
    "#     test_df = pd.concat(test_list, ignore_index=True)\n",
    "    \n",
    "#     return train_df, test_df\n",
    "\n",
    "# # Function to apply random noise to balance the dataset\n",
    "# def apply_random_noise_upsampling(train_df):\n",
    "#     rating_counts = train_df['rating'].value_counts()\n",
    "#     majority_count = rating_counts.max()\n",
    "    \n",
    "#     modified_dfs = []\n",
    "    \n",
    "#     for rating, count in rating_counts.items():\n",
    "#         class_df = train_df[train_df['rating'] == rating]\n",
    "#         num_duplicates = int(majority_count / count)\n",
    "#         duplicated_df = pd.concat([class_df] * num_duplicates, ignore_index=True)\n",
    "        \n",
    "#         # Add random noise to ratings\n",
    "#         duplicated_df['rating'] = duplicated_df['rating'] + np.random.uniform(-0.1, 0.1, size=duplicated_df.shape[0])\n",
    "#         duplicated_df['rating'] = duplicated_df['rating'].clip(1, 5)\n",
    "        \n",
    "#         modified_dfs.append(duplicated_df)\n",
    "    \n",
    "#     balanced_train = pd.concat(modified_dfs)\n",
    "#     train_df = balanced_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "#     return train_df\n",
    "\n",
    "# # Function to convert data to Surprise dataset\n",
    "# def convert_to_surprise(train_df, test_df):\n",
    "#     reader = Reader(rating_scale=(train_df['rating'].min(), train_df['rating'].max()))\n",
    "#     train_data = Dataset.load_from_df(train_df[['user_id', 'book_id', 'rating']], reader)\n",
    "#     test_data = Dataset.load_from_df(test_df[['user_id', 'book_id', 'rating']], reader)\n",
    "\n",
    "#     trainset = train_data.build_full_trainset()\n",
    "#     testset = test_data.construct_testset([(uid, iid, r, {}) for uid, iid, r in test_df[['user_id', 'book_id', 'rating']].values])\n",
    "    \n",
    "#     return train_data, test_data, trainset, testset\n",
    "\n",
    "# # Function to perform GridSearchCV and find the best parameters\n",
    "# def grid_search(train_data):\n",
    "#     param_grid = { \n",
    "#         'n_factors': [400, 300],  \n",
    "#         'n_epochs': [250, 150],  \n",
    "#         'reg_pu': [0.1, 0.01],\n",
    "#         'reg_qi': [0.1, 0.01],\n",
    "#         'lr_bu': [0.01, 0.1],\n",
    "#         'lr_bi': [0.01, 0.1],\n",
    "#         'random_state': [42]\n",
    "#     }\n",
    "\n",
    "#     gs = GridSearchCV(NMF, param_grid, measures=['rmse'], cv=2)\n",
    "#     gs.fit(train_data)\n",
    "    \n",
    "#     best_params = gs.best_params['rmse']\n",
    "#     best_nmf = NMF(**best_params)\n",
    "#     best_nmf.fit(train_data.build_full_trainset())\n",
    "    \n",
    "#     return best_nmf, gs.best_params\n",
    "\n",
    "# # Function to compute evaluation metrics\n",
    "# def precision_recall_ndcg_at_k(predictions, k, threshold):\n",
    "#     def dcg_at_k(scores, k):\n",
    "#         return sum([rel / log2(idx + 2) for idx, rel in enumerate(scores[:k])])\n",
    "\n",
    "#     user_est_true = defaultdict(list)\n",
    "#     for uid, _, true_r, est, _ in predictions:\n",
    "#         user_est_true[uid].append((est, true_r))\n",
    "\n",
    "#     precisions, recalls, ndcgs = dict(), dict(), dict()\n",
    "    \n",
    "#     for uid, user_ratings in user_est_true.items():\n",
    "#         user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "#         n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "#         n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "#         n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        \n",
    "#         # Precision@K\n",
    "#         precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        \n",
    "#         # Recall@K\n",
    "#         recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "        \n",
    "#         # nDCG@K\n",
    "#         actual = [true_r for (_, true_r) in user_ratings]\n",
    "#         ideal = sorted(actual, reverse=True)\n",
    "        \n",
    "#         idcg = dcg_at_k(ideal, k)\n",
    "#         dcg = dcg_at_k([rel for (est, rel) in user_ratings], k)\n",
    "#         ndcgs[uid] = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "#     precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "#     recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "#     ndcg = sum(ndcg for ndcg in ndcgs.values()) / len(ndcgs)\n",
    "    \n",
    "#     return precision, recall, ndcg\n",
    "\n",
    "# # Function to plot the results\n",
    "# def plot_results(predictions):\n",
    "#     rating_values = [pred[3] for pred in predictions]\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.hist(rating_values, bins=20, edgecolor='black', alpha=0.7)\n",
    "#     plt.title('Distribution of Predictions')\n",
    "#     plt.xlabel('Adjusted Rating')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.show()\n",
    "\n",
    "# # Main function to run the entire pipeline\n",
    "# def main():\n",
    "#     interactions = load_and_filter_data()\n",
    "#     train_df, test_df = split_data(interactions)\n",
    "#     train_df = apply_random_noise_upsampling(train_df)\n",
    "    \n",
    "#     train_data, test_data, trainset, testset = convert_to_surprise(train_df, test_df)\n",
    "#     best_nmf, best_params = grid_search(train_data)\n",
    "    \n",
    "#     # Test the model\n",
    "#     predictions = best_nmf.test(testset)\n",
    "    \n",
    "#     # Compute metrics\n",
    "#     precision, recall, ndcg = precision_recall_ndcg_at_k(predictions, k=10, threshold=3.5)\n",
    "#     print(f'Adjusted Precision: {precision}, Adjusted Recall: {recall}, Adjusted nDCG: {ndcg}')\n",
    "    \n",
    "#     # Plot the distribution of predictions\n",
    "#     plot_results(predictions)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
