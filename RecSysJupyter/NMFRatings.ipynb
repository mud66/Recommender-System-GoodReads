{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Precision: 0.8964500105172951, Adjusted Recall: 0.38449322037201994, Adjusted nDCG: 0.7555264203937998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise import Reader, Dataset, accuracy\n",
    "from tqdm import tqdm\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from surprise import NMF\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "tqdm.pandas()\n",
    "interactions = pd.read_pickle('Pickle/interactions.pkl')\n",
    "interactions = interactions[['user_id', 'book_id', 'rating', 'is_read']]  \n",
    "train_df, test_df = train_test_split(interactions, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate the global mean rating\n",
    "global_mean = train_df['rating'].mean()\n",
    "\n",
    "# Calculate user bias with regularization\n",
    "lambda_reg = 10\n",
    "user_sum_ratings = train_df.groupby('user_id')['rating'].sum()\n",
    "user_count_ratings = train_df.groupby('user_id')['rating'].count()\n",
    "user_bias = (user_sum_ratings - user_count_ratings * global_mean) / (user_count_ratings + lambda_reg)\n",
    "\n",
    "# Map user bias back to the original dataframe\n",
    "train_df['user_bias'] = train_df['user_id'].map(user_bias)\n",
    "\n",
    "# Calculate item bias with regularization\n",
    "item_sum_ratings = train_df.groupby('book_id')['rating'].sum()\n",
    "item_count_ratings = train_df.groupby('book_id')['rating'].count()\n",
    "item_bias = (item_sum_ratings - item_count_ratings * global_mean) / (item_count_ratings + lambda_reg)\n",
    "\n",
    "# Map item bias back to the original dataframe\n",
    "train_df['item_bias'] = train_df['book_id'].map(item_bias)\n",
    "\n",
    "# Normalize ratings\n",
    "train_df['normalised_rating'] = train_df['rating'] - train_df['user_bias'] - train_df['item_bias']\n",
    "\n",
    "# Convert standardized train_df to surprise dataset\n",
    "reader = Reader(rating_scale=(train_df['rating'].min(), train_df['rating'].max()))\n",
    "train_data = Dataset.load_from_df(train_df[['user_id', 'book_id', 'normalised_rating']], reader)\n",
    "\n",
    "# Convert test_df to surprise dataset without normalization\n",
    "test_data = Dataset.load_from_df(test_df[['user_id', 'book_id', 'rating']], reader)\n",
    "\n",
    "# Build full trainset and testset\n",
    "trainset = train_data.build_full_trainset()\n",
    "testset = test_data.construct_testset([(uid, iid, r, {}) for uid, iid, r in test_df[['user_id', 'book_id', 'rating']].values])\n",
    "\n",
    "# Define a parameter grid\n",
    "param_grid = { \n",
    "    'n_factors': [60], \n",
    "    'n_epochs': [50], \n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(NMF, param_grid, measures=['rmse'], cv=2)\n",
    "gs.fit(train_data)\n",
    "\n",
    "best_params = gs.best_params['rmse']\n",
    "best_params\n",
    "best_nmf = NMF(**best_params)\n",
    "best_nmf.fit(train_data.build_full_trainset())\n",
    "predictions = best_nmf.test(testset)\n",
    "\n",
    "# Reverse bias terms\n",
    "def reverse_bias_terms(uid, iid, est, user_bias, item_bias, global_mean):\n",
    "    user_b = user_bias.get(uid, 0)  # Default to 0 if the user/item is not in the training data\n",
    "    item_b = item_bias.get(iid, 0)\n",
    "    unbiased_prediction = est - user_b - item_b + global_mean\n",
    "    return unbiased_prediction\n",
    "\n",
    "# Rescale predictions by reversing bias terms\n",
    "def unbiased_predictions(predictions, user_bias, item_bias, global_mean):\n",
    "    adjusted_predictions = []\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        # Calculate the unbiased prediction\n",
    "        unbiased_prediction = reverse_bias_terms(uid, iid, est, user_bias, item_bias, global_mean)\n",
    "        # Clip the rating to the original scale (e.g., 1 to 5)\n",
    "        unbiased_prediction = min(5, max(1, unbiased_prediction))\n",
    "        adjusted_predictions.append((uid, iid, true_r, unbiased_prediction, _))\n",
    "    return adjusted_predictions\n",
    "\n",
    "# Rescale predictions\n",
    "adjusted_predictions = unbiased_predictions(predictions, user_bias.to_dict(), item_bias.to_dict(), train_df['rating'].mean())\n",
    "\n",
    "def precision_recall_ndcg_at_k(predictions, k, threshold):\n",
    "    \"\"\"Return precision, recall, and nDCG at k metrics for each user.\"\"\"\n",
    "    \n",
    "    # Helper function to calculate DCG and nDCG\n",
    "    def dcg_at_k(scores, k):\n",
    "        return sum([rel / log2(idx + 2) for idx, rel in enumerate(scores[:k])])\n",
    "\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    ndcgs = dict()\n",
    "    \n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        \n",
    "        # Precision@K\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        \n",
    "        # Recall@K\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "        \n",
    "        # nDCG@K\n",
    "        actual = [true_r for (_, true_r) in user_ratings]\n",
    "        ideal = sorted(actual, reverse=True)\n",
    "        \n",
    "        idcg = dcg_at_k(ideal, k)\n",
    "        dcg = dcg_at_k([rel for (est, rel) in user_ratings], k)\n",
    "        \n",
    "        ndcgs[uid] = dcg / idcg if idcg > 0 else 0\n",
    "    \n",
    "    precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "    recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "    ndcg = sum(ndcg for ndcg in ndcgs.values()) / len(ndcgs)\n",
    "    \n",
    "    return precision, recall, ndcg\n",
    "\n",
    "precision, recall, ndcg = precision_recall_ndcg_at_k(adjusted_predictions, k=10, threshold=2)\n",
    "print(f'Adjusted Precision: {precision}, Adjusted Recall: {recall}, Adjusted nDCG: {ndcg}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
