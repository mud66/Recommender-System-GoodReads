{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise import Reader, Dataset, accuracy\n",
    "from tqdm import tqdm\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "from sklearn.utils import resample\n",
    "tqdm.pandas()\n",
    "import surprise\n",
    "from surprise import NMF\n",
    "\n",
    "\n",
    "# Load and filter data\n",
    "interactions = pd.read_pickle('../Pickle/interactions.pkl')\n",
    "interactions = interactions[['user_id', 'book_id', 'rating', 'is_read']]  \n",
    "\n",
    "# Filter user IDs that appear more than 5 times\n",
    "# Filter book IDs that appear more than 5 times\n",
    "book_counts = interactions['book_id'].value_counts()\n",
    "interactions = interactions[interactions['book_id'].isin(book_counts[book_counts >= 5].index)]\n",
    "user_counts = interactions['user_id'].value_counts()\n",
    "interactions = interactions[interactions['user_id'].isin(user_counts[user_counts >= 5].index)]\n",
    "\n",
    "# Shuffle user interactions\n",
    "# Initialize empty lists for train and test splits\n",
    "train_list, test_list = [], []\n",
    "\n",
    "for user_id, user_data in interactions.groupby('user_id'):\n",
    "    # Shuffle the ratings for the user\n",
    "    user_data_shuffled = user_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split the data into train and test (80% for train, 20% for test)\n",
    "    train, test = train_test_split(user_data_shuffled, test_size=0.2, random_state=42, stratify=user_data_shuffled['user_id'])\n",
    "    \n",
    "    # Add the train and test data for this user to the overall list\n",
    "    train_list.append(train)\n",
    "    test_list.append(test)\n",
    "\n",
    "# Concatenate all train and test data\n",
    "train_df = pd.concat(train_list, ignore_index=True)\n",
    "test_df = pd.concat(test_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "4    78926\n",
       "5    69973\n",
       "3    46464\n",
       "2    12181\n",
       "0    10234\n",
       "1     3680\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rating counts\n",
    "rating_counts = train_df['rating'].value_counts()\n",
    "\n",
    "# Find the majority class size (highest count)\n",
    "majority_count = rating_counts.max()\n",
    "\n",
    "# Dictionary to store oversampled dataframes\n",
    "oversampled_dfs = []\n",
    "\n",
    "# Perform upsampling for each class to match a proportion of the majority count\n",
    "for rating, count in rating_counts.items():\n",
    "    class_df = train_df[train_df['rating'] == rating]\n",
    "    \n",
    "    # Upsample based on a percentage of the majority class size\n",
    "    if count < majority_count:\n",
    "        upsampled_df = resample(class_df, replace=True, n_samples=int(majority_count * 0.4  ), random_state=42)  # 75% of the majority class\n",
    "    else:\n",
    "        upsampled_df = class_df  # Keep majority class as is\n",
    "    \n",
    "    oversampled_dfs.append(upsampled_df)\n",
    "\n",
    "# Combine all upsampled data\n",
    "balanced_train = pd.concat(oversampled_dfs)\n",
    "\n",
    "# Shuffle dataset\n",
    "train_df = balanced_train.sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_reg = 0.1\n",
    "global_mean = train_df['rating'].mean()\n",
    "# Calculate user bias with regularization\n",
    "user_sum_ratings = train_df.groupby('user_id')['rating'].sum()\n",
    "user_count_ratings = train_df.groupby('user_id')['rating'].count()\n",
    "\n",
    "# Recalculate user bias with the new lambda_reg value\n",
    "user_bias = (user_sum_ratings - user_count_ratings * global_mean) / (user_count_ratings + lambda_reg)\n",
    "\n",
    "# Map user bias back to the original dataframe\n",
    "train_df['user_bias'] = train_df['user_id'].map(user_bias)\n",
    "\n",
    "# Calculate item bias with regularization\n",
    "item_sum_ratings = train_df.groupby('book_id')['rating'].sum()\n",
    "item_count_ratings = train_df.groupby('book_id')['rating'].count()\n",
    "item_bias = (item_sum_ratings - item_count_ratings * global_mean) / (item_count_ratings + lambda_reg)\n",
    "\n",
    "# Map item bias back to the original dataframe\n",
    "train_df['item_bias'] = train_df['book_id'].map(item_bias)\n",
    "\n",
    "# Calculate the global mean\n",
    "global_mean = train_df['rating'].mean()\n",
    "\n",
    "# Normalize ratings by Z-score per user and item\n",
    "train_df['user_mean'] = train_df.groupby('user_id')['rating'].transform('mean')\n",
    "train_df['user_std'] = train_df.groupby('user_id')['rating'].transform('std')\n",
    "\n",
    "train_df['item_mean'] = train_df.groupby('book_id')['rating'].transform('mean')\n",
    "train_df['item_std'] = train_df.groupby('book_id')['rating'].transform('std')\n",
    "\n",
    "# Normalize ratings using Z-score for user and item\n",
    "train_df['normalised_rating'] = (train_df['rating'] - train_df['user_mean']) / (train_df['user_std'] + 1e-5)  # avoid divide by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "4    78926\n",
       "1    31570\n",
       "3    31570\n",
       "0    31570\n",
       "2    31570\n",
       "5    31570\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert standardized train_df to surprise dataset\n",
    "reader = Reader(rating_scale=(train_df['normalised_rating'].min(), train_df['normalised_rating'].max()))\n",
    "train_data = Dataset.load_from_df(train_df[['user_id', 'book_id', 'normalised_rating']], reader)\n",
    "\n",
    "# Convert test_df to surprise dataset without normalization\n",
    "test_data = Dataset.load_from_df(test_df[['user_id', 'book_id', 'rating']], reader)\n",
    "\n",
    "# Build full trainset and testset\n",
    "trainset = train_data.build_full_trainset()\n",
    "testset = test_data.construct_testset([(uid, iid, r, {}) for uid, iid, r in test_df[['user_id', 'book_id', 'rating']].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'lr_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m { \n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_factors\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m190\u001b[39m, \u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m100\u001b[39m],  \u001b[38;5;66;03m# Expanded n_factors range\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m40\u001b[39m],  \u001b[38;5;66;03m# Expanded n_epochs range\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m42\u001b[39m]\n\u001b[0;32m     11\u001b[0m }\n\u001b[0;32m     14\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(NMF, param_grid, measures\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m], cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m best_params \u001b[38;5;241m=\u001b[39m gs\u001b[38;5;241m.\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     18\u001b[0m best_nmf \u001b[38;5;241m=\u001b[39m NMF(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params)\n",
      "File \u001b[1;32mc:\\Users\\maddy\\anaconda3\\envs\\goodreads\\Lib\\site-packages\\surprise\\model_selection\\search.py:104\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     90\u001b[0m cv \u001b[38;5;241m=\u001b[39m get_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv)\n\u001b[0;32m     92\u001b[0m delayed_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     93\u001b[0m     delayed(fit_and_score)(\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m     )\n\u001b[0;32m    103\u001b[0m )\n\u001b[1;32m--> 104\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoblib_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m (test_measures_dicts, train_measures_dicts, fit_times, test_times) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mout)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# test_measures_dicts is a list of dict like this:\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# [{'mae': 1, 'rmse': 2}, {'mae': 2, 'rmse': 3} ...]\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# E.g. for 5 splits, the first 5 dicts are for the first param\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# (n_parameters_combinations, n_splits). This way we can easily compute\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# the mean and std dev over all splits or over all param comb.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maddy\\anaconda3\\envs\\goodreads\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\maddy\\anaconda3\\envs\\goodreads\\Lib\\site-packages\\joblib\\parallel.py:1844\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;66;03m# Sequentially call the tasks and yield the results.\u001b[39;00m\n\u001b[1;32m-> 1844\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_dispatched_batches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   1846\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_dispatched_tasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32mc:\\Users\\maddy\\anaconda3\\envs\\goodreads\\Lib\\site-packages\\surprise\\model_selection\\search.py:94\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefit cannot be used when data has been \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded with load_from_folds().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m     )\n\u001b[0;32m     90\u001b[0m cv \u001b[38;5;241m=\u001b[39m get_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv)\n\u001b[0;32m     92\u001b[0m delayed_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     93\u001b[0m     delayed(fit_and_score)(\n\u001b[1;32m---> 94\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     95\u001b[0m         trainset,\n\u001b[0;32m     96\u001b[0m         testset,\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeasures,\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_train_measures,\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m params, (trainset, testset) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_combinations, cv\u001b[38;5;241m.\u001b[39msplit(data)\n\u001b[0;32m    102\u001b[0m     )\n\u001b[0;32m    103\u001b[0m )\n\u001b[0;32m    104\u001b[0m out \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    105\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    106\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_dispatch,\n\u001b[0;32m    107\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoblib_verbose,\n\u001b[0;32m    108\u001b[0m )(delayed_list)\n\u001b[0;32m    110\u001b[0m (test_measures_dicts, train_measures_dicts, fit_times, test_times) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mout)\n",
      "File \u001b[1;32mc:\\Users\\maddy\\anaconda3\\envs\\goodreads\\Lib\\site-packages\\surprise\\prediction_algorithms\\matrix_factorization.pyx:632\u001b[0m, in \u001b[0;36msurprise.prediction_algorithms.matrix_factorization.NMF.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'lr_all'"
     ]
    }
   ],
   "source": [
    "param_grid = { \n",
    "    'n_factors': [190, 150, 100],  # Expanded n_factors range\n",
    "    'n_epochs': [80, 60, 40],  # Expanded n_epochs range\n",
    "    'reg_pu': [0.01, 0.05, 0.1],  # Regularization for user factors\n",
    "    'reg_qi': [0.01, 0.05, 0.1],  # Regularization for item factors\n",
    "    'lr_bu': [0.002, 0.005, 0.01],  # Learning rate for user bias\n",
    "    'lr_bi': [0.002, 0.005, 0.01],  # Learning rate for item bias\n",
    "    'lr_all': [0.002, 0.005, 0.01],  # Learning rate for all parameters\n",
    "    'bias_type': ['bias', 'none'],  # Bias type (with or without biases)\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(NMF, param_grid, measures=['rmse'], cv=4)\n",
    "gs.fit(train_data)\n",
    "\n",
    "best_params = gs.best_params['rmse']\n",
    "best_nmf = NMF(**best_params)\n",
    "best_nmf.fit(train_data.build_full_trainset())\n",
    "predictions = best_nmf.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_bias_terms(uid, iid, est, user_bias, item_bias, global_mean, bias_scale=0.05):\n",
    "    # Retrieve user and item bias with the updated user/item bias calculations\n",
    "    user_b = user_bias.get(uid, 0)  # Default to 0 if the user/item is not in the training data\n",
    "    item_b = item_bias.get(iid, 0)\n",
    "    \n",
    "    # Recalculate the unbiased prediction\n",
    "    unbiased_prediction = est - user_b - item_b + global_mean\n",
    "\n",
    "    # Scale the unbiased prediction to reduce the effect of bias\n",
    "    unbiased_prediction = unbiased_prediction * bias_scale + global_mean  # Scale towards global mean\n",
    "    \n",
    "    return unbiased_prediction\n",
    "\n",
    "# Rescale predictions by reversing bias terms\n",
    "def unbiased_predictions(predictions, user_bias, item_bias, global_mean):\n",
    "    adjusted_predictions = []\n",
    "    \n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        # Calculate the unbiased prediction using the reverse_bias_terms function\n",
    "        unbiased_prediction = reverse_bias_terms(uid, iid, est, user_bias, item_bias, global_mean)\n",
    "        \n",
    "        # Clip the rating to the original scale (e.g., 1 to 5) to avoid out-of-bound values\n",
    "        unbiased_prediction = min(5, max(1, unbiased_prediction))\n",
    "        \n",
    "        # Append the adjusted predictions\n",
    "        adjusted_predictions.append((uid, iid, true_r, unbiased_prediction, _))\n",
    "    \n",
    "    return adjusted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale predictions\n",
    "adjusted_predictions = unbiased_predictions(predictions, user_bias, item_bias, global_mean)\n",
    "\n",
    "# Evaluation metrics: Precision, Recall, and nDCG at k\n",
    "def precision_recall_ndcg_at_k(predictions, k, threshold):\n",
    "    \"\"\"Return precision, recall, and nDCG at k metrics for each user.\"\"\"\n",
    "    \n",
    "    # Helper function to calculate DCG and nDCG\n",
    "    def dcg_at_k(scores, k):\n",
    "        return sum([rel / log2(idx + 2) for idx, rel in enumerate(scores[:k])])\n",
    "\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    ndcgs = dict()\n",
    "    \n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        \n",
    "        # Precision@K\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        \n",
    "        # Recall@K\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "        \n",
    "        # nDCG@K\n",
    "        actual = [true_r for (_, true_r) in user_ratings]\n",
    "        ideal = sorted(actual, reverse=True)\n",
    "        \n",
    "        idcg = dcg_at_k(ideal, k)\n",
    "        dcg = dcg_at_k([rel for (est, rel) in user_ratings], k)\n",
    "        \n",
    "        ndcgs[uid] = dcg / idcg if idcg > 0 else 0\n",
    "    \n",
    "    precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "    recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "    ndcg = sum(ndcg for ndcg in ndcgs.values()) / len(ndcgs)\n",
    "    \n",
    "    return precision, recall, ndcg\n",
    "\n",
    "# Compute the evaluation metrics\n",
    "precision, recall, ndcg = precision_recall_ndcg_at_k(adjusted_predictions, k=10, threshold=3.5)\n",
    "print(f'Adjusted Precision: {precision}, Adjusted Recall: {recall}, Adjusted nDCG: {ndcg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the adjusted predictions\n",
    "adjusted_rating_values = [pred[3] for pred in adjusted_predictions] \n",
    "\n",
    "# Plot the distribution of adjusted ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(adjusted_rating_values, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Adjusted Predictions')\n",
    "plt.xlabel('Adjusted Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some of the predictions before and after bias adjustment\n",
    "for uid, iid, true_r, est, _ in predictions[:10]:\n",
    "    print(f\"User: {uid}, Book: {iid}, True Rating: {true_r}, Predicted: {est}\")\n",
    "\n",
    "\n",
    "for uid, iid, true_r, adjusted_est, _ in adjusted_predictions[:10]:\n",
    "    print(f\"User: {uid}, Book: {iid}, True Rating: {true_r}, Adjusted Prediction: {adjusted_est}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample user biases:\", {uid: user_bias.get(uid, 0) for uid in list(train_df['user_id'].head())})\n",
    "print(\"Sample item biases:\", {iid: item_bias.get(iid, 0) for iid in list(train_df['book_id'].head())})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid, iid, true_r, est, _ in predictions[:10]:\n",
    "    print(f\"Raw Prediction (before bias): {est} for User: {uid}, Book: {iid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
