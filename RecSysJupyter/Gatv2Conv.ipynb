{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_random_seed(seed_value):\n",
    "    \"\"\"Set the random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)  # Set the seed for Python's random module\n",
    "    np.random.seed(seed_value)  # Set the seed for NumPy\n",
    "    torch.manual_seed(seed_value)  # Set the seed for PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed_value)  # Set the seed for PyTorch\n",
    "    torch.cuda.manual_seed_all(seed_value)  # Set the seed for all GPUs\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior on GPU\n",
    "    torch.backends.cudnn.benchmark = False  # Turn off benchmarks for reproducibility\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_data():\n",
    "    reviews = pd.read_pickle('../Pickle/reviews.pkl')\n",
    "    books_list = []\n",
    "    with open('../Pickle/books.pkl', 'rb') as file:\n",
    "        while True:\n",
    "            try:\n",
    "                books_list.append(pickle.load(file))\n",
    "            except EOFError:\n",
    "                break\n",
    "    books = pd.concat(books_list, ignore_index=True).drop_duplicates(subset='title', keep='first')\n",
    "\n",
    "    read = pd.read_pickle('../Pickle/read.pkl')\n",
    "    user_genres = pd.read_pickle('../Pickle/user_most_common_genres.pkl')\n",
    "    review_embeddings = pd.read_pickle('../Pickle/review_embeddings.pkl')\n",
    "    review_sentiment = pd.read_pickle('../Pickle/review_score.pkl')\n",
    "    imputed = pd.read_pickle('../Pickle/imputed_ratings.pkl')\n",
    "    return reviews, books, read, user_genres, review_embeddings, review_sentiment, imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ratings_data, user_genres, test_size=0.1, val_size=0.15, random_state=42):\n",
    "\n",
    "    \"\"\"\n",
    "    Splits the ratings data into training, validation, and test datasets while handling users and books\n",
    "    that have limited interactions. It first removes interactions involving users or books that appear \n",
    "    only once in the dataset and then performs the data split.\n",
    "\n",
    "    Args:\n",
    "    ratings_data (DataFrame): A dataframe containing the ratings data with columns 'user_id' and 'book_id'.\n",
    "    user_genres (DataFrame): A dataframe containing 'user_id' and the most common genres for each user.\n",
    "    test_size (float, optional): The proportion of data to be used as the test set. Default is 0.1 (10%).\n",
    "    val_size (float, optional): The proportion of data to be used as the validation set. Default is 0.15 (15%).\n",
    "    random_state (int, optional): The seed for the random number generator to ensure reproducibility. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing three DataFrames:\n",
    "        - train_data (DataFrame): The training dataset, including interactions where users or books appear only once.\n",
    "        - val_data (DataFrame): The validation dataset.\n",
    "        - test_data (DataFrame): The test dataset.\n",
    "\n",
    "    Notes:\n",
    "    - The function removes interactions where a user or book appears only once in the dataset to prevent data sparsity issues.\n",
    "    - The remaining interactions are split into train, validation, and test sets, with the validation and test sizes being proportionally adjusted.\n",
    "    - The single interactions (those involving users or books that appear only once) are added back to the training set.\n",
    "    \"\"\"\n",
    "    merged_data = ratings_data.merge(user_genres[['user_id', 'most_common_genres']], on='user_id', how='inner')\n",
    "    user_counts = merged_data['user_id'].value_counts()\n",
    "    book_counts = merged_data['book_id'].value_counts()\n",
    "    single_interactions = merged_data[\n",
    "        merged_data['user_id'].isin(user_counts[user_counts == 1].index) | \n",
    "        merged_data['book_id'].isin(book_counts[book_counts == 1].index)\n",
    "    ]\n",
    "\n",
    "    remaining_interactions = merged_data[~merged_data.index.isin(single_interactions.index)]\n",
    "    train_df, temp_data = train_test_split(remaining_interactions, test_size=test_size+val_size, random_state=random_state)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=test_size/(test_size+val_size), random_state=random_state)\n",
    "\n",
    "    train_data = pd.concat([train_df, single_interactions], ignore_index=True)\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_id_mappings(combined_data):\n",
    "    \"\"\"\n",
    "    Initializes mappings between user IDs and book IDs to integer indices.\n",
    "\n",
    "    Args:\n",
    "    combined_data (DataFrame): A dataframe containing user-item interactions, with columns 'user_id' and 'book_id'.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two dictionaries:\n",
    "        - user_id_to_index (dict): A mapping of unique user IDs to integer indices.\n",
    "        - book_id_to_index (dict): A mapping of unique book IDs to integer indices.\n",
    "\n",
    "   \"\"\"\n",
    "    unique_user_ids = set(combined_data['user_id'])\n",
    "    unique_book_ids = set(combined_data['book_id'])\n",
    "    user_id_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "    book_id_to_index = {book_id: idx for idx, book_id in enumerate(unique_book_ids)}\n",
    "    return user_id_to_index, book_id_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_ratings(\n",
    "    train_df,\n",
    "    rating_col='rating',\n",
    "    balance_ratio=0.75,\n",
    "    noise_range=(-0.1, 0.1),\n",
    "    rating_min=1,\n",
    "    rating_max=5,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Balance the rating distribution by upsampling and adding small noise.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Training dataframe containing a rating column.\n",
    "        rating_col (str): The name of the rating column in the dataframe.\n",
    "        balance_ratio (float): Target ratio relative to the majority class count.\n",
    "        noise_range (tuple): Range (min, max) of uniform noise to add to ratings during upsampling.\n",
    "        rating_min (float): Minimum allowable rating after adding noise.\n",
    "        rating_max (float): Maximum allowable rating after adding noise.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A balanced and shuffled dataframe.\n",
    "    \"\"\"\n",
    "    rating_counts = train_df[rating_col].value_counts()\n",
    "    majority_count = rating_counts.max()\n",
    "    max_target_size = int(balance_ratio * majority_count)\n",
    "\n",
    "    modified_dfs = []\n",
    "    for rating, count in rating_counts.items():\n",
    "        class_df = train_df[train_df[rating_col] == rating]\n",
    "\n",
    "        if count >= max_target_size:\n",
    "            balanced_df = class_df.copy()\n",
    "        else:\n",
    "            balanced_df = resample(\n",
    "                class_df,\n",
    "                replace=True,\n",
    "                n_samples=max_target_size,\n",
    "                random_state=random_state\n",
    "            )\n",
    "\n",
    "            noise = np.random.uniform(noise_range[0], noise_range[1], size=balanced_df.shape[0])\n",
    "            balanced_df[rating_col] = balanced_df[rating_col] + noise\n",
    "\n",
    "            balanced_df[rating_col] = balanced_df[rating_col].clip(rating_min, rating_max)\n",
    "\n",
    "        modified_dfs.append(balanced_df)\n",
    "\n",
    "    balanced_train = pd.concat(modified_dfs, ignore_index=True)\n",
    "\n",
    "    balanced_train = balanced_train.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return balanced_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_ratings(train_data, test_data, val_data):\n",
    "    \"\"\"\n",
    "    Normalizes the ratings in the train, test, and validation datasets using a log scale transformation.\n",
    "\n",
    "    This function applies a log1p transformation (log(x + 1)) to the ratings to reduce the skewness of the rating distribution.\n",
    "\n",
    "    Args:\n",
    "    train_data (DataFrame): Training dataset containing ratings.\n",
    "    test_data (DataFrame): Testing dataset containing ratings.\n",
    "    val_data (DataFrame): Validation dataset containing ratings.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - The normalized training dataset (with log-scaled ratings).\n",
    "        - The normalized testing dataset (with log-scaled ratings).\n",
    "        - The normalized validation dataset (with log-scaled ratings).\n",
    "        - The minimum rating value in the training dataset (before normalization).\n",
    "    \"\"\"\n",
    "    min_rating = train_data['rating'].min()\n",
    "    train_data['rating'] = np.log1p(train_data['rating'])\n",
    "    test_data['rating'] = np.log1p(test_data['rating'])\n",
    "    val_data['rating'] = np.log1p(val_data['rating'])\n",
    "    \n",
    "    return train_data, test_data, val_data, min_rating\n",
    "\n",
    "\n",
    "def denormalize_rating(log_scaled_ratings, min_rating):\n",
    "    \"\"\"\n",
    "    Denormalizes the log-scaled ratings back to their original scale.\n",
    "\n",
    "    This function reverses the log1p transformation applied during normalization and adjusts for the minimum rating.\n",
    "    The resulting ratings are clipped to a range between 0 and 5.\n",
    "\n",
    "    Args:\n",
    "    log_scaled_ratings (list or array-like): Log-transformed ratings (using log1p).\n",
    "    min_rating (float): The minimum rating value before normalization.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The denormalized ratings, clipped between 0 and 5.\n",
    "\n",
    "    \"\"\"\n",
    "    log_scaled_ratings = np.asarray(log_scaled_ratings, dtype=float)  # Ensure NumPy array\n",
    "    # Reverse log1p transformation\n",
    "    original_ratings = np.expm1(log_scaled_ratings)\n",
    "    # Adjust for minimum rating\n",
    "    if min_rating:\n",
    "        original_ratings += min_rating\n",
    "    # Clip values between 0 and 5\n",
    "    return np.clip(original_ratings, 0, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_book_features(filtered_data, user_id_to_index, book_id_to_index):\n",
    "    \"\"\"\n",
    "    Generates user and book genre features based on the most common genres for users and the filtered genres for books.\n",
    "\n",
    "    This function creates feature vectors representing the genres for both users and books. For users, it combines their \n",
    "    most common genres into a feature vector. For books, it combines the filtered genres into a feature vector. These vectors \n",
    "    are represented as binary vectors where each genre corresponds to a dimension, and a 1 indicates the presence of the genre.\n",
    "\n",
    "    Args:\n",
    "    filtered_data (DataFrame): DataFrame containing user-book interactions, including columns 'user_id', 'book_id', \n",
    "                                'filtered_genres', and 'most_common_genres'.\n",
    "    user_id_to_index (dict): A dictionary mapping user IDs to internal indices.\n",
    "    book_id_to_index (dict): A dictionary mapping book IDs to internal indices.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - user_genre_features (dict): A dictionary where the keys are user indices and the values are torch tensors\n",
    "                                      representing the binary genre vectors for each user.\n",
    "        - book_genre_features (dict): A dictionary where the keys are book indices and the values are torch tensors\n",
    "                                      representing the binary genre vectors for each book.\n",
    "\n",
    "    \"\"\"\n",
    "    # Combine user and book genres to create a unified genre list\n",
    "    unique_genres = sorted(set(filtered_data['filtered_genres'].str.split(',').explode()).union(\n",
    "        set(filtered_data['most_common_genres'].explode())\n",
    "    ))\n",
    "\n",
    "    # Create a common genre dictionary\n",
    "    genre_dict = {genre: idx for idx, genre in enumerate(unique_genres)}\n",
    "    \n",
    "    # Prepare user genre features\n",
    "    user_genre_features = {}\n",
    "    # Group by user_id and process all genres at once\n",
    "    for user_id, group in filtered_data.groupby('user_id'):\n",
    "        genres = group['most_common_genres'].iloc[0]  # All rows for this user should have the same genres\n",
    "        genre_vector = np.zeros(len(genre_dict))  # Size based on the unified genre list\n",
    "        for genre in genres:\n",
    "            if genre in genre_dict:\n",
    "                genre_vector[genre_dict[genre]] = 1\n",
    "        user_genre_features[user_id_to_index[user_id]] = torch.tensor(genre_vector, dtype=torch.float32)\n",
    "\n",
    "    # Prepare book genre features\n",
    "    book_genre_features = {}\n",
    "    for book_id, group in filtered_data.groupby('book_id'):\n",
    "        genres = group['filtered_genres'].iloc[0].split(',')  # Assuming all rows for this book have the same genres\n",
    "        genre_vector = np.zeros(len(genre_dict))  # Size based on the unified genre list\n",
    "        for genre in genres:\n",
    "            if genre in genre_dict:\n",
    "                genre_vector[genre_dict[genre]] = 1\n",
    "        book_genre_features[book_id_to_index[book_id]] = torch.tensor(genre_vector, dtype=torch.float32)\n",
    "\n",
    "    return user_genre_features, book_genre_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_edge_index_ratings_attributes(df, user_id_to_index, book_id_to_index):\n",
    "    \"\"\"\n",
    "    Prepares the edge index and associated attributes for a graph-based recommendation system, where the nodes represent \n",
    "    users and books, and the edges represent interactions between them with ratings, confidence scores, and embeddings.\n",
    "\n",
    "    This function processes the user-book interactions, maps user and book IDs to their respective indices, and generates\n",
    "    the edge index as well as the edge attributes, which include ratings, confidence scores, and embeddings.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): DataFrame containing the user-book interactions with columns 'user_id', 'book_id', 'rating', \n",
    "                    'confidence_score', and 'embeddings'.\n",
    "    user_id_to_index (dict): A dictionary mapping user IDs to internal user indices.\n",
    "    book_id_to_index (dict): A dictionary mapping book IDs to internal book indices.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - edge_index (torch.Tensor): A tensor of shape (2, num_edges) representing the user-book pairs as edges in the graph.\n",
    "        - edge_attr (torch.Tensor): A tensor of shape (num_edges, 3 + embedding_size) representing the edge attributes,\n",
    "                                   concatenating ratings, confidence scores, and embeddings.\n",
    "\n",
    "    \"\"\"\n",
    "    # Map user and book IDs to indices\n",
    "    user_indices = df['user_id'].map(user_id_to_index).dropna().astype(int).values\n",
    "    book_indices = df['book_id'].map(book_id_to_index).dropna().astype(int).values\n",
    "\n",
    "    # Ensure valid mappings\n",
    "    valid_mask = (user_indices >= 0) & (book_indices >= 0)\n",
    "    user_indices = user_indices[valid_mask]\n",
    "    book_indices = book_indices[valid_mask]\n",
    "\n",
    "    # Create edge index\n",
    "    edge_index = torch.tensor([user_indices, book_indices], dtype=torch.long)\n",
    "\n",
    "    # Convert ratings and confidence scores to tensors\n",
    "    ratings_tensor = torch.tensor(df.loc[valid_mask, 'rating'].values, dtype=torch.float32).view(-1, 1)\n",
    "    confidence_tensor = torch.tensor(df.loc[valid_mask, 'confidence_score'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Efficiently convert list of NumPy arrays to a tensor\n",
    "    embeddings_np = np.stack(df.loc[valid_mask, 'embeddings'].values)  # Stack directly\n",
    "    embeddings_tensor = torch.from_numpy(embeddings_np).float()  # Convert efficiently\n",
    "\n",
    "    # Concatenate ratings, confidence scores, and embeddings into a single edge attribute tensor\n",
    "    edge_attr = torch.cat([ratings_tensor, confidence_tensor, embeddings_tensor], dim=1)\n",
    "\n",
    "    return edge_index, edge_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_objects(train_data, val_data, test_data, user_genre_features, book_genre_features, user_id_to_index, book_id_to_index):\n",
    "    \"\"\"\n",
    "    Prepares the data objects for a graph-based recommendation system, including the edge indices, edge attributes, \n",
    "    and node features for users and books. These objects are used for training, validation, and testing in PyTorch Geometric (PyG).\n",
    "\n",
    "    The function processes the input data, creates embeddings for users and books based on their genres, and constructs \n",
    "    the corresponding graph structures with the appropriate edge indices and attributes. The resulting PyG data objects \n",
    "    can then be used for graph-based learning.\n",
    "\n",
    "    Args:\n",
    "    train_data (DataFrame): The training set containing user-book interactions with ratings, confidence scores, and embeddings.\n",
    "    val_data (DataFrame): The validation set containing user-book interactions with ratings, confidence scores, and embeddings.\n",
    "    test_data (DataFrame): The test set containing user-book interactions with ratings, confidence scores, and embeddings.\n",
    "    user_genre_features (dict): A dictionary where keys are user indices and values are tensors representing user genre features.\n",
    "    book_genre_features (dict): A dictionary where keys are book indices and values are tensors representing book genre features.\n",
    "    user_id_to_index (dict): A dictionary mapping user IDs to internal user indices.\n",
    "    book_id_to_index (dict): A dictionary mapping book IDs to internal book indices.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - train_data_obj (torch_geometric.data.Data): The PyG data object for the training set with node features, edge indices, and edge attributes.\n",
    "        - val_data_obj (torch_geometric.data.Data): The PyG data object for the validation set with node features, edge indices, and edge attributes.\n",
    "        - test_data_obj (torch_geometric.data.Data): The PyG data object for the test set with node features, edge indices, and edge attributes.\n",
    "\n",
    "    \"\"\"\n",
    "    train_edge_index, train_edge_attr = prepare_edge_index_ratings_attributes(\n",
    "        train_data, user_id_to_index, book_id_to_index\n",
    "    )\n",
    "    val_edge_index, val_edge_attr = prepare_edge_index_ratings_attributes(\n",
    "        val_data, user_id_to_index, book_id_to_index\n",
    "    )\n",
    "    test_edge_index, test_edge_attr = prepare_edge_index_ratings_attributes(\n",
    "        test_data, user_id_to_index, book_id_to_index\n",
    "    )\n",
    "    user_embeddings = torch.from_numpy(np.stack(list(user_genre_features.values()))).float()\n",
    "    book_embeddings = torch.from_numpy(np.stack(list(book_genre_features.values()))).float()\n",
    "    node_embeddings = torch.cat([user_embeddings, book_embeddings], dim=0)\n",
    "    train_edge_index = train_edge_index.clone().detach() \n",
    "    val_edge_index = val_edge_index.clone().detach()\n",
    "    test_edge_index = test_edge_index.clone().detach()\n",
    "\n",
    "    train_data_obj = Data(\n",
    "        x=node_embeddings,\n",
    "        edge_index=train_edge_index,\n",
    "        edge_attr=train_edge_attr  \n",
    "    )\n",
    "\n",
    "    val_data_obj = Data(\n",
    "        x=node_embeddings,\n",
    "        edge_index=val_edge_index,\n",
    "        edge_attr=val_edge_attr\n",
    "    )\n",
    "    \n",
    "    test_data_obj = Data(\n",
    "        x=node_embeddings,\n",
    "        edge_index=test_edge_index,\n",
    "        edge_attr=test_edge_attr\n",
    "    )\n",
    "\n",
    "    return train_data_obj, val_data_obj, test_data_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews, books, read, user_genres, review_embeddings, review_sentiment, imputed= load_data()\n",
    "reviews = reviews[reviews['rating'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(reviews, imputed[['user_id', 'book_id', 'rating']], on=['user_id', 'book_id'], how='left', suffixes=('', '_new'))\n",
    "merged_df['rating'] = merged_df['rating_new'].fillna(merged_df['rating'])  \n",
    "final_df = merged_df.drop(columns=['rating_new']) \n",
    "reviews = final_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.merge(reviews, review_embeddings, on=\"review_id\", how=\"inner\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.merge(reviews, review_sentiment, on=\"review_id\", how=\"inner\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews  = reviews[['rating', 'user_id', 'book_id', 'confidence_score', 'embeddings']]   \n",
    "books = books[['book_id', 'title', 'authors', 'filtered_genres', 'average_rating']]\n",
    "data = pd.merge(books, reviews, on='book_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)\n",
    "user_genres = user_genres.reset_index()\n",
    "user_genres = user_genres[user_genres['most_common_genres'].apply(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, user_genres, on='user_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_data(data, user_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_to_index, book_id_to_index = initialize_id_mappings(data)\n",
    "train_data = balance_ratings(train_data)\n",
    "train_data, test_data, val_data, min_rating = normalise_ratings(train_data, test_data, val_data)\n",
    "user_genre_features, book_genre_features = user_book_features(data, user_id_to_index, book_id_to_index)\n",
    "train_data_obj, val_data_obj, test_data_obj = prepare_data_objects(\n",
    "    train_data, val_data, test_data, user_genre_features, book_genre_features, user_id_to_index, book_id_to_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../Pickle/user_id_to_index_gat.pkl', 'wb') as f:\n",
    "    pickle.dump(user_id_to_index, f)\n",
    "\n",
    "with open('../Pickle/book_id_to_index_gat.pkl', 'wb') as f:\n",
    "    pickle.dump(book_id_to_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads, edge_feature_dim, dropout_rate=0.2):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Define the GAT layers\n",
    "        self.gat1 = GATv2Conv(in_channels, hidden_channels, heads=num_heads, edge_dim=edge_feature_dim)\n",
    "        self.gat2 = GATv2Conv(hidden_channels * num_heads, hidden_channels, heads=num_heads, edge_dim=edge_feature_dim)\n",
    "        self.gat3 = GATv2Conv(hidden_channels * num_heads, hidden_channels, heads=num_heads, edge_dim=edge_feature_dim)\n",
    "        self.gat4 = GATv2Conv(hidden_channels * num_heads, hidden_channels, heads=num_heads, edge_dim=edge_feature_dim)\n",
    "        self.gat5 = GATv2Conv(hidden_channels * num_heads, out_channels, heads=num_heads, concat=False, edge_dim=edge_feature_dim)\n",
    "\n",
    "        # LayerNorm layers (replacing BatchNorm)\n",
    "        self.ln1 = nn.LayerNorm(hidden_channels * num_heads)\n",
    "        self.ln2 = nn.LayerNorm(hidden_channels * num_heads)\n",
    "        self.ln3 = nn.LayerNorm(hidden_channels * num_heads)\n",
    "        self.ln4 = nn.LayerNorm(hidden_channels * num_heads)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.gat1(x, edge_index, edge_attr)\n",
    "        x = self.ln1(x)  # LayerNorm instead of BatchNorm\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.gat2(x, edge_index, edge_attr)\n",
    "        x = self.ln2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.gat3(x, edge_index, edge_attr)\n",
    "        x = self.ln3(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.gat4(x, edge_index, edge_attr)\n",
    "        x = self.ln4(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.gat5(x, edge_index, edge_attr)\n",
    "\n",
    "        edge_outputs = torch.sum(x[edge_index[0]] * x[edge_index[1]], dim=-1)\n",
    "\n",
    "        return edge_outputs, x\n",
    "\n",
    "    def predict(self, x, edge_index, edge_attr):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            edge_outputs, node_embeddings = self.forward(x, edge_index, edge_attr)\n",
    "            return node_embeddings, edge_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data_list = [train_data_obj]  \n",
    "test_data_list = [test_data_obj]\n",
    "val_data_list = [val_data_obj]\n",
    "\n",
    "train_loader = DataLoader(train_data_list, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_data_list, batch_size=4, shuffle=False)\n",
    "val_loader = DataLoader(val_data_list, batch_size=4, shuffle=False)\n",
    "\n",
    "all_embeddings = train_data['embeddings'].tolist() + test_data['embeddings'].tolist()\n",
    "\n",
    "all_embeddings = np.array(all_embeddings, dtype=object)\n",
    "\n",
    "embedding_size = len(all_embeddings[0])\n",
    "edge_feature_dim = 1 + 1 + embedding_size  # Rating + Confidence + Embedding Size\n",
    "\n",
    "model = GATModel(\n",
    "    in_channels=train_data_obj.x.shape[1],  # Input features per node\n",
    "    hidden_channels=30,\n",
    "    out_channels=1,\n",
    "    num_heads=25,\n",
    "    edge_feature_dim=edge_feature_dim  # Correct edge feature dimension\n",
    ").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(predictions, true_values):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(true_values, predictions, alpha=0.5, color='blue', label=\"Predictions vs True\")\n",
    "    plt.plot([min(true_values), max(true_values)], [min(true_values), max(true_values)], 'r--', label='Perfect Prediction')\n",
    "    plt.xlabel('True Ratings')\n",
    "    plt.ylabel('Predicted Ratings')\n",
    "    plt.title('Predicted Ratings vs True Ratings')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, delta):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def __call__(self, val_loss, epoch):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_epoch = epoch\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def plot_loss(train_losses, test_losses):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(test_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gat(model, train_loader, val_loader, test_loader, num_epochs, lr, device):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # Learning rate scheduler\n",
    "    criterion = nn.MSELoss()\n",
    "    early_stopping = EarlyStopping(patience=10, delta=0.0001)\n",
    "\n",
    "    all_true_values = []\n",
    "    all_predicted_values = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        num_train_batches = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            edge_outputs, embeddings = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            edge_outputs = edge_outputs.view(-1, 1)\n",
    "            target = batch.edge_attr[:, 0].view(-1, 1)\n",
    "\n",
    "            all_true_values.extend(target.cpu().numpy())\n",
    "            all_predicted_values.extend(edge_outputs.cpu().detach().numpy())\n",
    "\n",
    "            loss = criterion(edge_outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "\n",
    "        average_train_loss = total_train_loss / num_train_batches\n",
    "        train_losses.append(average_train_loss)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_train_loss:.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        num_val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                val_out, _ = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                val_out = val_out.view(-1, 1)\n",
    "                target = batch.edge_attr[:, 0].view(-1, 1)\n",
    "                val_loss = criterion(val_out, target)\n",
    "                total_val_loss += val_loss.item()\n",
    "                num_val_batches += 1\n",
    "\n",
    "        average_val_loss = total_val_loss / num_val_batches\n",
    "        val_losses.append(average_val_loss)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {average_val_loss:.4f}')\n",
    "\n",
    "        if average_val_loss < early_stopping.best_loss:\n",
    "            best_model_state = model.state_dict()\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(best_model_state, '../Pickle/gat_model.pth')\n",
    "\n",
    "        early_stopping(average_val_loss, epoch + 1)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}. Best model was at epoch {best_epoch}.\")\n",
    "            break\n",
    "\n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "    plot_loss(train_losses, val_losses)\n",
    "    model.load_state_dict(torch.load('../Pickle/gat_model.pth'))\n",
    "    print(f\"Loaded best model from epoch {best_epoch} for final evaluation.\")\n",
    "\n",
    "    # Evaluation phase using the test set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    all_embeddings = [] \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            test_out, embeddings = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            test_out = test_out.view(-1, 1)\n",
    "            target = batch.edge_attr[:, 0].view(-1, 1)\n",
    "            all_preds.append(test_out)\n",
    "            all_true.append(target)\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0).cpu().numpy()\n",
    "    all_true = torch.cat(all_true, dim=0).cpu().numpy()\n",
    "    all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "    rmse = root_mean_squared_error(all_true, all_preds)\n",
    "    mae = mean_absolute_error(all_true, all_preds)\n",
    "    print(f\"Mean Squared Error: {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "\n",
    "    return rmse, mae, all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse, mae, all_embeddings = train_gat(model, train_loader, val_loader, test_loader, num_epochs=150, lr=0.0001, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings, predicted_ratings = model.predict(test_data_obj.x, test_data_obj.edge_index, test_data_obj.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = test_data_obj.edge_index[0]  \n",
    "item_ids = test_data_obj.edge_index[1]  \n",
    "true_ratings = test_data_obj.edge_attr[:, 0].cpu().numpy()\n",
    "predictions = [(uid.item(), iid.item(), true_ratings[idx], predicted_ratings[idx].item()) \n",
    "               for idx, (uid, iid) in enumerate(zip(user_ids, item_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/gat_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(all_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "books['book_id'] = books['book_id'].astype(str)\n",
    "book_ids = [str(int(prediction.iid)) for prediction in predictions]\n",
    "books_df = books.set_index('book_id')\n",
    "average_ratings = []\n",
    "for book_id in book_ids:\n",
    "    if book_id in books_df.index:\n",
    "        average_ratings.append(float(books_df.loc[book_id, 'average_rating']))\n",
    "    else:\n",
    "        continue\n",
    "average_ratings.sort()\n",
    "bins = np.arange(0, 5.1, 0.5)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(average_ratings, bins=bins, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Actual Average Ratings of Predictions (NMF)', fontsize=14)\n",
    "plt.xlabel('Average Rating', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(np.arange(0, 5.5, 0.5)) \n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, min_rating, k, threshold, item_popularity, num_users):\n",
    "    denormalized_predictions = [\n",
    "        (uid, iid, true_r * (5 - min_rating) + min_rating, est * (5 - min_rating) + min_rating)\n",
    "        for (uid, iid, true_r, est) in predictions\n",
    "    ]\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, iid, true_r, est in denormalized_predictions:\n",
    "        user_est_true[uid].append((iid, est, true_r))\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    map_scores = []\n",
    "    mrr_scores = [] \n",
    "    novelty_scores = []\n",
    "    hit_rate_scores = []\n",
    "    recommended_relevant_items = set() \n",
    "\n",
    "    for user_id, user_ratings in user_est_true.items():\n",
    "        user_ratings_sorted = sorted(user_ratings, key=lambda x: x[1], reverse=True)\n",
    "        top_k = user_ratings_sorted[:k]\n",
    "        top_k_items = [iid for iid, _, _ in top_k]\n",
    "        n_rel = sum((true_r >= threshold) for (_, _, true_r) in user_ratings)\n",
    "        n_rel_and_rec_k = sum((true_r >= threshold) for (_, _, true_r) in top_k)\n",
    "        precision = n_rel_and_rec_k / k if k != 0 else 0\n",
    "        recall = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        dcg = sum(\n",
    "            (true_r >= threshold) / np.log2(idx + 2)\n",
    "            for idx, (_, _, true_r) in enumerate(top_k)\n",
    "        )\n",
    "        idcg = sum(\n",
    "            1.0 / np.log2(idx + 2) for idx in range(min(n_rel, k))\n",
    "        )\n",
    "        ndcg = dcg / idcg if idcg != 0 else 0\n",
    "        ndcgs.append(ndcg)\n",
    "        hits = 0\n",
    "        sum_precisions = 0\n",
    "        for idx, (_, _, true_r) in enumerate(top_k):\n",
    "            if true_r >= threshold:\n",
    "                hits += 1\n",
    "                sum_precisions += hits / (idx + 1)\n",
    "        ap = sum_precisions / min(n_rel, k) if n_rel != 0 else 0\n",
    "        map_scores.append(ap)\n",
    "        mrr = 0\n",
    "        for idx, (_, _, true_r) in enumerate(top_k):\n",
    "            if true_r >= threshold:\n",
    "                mrr = 1 / (idx + 1)\n",
    "                break\n",
    "        mrr_scores.append(mrr)\n",
    "        novelty = np.mean([1 - (item_popularity.get(iid, 1) / num_users) for iid in top_k_items])\n",
    "        novelty_scores.append(novelty)\n",
    "        hit_rate = 1 if n_rel_and_rec_k > 0 else 0\n",
    "        hit_rate_scores.append(hit_rate)\n",
    "        recommended_relevant_items.update([iid for iid, est, true_r in top_k if true_r >= threshold])\n",
    "    user_coverage = sum(1 for user_ratings in user_est_true.values() if any(true_r >= threshold for _, _, true_r in user_ratings)) / len(user_est_true)\n",
    "\n",
    "    return (\n",
    "        np.mean(precisions),\n",
    "        np.mean(recalls),\n",
    "        np.mean(ndcgs),\n",
    "        np.mean(map_scores),\n",
    "        np.mean(mrr_scores),  \n",
    "        np.mean(novelty_scores),\n",
    "        np.mean(hit_rate_scores),\n",
    "        user_coverage\n",
    "    )\n",
    "\n",
    "k = 5\n",
    "threshold = 4 \n",
    "item_popularity = test_data['book_id'].value_counts().to_dict()\n",
    "num_users = test_data['user_id'].nunique()\n",
    "\n",
    "precision, recall, ndcg, map_score, mrr, novelty, hit_rate, user_coverage = evaluate_model(\n",
    "    predictions, min_rating=min_rating, k=k, threshold=threshold, item_popularity=item_popularity, num_users=num_users\n",
    ")\n",
    "print(f'Precision@{k}: {precision:.4f}')\n",
    "print(f'Recall@{k}: {recall:.4f}')\n",
    "print(f'nDCG@{k}: {ndcg:.4f}')\n",
    "print(f'MAP@{k}: {map_score:.4f}')\n",
    "print(f'MRR@{k}: {mrr:.4f}')\n",
    "print(f'Novelty: {novelty:.4f}')\n",
    "print(f'Hit Rate: {hit_rate:.4f}')\n",
    "print(f'User Coverage: {user_coverage:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "user_predictions = {}\n",
    "\n",
    "# Loop through all predictions (user-item pairs and their predicted ratings)\n",
    "for uid, iid, true_rating, predicted_rating in predictions:\n",
    "    if uid not in user_predictions:\n",
    "        user_predictions[uid] = []\n",
    "    user_predictions[uid].append((iid, predicted_rating))\n",
    "\n",
    "top_k = 5  # Choose the value of k, e.g., top-5 recommendations\n",
    "top_k_predictions = {}\n",
    "\n",
    "for uid, items in user_predictions.items():\n",
    "    # Sort items by predicted ratings in descending order and get top-k\n",
    "    sorted_items = sorted(items, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    top_k_predictions[uid] = sorted_items\n",
    "\n",
    "# Genres for top-k recommended books\n",
    "predicted_book_genres = []\n",
    "\n",
    "# Loop through top-k recommendations\n",
    "for uid, top_items in top_k_predictions.items():\n",
    "    for iid, predicted_rating in top_items:\n",
    "        book = books[books['book_id'] == iid]  # Match book ID to the books dataset\n",
    "        if not book.empty:\n",
    "            genres = book.iloc[0]['filtered_genres'].split(',')  # assuming comma-separated\n",
    "            predicted_book_genres.extend([genre.strip() for genre in genres])\n",
    "\n",
    "# Count genres in top-k recommendations\n",
    "predicted_genre_counts = Counter(predicted_book_genres)\n",
    "\n",
    "# Get genre counts from the training data\n",
    "genre_list = []\n",
    "for genres in books['filtered_genres']:\n",
    "    genre_list.extend([genre.strip() for genre in genres.split(',')])\n",
    "\n",
    "train_genre_counts_series = pd.Series(genre_list).value_counts()\n",
    "train_genre_counts_df = train_genre_counts_series.reset_index()\n",
    "train_genre_counts_df.columns = ['Genre', 'Count']\n",
    "\n",
    "# Align genres for both plots by using the training genre order\n",
    "genre_order = train_genre_counts_df['Genre'].tolist()\n",
    "\n",
    "# Reindex predicted genre counts to match training genre order\n",
    "predicted_genre_counts_ordered = [predicted_genre_counts.get(genre, 0) for genre in genre_order]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot for training data\n",
    "ax[0].bar(genre_order, train_genre_counts_df['Count'], color='lightblue')\n",
    "ax[0].set_xlabel('Genre')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_title('Genre Distribution in Gat Training Data')\n",
    "ax[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot for predicted (top-k) data\n",
    "ax[1].bar(genre_order, predicted_genre_counts_ordered, color='lightgreen')\n",
    "ax[1].set_xlabel('Genre')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_title(f'Genre Distribution of Top-{top_k} Gat Recommended Books')\n",
    "ax[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Overlap calculation\n",
    "overlap = sum(\n",
    "    min(train_genre_counts_series.get(genre, 0), predicted_genre_counts.get(genre, 0)) \n",
    "    for genre in train_genre_counts_series.index\n",
    ")\n",
    "total_train_genres = train_genre_counts_series.sum()\n",
    "total_predicted_genres = sum(predicted_genre_counts.values())\n",
    "\n",
    "# Count total unique genres\n",
    "unique_train_genres = train_genre_counts_series.index.nunique()\n",
    "unique_predicted_genres = len(predicted_genre_counts)\n",
    "\n",
    "print(f\"Overlap of genres: {overlap}\")\n",
    "print(f\"Total genres in training data (counts): {total_train_genres}\")\n",
    "print(f\"Total genres in top-{top_k} recommendations (counts): {total_predicted_genres}\")\n",
    "print(f\"Total unique genres in training data: {unique_train_genres}\")\n",
    "print(f\"Total unique genres in top-{top_k} recommendations: {unique_predicted_genres}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings = denormalize_rating(predicted_ratings, min_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_prediction_histogram(predictions):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(predicted_ratings, bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title('Distribution of Predicted Ratings Gatv2Conv')\n",
    "    plt.xlabel('Predicted Ratings')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "plot_prediction_histogram(predicted_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings.min().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rating'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
