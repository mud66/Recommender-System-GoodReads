{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_random_seed(seed_value):\n",
    "    \"\"\"Set the random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)  # Set the seed for Python's random module\n",
    "    np.random.seed(seed_value)  # Set the seed for NumPy\n",
    "    torch.manual_seed(seed_value)  # Set the seed for PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed_value)  # Set the seed for PyTorch\n",
    "    torch.cuda.manual_seed_all(seed_value)  # Set the seed for all GPUs\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior on GPU\n",
    "    torch.backends.cudnn.benchmark = False  # Turn off benchmarks for reproducibility\n",
    "\n",
    "# Set a specific random seed value (e.g., 42)\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_data():\n",
    "    reviews = pd.read_pickle('../Pickle/reviews.pkl')\n",
    "    books = pd.read_pickle('../Pickle/books.pkl')\n",
    "    read = pd.read_pickle('../Pickle/read.pkl')\n",
    "    user_genres = pd.read_pickle('../Pickle/user_most_common_genres.pkl')\n",
    "    review_embeddings = pd.read_pickle('../Pickle/review_embeddings.pkl')\n",
    "    review_sentiment = pd.read_pickle('../Pickle/review_score.pkl')\n",
    "    return reviews, books, read, user_genres, review_embeddings, review_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ID mappings for users and books\n",
    "def initialize_id_mappings(combined_data):\n",
    "    unique_user_ids = set(combined_data['user_id'])\n",
    "    unique_book_ids = set(combined_data['book_id'])\n",
    "\n",
    "    user_id_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "    book_id_to_index = {book_id: idx for idx, book_id in enumerate(unique_book_ids)}\n",
    "\n",
    "    return user_id_to_index, book_id_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_split_data(ratings_data, user_genres, test_size=0.2, random_state=42):\n",
    "    # Filter out books that have been rated fewer than twice\n",
    "    book_user_counts = ratings_data['book_id'].value_counts()\n",
    "    eligible_books = book_user_counts[book_user_counts > 5].index  \n",
    "    ratings_data = ratings_data[ratings_data['book_id'].isin(eligible_books)]\n",
    "\n",
    "    # Filter out users who don't have enough ratings\n",
    "    user_book_counts = ratings_data['user_id'].value_counts()\n",
    "    eligible_users = user_book_counts[user_book_counts > 5].index  \n",
    "    ratings_data = ratings_data[ratings_data['user_id'].isin(eligible_users)]    \n",
    "\n",
    "    # Ensure users are in both ratings_data and user_genres\n",
    "    eligible_users_in_genres = user_genres['user_id'].isin(eligible_users)\n",
    "    user_genres = user_genres[eligible_users_in_genres]\n",
    "    \n",
    "    # Merge the ratings_data and user_genres on user_id to get the most common genres for users\n",
    "    filtered_data = ratings_data.merge(user_genres[['user_id', 'most_common_genres']], on='user_id', how='inner')\n",
    "\n",
    "    # Now proceed with train-test split\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "\n",
    "    # Create a mask to check for users with enough rated books (>= 5)\n",
    "    user_data_valid = filtered_data.groupby('user_id').filter(lambda x: len(x) > 5)\n",
    "\n",
    "    # Split train-test for each user\n",
    "    for user_id, user_data in user_data_valid.groupby('user_id'):\n",
    "        books = user_data['book_id'].unique()  # All books rated by the user\n",
    "        \n",
    "        # Split the books into train and test sets\n",
    "        train_books, test_books = train_test_split(books, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        # Get the train and test data for the user\n",
    "        user_train_data = user_data[user_data['book_id'].isin(train_books)]\n",
    "        user_test_data = user_data[user_data['book_id'].isin(test_books)]\n",
    "        \n",
    "        # Append to train and test lists\n",
    "        train_dfs.append(user_train_data)\n",
    "        test_dfs.append(user_test_data)\n",
    "    \n",
    "    # Combine all the train and test data into single dataframes\n",
    "    train_data = pd.concat(train_dfs)\n",
    "    test_data = pd.concat(test_dfs)\n",
    "\n",
    "    return train_data, test_data, user_genres, filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log scale ratings\n",
    "def normalize_ratings(train_data, test_data):\n",
    "    min_rating = train_data['rating'].min()\n",
    "    \n",
    "    if min_rating < 0:\n",
    "        train_data['rating'] = train_data['rating'] - min_rating\n",
    "        test_data['rating'] = test_data['rating'] - min_rating\n",
    "\n",
    "    train_data['rating'] = np.log1p(train_data['rating'])\n",
    "    test_data['rating'] = np.log1p(test_data['rating'])\n",
    "\n",
    "    return train_data, test_data, min_rating\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def denormalize_rating(log_scaled_ratings, min_rating=0):\n",
    "    log_scaled_ratings = np.asarray(log_scaled_ratings, dtype=float)  # Ensure NumPy array\n",
    "\n",
    "    # Reverse log1p transformation\n",
    "    original_ratings = np.expm1(log_scaled_ratings)\n",
    "\n",
    "    # Adjust for minimum rating\n",
    "    if min_rating:\n",
    "        original_ratings += min_rating\n",
    "\n",
    "    # Clip values between 0 and 5\n",
    "    return np.clip(original_ratings, 0, 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def balance_ratings(train_data):\n",
    "    # Get the value counts of the rating column\n",
    "    rating_counts = train_data['rating'].value_counts()\n",
    "\n",
    "    # Find the target number of samples for balancing (median of class counts)\n",
    "    target_count = 12000\n",
    "\n",
    "    # Separate the data by ratings\n",
    "    balanced_data = []\n",
    "\n",
    "    for rating, count in rating_counts.items():\n",
    "        rating_data = train_data[train_data['rating'] == rating]\n",
    "        \n",
    "        if count > target_count:\n",
    "            # Under-sample if the class has more samples than the target\n",
    "            rating_data = resample(rating_data, \n",
    "                                   replace=False, \n",
    "                                   n_samples=target_count, \n",
    "                                   random_state=42)\n",
    "        elif count < target_count:\n",
    "            # Over-sample if the class has fewer samples than the target\n",
    "            rating_data = resample(rating_data, \n",
    "                                   replace=True, \n",
    "                                   n_samples=target_count, \n",
    "                                   random_state=42)\n",
    "\n",
    "        # Add the balanced data for this rating to the list\n",
    "        balanced_data.append(rating_data)\n",
    "\n",
    "    # Combine all the balanced data\n",
    "    balanced_train_data = pd.concat(balanced_data, axis=0)\n",
    "\n",
    "    # Shuffle the dataset to mix the over-sampled and under-sampled data\n",
    "    balanced_train_data = balanced_train_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return balanced_train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def prepare_edge_index_ratings_attributes(df, user_id_to_index, book_id_to_index):\n",
    "    # Map user and book IDs to indices\n",
    "    user_indices = df['user_id'].map(user_id_to_index).dropna().astype(int).values\n",
    "    book_indices = df['book_id'].map(book_id_to_index).dropna().astype(int).values\n",
    "\n",
    "    # Ensure valid mappings\n",
    "    valid_mask = (user_indices >= 0) & (book_indices >= 0)\n",
    "    user_indices = user_indices[valid_mask]\n",
    "    book_indices = book_indices[valid_mask]\n",
    "\n",
    "    # Create edge index\n",
    "    edge_index = torch.tensor([user_indices, book_indices], dtype=torch.long)\n",
    "\n",
    "    # Convert ratings and confidence scores to tensors\n",
    "    ratings_tensor = torch.tensor(df.loc[valid_mask, 'rating'].values, dtype=torch.float32).view(-1, 1)\n",
    "    confidence_tensor = torch.tensor(df.loc[valid_mask, 'confidence_score'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Efficiently convert list of NumPy arrays to a tensor\n",
    "    embeddings_np = np.stack(df.loc[valid_mask, 'embeddings'].values)  # Stack directly\n",
    "    embeddings_tensor = torch.from_numpy(embeddings_np).float()  # Convert efficiently\n",
    "\n",
    "    # Concatenate ratings, confidence scores, and embeddings into a single edge attribute tensor\n",
    "    edge_attr = torch.cat([ratings_tensor, confidence_tensor, embeddings_tensor], dim=1)\n",
    "\n",
    "    return edge_index, edge_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_user_and_book_features(filtered_data, user_id_to_index, book_id_to_index):\n",
    "    # Create a dictionary for all possible genres (book genres from filtered data)\n",
    "    unique_book_genres = sorted(set(filtered_data['filtered_genres'].str.split(',').explode()))\n",
    "    book_genre_dict = {genre: idx for idx, genre in enumerate(unique_book_genres)}\n",
    "\n",
    "    # Prepare user genre features\n",
    "    user_genre_features = {}\n",
    "    \n",
    "    # Group by user_id and process all genres at once\n",
    "    for user_id, group in filtered_data.groupby('user_id'):\n",
    "        genres = group['most_common_genres'].iloc[0]  # All rows for this user should have the same genres\n",
    "        genre_vector = np.zeros(len(book_genre_dict))  # Size based on unique book genres\n",
    "        for genre in genres:\n",
    "            if genre in book_genre_dict:\n",
    "                genre_vector[book_genre_dict[genre]] = 1\n",
    "        user_genre_features[user_id_to_index[user_id]] = torch.tensor(genre_vector, dtype=torch.float32)\n",
    "\n",
    "    # Prepare book genre features\n",
    "    book_genre_features = {}\n",
    "    for book_id, group in filtered_data.groupby('book_id'):\n",
    "        genres = group['filtered_genres'].iloc[0].split(',')  # Assuming all rows for this book have the same genres\n",
    "        genre_vector = np.zeros(len(book_genre_dict))  # Size based on unique book genres\n",
    "        for genre in genres:\n",
    "            if genre in book_genre_dict:\n",
    "                genre_vector[book_genre_dict[genre]] = 1\n",
    "        book_genre_features[book_id_to_index[book_id]] = torch.tensor(genre_vector, dtype=torch.float32)\n",
    "\n",
    "    return user_genre_features, book_genre_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_pca_on_features(user_genre_features, book_genre_features, n_components=5):\n",
    "    # Combine user and book features into one array for PCA\n",
    "    all_user_features = torch.stack(list(user_genre_features.values()))\n",
    "    all_book_features = torch.stack(list(book_genre_features.values()))\n",
    "\n",
    "    all_features = torch.cat([all_user_features, all_book_features], dim=0)  # Combine user and book features\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_features = pca.fit_transform(all_features)\n",
    "\n",
    "    # Split back the reduced features into user and book features\n",
    "    reduced_user_features = reduced_features[:len(user_genre_features)]\n",
    "    reduced_book_features = reduced_features[len(user_genre_features):]\n",
    "\n",
    "    # Update the user and book genre features dictionaries with the reduced features\n",
    "    updated_user_genre_features = {key: torch.tensor(val) for key, val in zip(user_genre_features.keys(), reduced_user_features)}\n",
    "    updated_book_genre_features = {key: torch.tensor(val) for key, val in zip(book_genre_features.keys(), reduced_book_features)}\n",
    "\n",
    "    return updated_user_genre_features, updated_book_genre_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def prepare_data_objects(train_data, test_data, user_genre_features, book_genre_features, user_id_to_index, book_id_to_index):\n",
    "    train_edge_index, train_edge_attr = prepare_edge_index_ratings_attributes(\n",
    "        train_data, user_id_to_index, book_id_to_index\n",
    "    )\n",
    "    test_edge_index, test_edge_attr = prepare_edge_index_ratings_attributes(\n",
    "        test_data, user_id_to_index, book_id_to_index\n",
    "    )\n",
    "\n",
    "    # Convert user and book genre features efficiently\n",
    "    user_embeddings = torch.from_numpy(np.stack(list(user_genre_features.values()))).float()\n",
    "    book_embeddings = torch.from_numpy(np.stack(list(book_genre_features.values()))).float()\n",
    "\n",
    "    # Combine user and book embeddings into node features\n",
    "    node_embeddings = torch.cat([user_embeddings, book_embeddings], dim=0)\n",
    "\n",
    "    # Ensure edge_index is correctly formatted\n",
    "    train_edge_index = train_edge_index.clone().detach()  # Ensure it's a tensor\n",
    "    test_edge_index = test_edge_index.clone().detach()\n",
    "\n",
    "    # Create PyG Data objects\n",
    "    train_data_obj = Data(\n",
    "        x=node_embeddings,\n",
    "        edge_index=train_edge_index,\n",
    "        edge_attr=train_edge_attr  \n",
    "    )\n",
    "    \n",
    "    test_data_obj = Data(\n",
    "        x=node_embeddings,\n",
    "        edge_index=test_edge_index,\n",
    "        edge_attr=test_edge_attr\n",
    "    )\n",
    "\n",
    "    return train_data_obj, test_data_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews, books, read, user_genres, review_embeddings, review_sentiment = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.merge(reviews, review_embeddings, on=\"review_id\", how=\"inner\")  \n",
    "reviews = pd.merge(reviews, review_sentiment, on=\"review_id\", how=\"inner\") \n",
    "reviews  = reviews[['rating', 'user_id', 'book_id', 'confidence_score', 'embeddings']]   \n",
    "books = books[['book_id', 'title', 'authors', 'filtered_genres', 'average_rating']]\n",
    "data = pd.merge(books, reviews, on='book_id', how='inner')\n",
    "data = data.reset_index(drop=True)\n",
    "user_genres = user_genres.reset_index()\n",
    "user_genres = user_genres[user_genres['most_common_genres'].apply(lambda x: len(x) > 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the average rating per user (excluding 0s) and round to the nearest whole number\n",
    "user_avg_rating = data[data['rating'] > 0].groupby('user_id')['rating'].mean().round().astype(int)\n",
    "\n",
    "#Round the book's average rating column\n",
    "data['average_rating'] = pd.to_numeric(data['average_rating'], errors='coerce').round().astype('Int64')\n",
    "\n",
    "#Replace 0 ratings with the user's average rating or the book's average rating if the user has no ratings\n",
    "def impute_rating(row):\n",
    "    if row['rating'] == 0:\n",
    "        return user_avg_rating.get(row['user_id'], row['average_rating'])  # Use user avg or book avg\n",
    "    return row['rating']\n",
    "\n",
    "# Apply the function\n",
    "data['rating'] = data.apply(impute_rating, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, user_genres, filtered_data = filter_and_split_data(data, user_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_to_index, book_id_to_index = initialize_id_mappings(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = balance_ratings(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, min_rating = normalize_ratings(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_genre_features, book_genre_features = align_user_and_book_features(filtered_data, user_id_to_index, book_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_obj, test_data_obj = prepare_data_objects(\n",
    "    train_data, test_data, user_genre_features, book_genre_features, user_id_to_index, book_id_to_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn as nn\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads, edge_feature_dim, dropout_rate=0.2):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.dropout_rate = dropout_rate  # Dropout rate\n",
    "\n",
    "        # Define the GAT layers\n",
    "        self.gat1 = GATv2Conv(in_channels, hidden_channels, heads=num_heads, edge_dim=edge_feature_dim)\n",
    "        self.gat2 = GATv2Conv(hidden_channels * num_heads, hidden_channels, heads=num_heads, edge_dim=edge_feature_dim)\n",
    "        self.gat3 = GATv2Conv(hidden_channels * num_heads, hidden_channels, heads=num_heads, edge_dim=edge_feature_dim)\n",
    "        self.gat4 = GATv2Conv(hidden_channels * num_heads, hidden_channels, heads=num_heads, edge_dim=edge_feature_dim)\n",
    "        self.gat5 = GATv2Conv(hidden_channels * num_heads, out_channels, heads=num_heads, concat=False, edge_dim=edge_feature_dim)\n",
    "\n",
    "        # Batch Normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels * num_heads)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels * num_heads)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_channels * num_heads)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_channels * num_heads)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Pass through the GAT layers with BatchNorm, Dropout, and activation\n",
    "        x = self.gat1(x, edge_index, edge_attr)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = F.elu(x)  # Activation function\n",
    "        x = self.dropout(x)  # Dropout after activation\n",
    "\n",
    "        x = self.gat2(x, edge_index, edge_attr)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.gat3(x, edge_index, edge_attr)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.gat4(x, edge_index, edge_attr)\n",
    "        x = self.bn4(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x, attn_weights_5 = self.gat5(x, edge_index, edge_attr, return_attention_weights=True)\n",
    "\n",
    "        # Compute edge outputs using node embeddings\n",
    "        edge_outputs = torch.sum(x[edge_index[0]] * x[edge_index[1]], dim=-1)\n",
    "\n",
    "        # Return edge outputs and attention weights from all layers\n",
    "        return edge_outputs, attn_weights_5\n",
    "\n",
    "    def predict(self, x, edge_index, edge_attr):\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            edge_outputs, attn_weights_5 = self.forward(x, edge_index, edge_attr)\n",
    "            return edge_outputs, attn_weights_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(predictions, true_values):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(true_values, predictions, alpha=0.5, color='blue', label=\"Predictions vs True\")\n",
    "    plt.plot([min(true_values), max(true_values)], [min(true_values), max(true_values)], 'r--', label='Perfect Prediction')\n",
    "    plt.xlabel('True Ratings')\n",
    "    plt.ylabel('Predicted Ratings')\n",
    "    plt.title('Predicted Ratings vs True Ratings')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_gat(model, train_loader, test_loader, num_epochs, lr, device):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)  # Adam optimizer\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error loss for rating prediction\n",
    "\n",
    "    all_true_values = []  # To store true values\n",
    "    all_predicted_values = []  # To store predicted values\n",
    "\n",
    "    attention_weights_history = []  # To store only final attention weights\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        optimizer.zero_grad()  # Zero the gradients at the start of each epoch\n",
    "\n",
    "        # Training loss calculation\n",
    "        total_train_loss = 0\n",
    "        num_train_batches = 0\n",
    "\n",
    "        # Iterate over batches in the train_loader\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)  # Move to device\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            edge_outputs, attn_weights_5 = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "            # Reshape the output to match the target shape\n",
    "            edge_outputs = edge_outputs.view(-1, 1)\n",
    "\n",
    "            # Ensure target is reshaped correctly\n",
    "            target = batch.edge_attr[:, 0].view(-1, 1)  # Select only the rating part\n",
    "\n",
    "            # Store true and predicted values for plotting later\n",
    "            all_true_values.extend(target.cpu().numpy())  # Move to CPU for plotting\n",
    "            all_predicted_values.extend(edge_outputs.cpu().detach().numpy())  # Move to CPU and detach gradients\n",
    "\n",
    "            # Loss calculation\n",
    "            loss = criterion(edge_outputs, target)\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "\n",
    "        # Calculate and log the average training loss for the epoch\n",
    "        average_train_loss = total_train_loss / num_train_batches\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_train_loss:.4f}')\n",
    "\n",
    "        # Evaluate the model on test data after each epoch\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        num_test_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch = batch.to(device)\n",
    "\n",
    "                test_out, attn_weights_5 = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "                # Ensure that attn_weights_5 is the last element in the returned tuple\n",
    "                final_attn_weights = attn_weights_5  # Already part of the tuple\n",
    "\n",
    "                # Extract the attention weights from the tuple if necessary (if returned as a tensor or matrix)\n",
    "                if isinstance(final_attn_weights, torch.Tensor):\n",
    "                    final_attn_weights = final_attn_weights.cpu().detach().numpy()\n",
    "                elif isinstance(final_attn_weights, tuple):\n",
    "                    final_attn_weights = final_attn_weights[0].cpu().detach().numpy()  # If it's a tuple itself\n",
    "\n",
    "                # Store only the final attention weights for evaluation phase\n",
    "                attention_weights_history.append(final_attn_weights)\n",
    "\n",
    "                # Loss calculation for test data\n",
    "                test_out = test_out.view(-1, 1)\n",
    "                target = batch.edge_attr[:, 0].view(-1, 1)\n",
    "                test_loss = criterion(test_out, target)\n",
    "                total_test_loss += test_loss.item()\n",
    "                num_test_batches += 1\n",
    "\n",
    "        # Calculate and log the average test loss for the epoch\n",
    "        average_test_loss = total_test_loss / num_test_batches\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Test Loss: {average_test_loss:.4f}')\n",
    "\n",
    "    # After all epochs, plot the predictions\n",
    "    plot_predictions(all_true_values, all_predicted_values)\n",
    "\n",
    "    # Evaluation phase (Post-training)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)  # Move to device\n",
    "\n",
    "            # Forward pass for test data\n",
    "            test_out, attn_weights_5 = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "            # Ensure the test output is reshaped correctly\n",
    "            test_out = test_out.view(-1, 1)\n",
    "\n",
    "            # Ensure target is reshaped correctly for the test batch\n",
    "            target = batch.edge_attr[:, 0].view(-1, 1)\n",
    "\n",
    "            all_preds.append(test_out)\n",
    "            all_true.append(target)\n",
    "\n",
    "    # Concatenate all predictions and true values\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_true = torch.cat(all_true, dim=0)\n",
    "\n",
    "    # Convert predictions and targets to numpy arrays for scikit-learn \n",
    "    all_preds = all_preds.cpu().numpy()\n",
    "    all_true = all_true.cpu().numpy()\n",
    "\n",
    "    # Regression metrics (MSE and MAE)\n",
    "    mse = mean_squared_error(all_true, all_preds)\n",
    "    mae = mean_absolute_error(all_true, all_preds)    \n",
    "\n",
    "    # Print regression metrics\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "    torch.save(model.state_dict(), '../Pickle/gat_model.pth')\n",
    "\n",
    "    # Return only final attention weights and other metrics\n",
    "    return attention_weights_history, mse, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data_list = [train_data_obj]  \n",
    "test_data_list = [test_data_obj]\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_data_list, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_data_list, batch_size=8, shuffle=False)\n",
    "\n",
    "all_embeddings = train_data['embeddings'].tolist() + test_data['embeddings'].tolist()\n",
    "\n",
    "# Convert to NumPy array to ensure consistency\n",
    "all_embeddings = np.array(all_embeddings, dtype=object)\n",
    "\n",
    "# Get the size of the first valid embedding\n",
    "embedding_size = len(all_embeddings[0])\n",
    "edge_feature_dim = 1 + 1 + embedding_size  # Rating + Confidence + Embedding Size\n",
    "\n",
    "model = GATModel(\n",
    "    in_channels=train_data_obj.x.shape[1],  # Input features per node\n",
    "    hidden_channels=35,\n",
    "    out_channels=1,\n",
    "    num_heads=30,\n",
    "    edge_feature_dim=edge_feature_dim  # Correct edge feature dimension\n",
    ").to(device)  # Move model to GPU if available\n",
    "\n",
    "# Train the model\n",
    "train_gat(model, train_loader, test_loader, num_epochs=80, lr=0.001, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, attention_weight_5 = model.predict(test_data_obj.x, test_data_obj.edge_index, test_data_obj.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_ratings = predictions[0]  # Extract the ratings tensor\n",
    "predictions = denormalize_rating(predictions, min_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_prediction_histogram(predictions):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(predictions, bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title('Distribution of Predicted Ratings')\n",
    "    plt.xlabel('Predicted Ratings')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "plot_prediction_histogram(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.min().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def visualize_embeddings_with_clustering(data_obj, embed_type=\"node\", n_clusters=6):\n",
    "    if embed_type == \"node\":\n",
    "        embeddings = data_obj.x.cpu().detach().numpy()\n",
    "        title = \"PCA of Node Embeddings with Clusters\"\n",
    "    else:\n",
    "        embeddings = data_obj.edge_attr.cpu().detach().numpy()\n",
    "        title = \"t-SNE of Edge Embeddings with Clusters\"\n",
    "\n",
    "    if embeddings.shape[1] > 2:  # Apply dimensionality reduction if needed\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    else:\n",
    "        reduced_embeddings = embeddings\n",
    "\n",
    "    # Perform KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(reduced_embeddings)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings_with_clustering(train_data_obj, embed_type=\"edge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings_with_clustering(train_data_obj, embed_type=\"node\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
