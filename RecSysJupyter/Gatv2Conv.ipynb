{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_random_seed(seed_value):\n",
    "    \"\"\"Set the random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)  # Set the seed for Python's random module\n",
    "    np.random.seed(seed_value)  # Set the seed for NumPy\n",
    "    torch.manual_seed(seed_value)  # Set the seed for PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed_value)  # Set the seed for PyTorch\n",
    "    torch.cuda.manual_seed_all(seed_value)  # Set the seed for all GPUs\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior on GPU\n",
    "    torch.backends.cudnn.benchmark = False  # Turn off benchmarks for reproducibility\n",
    "\n",
    "# Set a specific random seed value (e.g., 42)\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_data():\n",
    "    reviews = pd.read_pickle('../Pickle/reviews.pkl')\n",
    "    books = pd.read_pickle('../Pickle/books.pkl')\n",
    "    read = pd.read_pickle('../Pickle/read.pkl')\n",
    "    user_genres = pd.read_pickle('../Pickle/user_most_common_genres.pkl')\n",
    "    review_embeddings = pd.read_pickle('../Pickle/review_embeddings.pkl')\n",
    "    review_sentiment = pd.read_pickle('../Pickle/review_score.pkl')\n",
    "    return reviews, books, read, user_genres, review_embeddings, review_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ratings_data, user_genres, test_size=0.1, val_size=0.15, random_state=42):\n",
    "    # Merge the ratings_data and user_genres on user_id to get the most common genres for users\n",
    "    merged_data = ratings_data.merge(user_genres[['user_id', 'most_common_genres']], on='user_id', how='inner')\n",
    "\n",
    "    # Identify users and books that appear only once in the dataset\n",
    "    user_counts = merged_data['user_id'].value_counts()\n",
    "    book_counts = merged_data['book_id'].value_counts()\n",
    "\n",
    "    # Find interactions where user or book appears only once\n",
    "    single_interactions = merged_data[\n",
    "        merged_data['user_id'].isin(user_counts[user_counts == 1].index) | \n",
    "        merged_data['book_id'].isin(book_counts[book_counts == 1].index)\n",
    "    ]\n",
    "\n",
    "    # Remove those interactions from the main dataset\n",
    "    remaining_interactions = merged_data[~merged_data.index.isin(single_interactions.index)]\n",
    "\n",
    "    # Split the remaining interactions into train, validation, and test\n",
    "    train_df, temp_data = train_test_split(remaining_interactions, test_size=test_size+val_size, random_state=random_state)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=test_size/(test_size+val_size), random_state=random_state)\n",
    "\n",
    "    # Add the single interactions to the training set\n",
    "    train_data = pd.concat([train_df, single_interactions], ignore_index=True)\n",
    "\n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ID mappings for users and books\n",
    "def initialize_id_mappings(combined_data):\n",
    "    unique_user_ids = set(combined_data['user_id'])\n",
    "    unique_book_ids = set(combined_data['book_id'])\n",
    "\n",
    "    user_id_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "    book_id_to_index = {book_id: idx for idx, book_id in enumerate(unique_book_ids)}\n",
    "\n",
    "    return user_id_to_index, book_id_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def balance_ratings(df, target_column='rating', target_count=12000):\n",
    "    balanced_data = []\n",
    "    \n",
    "    for rating in df[target_column].unique():\n",
    "        class_data = df[df[target_column] == rating]\n",
    "        original_data = class_data.copy()  # Preserve original ratings\n",
    "        \n",
    "        if len(class_data) < target_count:\n",
    "            # Oversample\n",
    "            class_data_resampled = resample(class_data, replace=True, n_samples=target_count, random_state=42)\n",
    "            \n",
    "            # Identify new duplicate rows (not in original data)\n",
    "            duplicated_mask = class_data_resampled.index.isin(original_data.index)\n",
    "            \n",
    "            # Modify only the new duplicate ratings\n",
    "            class_data_resampled.loc[~duplicated_mask, target_column] += np.random.uniform(-0.3, 0.3, size=sum(~duplicated_mask))\n",
    "        else:\n",
    "            # Under-sample\n",
    "            class_data_resampled = resample(class_data, replace=False, n_samples=target_count, random_state=42)\n",
    "        \n",
    "        balanced_data.append(class_data_resampled)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    balanced_df = pd.concat(balanced_data, axis=0).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log scale ratings\n",
    "def normalise_ratings(train_data, test_data, val_data):\n",
    "    min_rating = train_data['rating'].min()\n",
    "\n",
    "\n",
    "    train_data['rating'] = np.log1p(train_data['rating'])\n",
    "    test_data['rating'] = np.log1p(test_data['rating'])\n",
    "    val_data['rating'] = np.log1p(val_data['rating'])\n",
    "\n",
    "\n",
    "    return train_data, test_data, val_data, min_rating\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def denormalize_rating(log_scaled_ratings, min_rating):\n",
    "    log_scaled_ratings = np.asarray(log_scaled_ratings, dtype=float)  # Ensure NumPy array\n",
    "\n",
    "    # Reverse log1p transformation\n",
    "    original_ratings = np.expm1(log_scaled_ratings)\n",
    "\n",
    "    # Adjust for minimum rating\n",
    "    if min_rating:\n",
    "        original_ratings += min_rating\n",
    "\n",
    "    # Clip values between 0 and 5\n",
    "    return np.clip(original_ratings, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_book_features(filtered_data, user_id_to_index, book_id_to_index):\n",
    "    # Combine user and book genres to create a unified genre list\n",
    "    unique_genres = sorted(set(filtered_data['filtered_genres'].str.split(',').explode()).union(\n",
    "        set(filtered_data['most_common_genres'].explode())\n",
    "    ))\n",
    "    \n",
    "    # Create a common genre dictionary\n",
    "    genre_dict = {genre: idx for idx, genre in enumerate(unique_genres)}\n",
    "\n",
    "    # Prepare user genre features\n",
    "    user_genre_features = {}\n",
    "    \n",
    "    # Group by user_id and process all genres at once\n",
    "    for user_id, group in filtered_data.groupby('user_id'):\n",
    "        genres = group['most_common_genres'].iloc[0]  # All rows for this user should have the same genres\n",
    "        genre_vector = np.zeros(len(genre_dict))  # Size based on the unified genre list\n",
    "        for genre in genres:\n",
    "            if genre in genre_dict:\n",
    "                genre_vector[genre_dict[genre]] = 1\n",
    "        user_genre_features[user_id_to_index[user_id]] = torch.tensor(genre_vector, dtype=torch.float32)\n",
    "\n",
    "    # Prepare book genre features\n",
    "    book_genre_features = {}\n",
    "    for book_id, group in filtered_data.groupby('book_id'):\n",
    "        genres = group['filtered_genres'].iloc[0].split(',')  # Assuming all rows for this book have the same genres\n",
    "        genre_vector = np.zeros(len(genre_dict))  # Size based on the unified genre list\n",
    "        for genre in genres:\n",
    "            if genre in genre_dict:\n",
    "                genre_vector[genre_dict[genre]] = 1\n",
    "        book_genre_features[book_id_to_index[book_id]] = torch.tensor(genre_vector, dtype=torch.float32)\n",
    "\n",
    "    return user_genre_features, book_genre_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_edge_index_ratings_attributes(df, user_id_to_index, book_id_to_index):\n",
    "    # Map user and book IDs to indices\n",
    "    user_indices = df['user_id'].map(user_id_to_index).dropna().astype(int).values\n",
    "    book_indices = df['book_id'].map(book_id_to_index).dropna().astype(int).values\n",
    "\n",
    "    # Ensure valid mappings\n",
    "    valid_mask = (user_indices >= 0) & (book_indices >= 0)\n",
    "    user_indices = user_indices[valid_mask]\n",
    "    book_indices = book_indices[valid_mask]\n",
    "\n",
    "    # Create edge index\n",
    "    edge_index = torch.tensor([user_indices, book_indices], dtype=torch.long)\n",
    "\n",
    "    # Convert ratings and confidence scores to tensors\n",
    "    ratings_tensor = torch.tensor(df.loc[valid_mask, 'rating'].values, dtype=torch.float32).view(-1, 1)\n",
    "    confidence_tensor = torch.tensor(df.loc[valid_mask, 'confidence_score'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Efficiently convert list of NumPy arrays to a tensor\n",
    "    embeddings_np = np.stack(df.loc[valid_mask, 'embeddings'].values)  # Stack directly\n",
    "    embeddings_tensor = torch.from_numpy(embeddings_np).float()  # Convert efficiently\n",
    "\n",
    "    # Concatenate ratings, confidence scores, and embeddings into a single edge attribute tensor\n",
    "    edge_attr = torch.cat([ratings_tensor, confidence_tensor, embeddings_tensor], dim=1)\n",
    "\n",
    "    return edge_index, edge_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_objects(train_data, val_data, test_data, user_genre_features, book_genre_features, user_id_to_index, book_id_to_index):\n",
    "    train_edge_index, train_edge_attr = prepare_edge_index_ratings_attributes(\n",
    "        train_data, user_id_to_index, book_id_to_index\n",
    "    )\n",
    "    val_edge_index, val_edge_attr = prepare_edge_index_ratings_attributes(\n",
    "        val_data, user_id_to_index, book_id_to_index\n",
    "    )\n",
    "    test_edge_index, test_edge_attr = prepare_edge_index_ratings_attributes(\n",
    "        test_data, user_id_to_index, book_id_to_index\n",
    "    )\n",
    "\n",
    "    # Convert user and book genre features efficiently\n",
    "    user_embeddings = torch.from_numpy(np.stack(list(user_genre_features.values()))).float()\n",
    "    book_embeddings = torch.from_numpy(np.stack(list(book_genre_features.values()))).float()\n",
    "\n",
    "    # Combine user and book embeddings into node features\n",
    "    node_embeddings = torch.cat([user_embeddings, book_embeddings], dim=0)\n",
    "\n",
    "    # Ensure edge_index is correctly formatted\n",
    "    train_edge_index = train_edge_index.clone().detach()  # Ensure it's a tensor\n",
    "    val_edge_index = val_edge_index.clone().detach()\n",
    "    test_edge_index = test_edge_index.clone().detach()\n",
    "\n",
    "    # Create PyG Data objects\n",
    "    train_data_obj = Data(\n",
    "        x=node_embeddings,\n",
    "        edge_index=train_edge_index,\n",
    "        edge_attr=train_edge_attr  \n",
    "    )\n",
    "\n",
    "    val_data_obj = Data(\n",
    "        x=node_embeddings,\n",
    "        edge_index=val_edge_index,\n",
    "        edge_attr=val_edge_attr\n",
    "    )\n",
    "    \n",
    "    test_data_obj = Data(\n",
    "        x=node_embeddings,\n",
    "        edge_index=test_edge_index,\n",
    "        edge_attr=test_edge_attr\n",
    "    )\n",
    "\n",
    "    return train_data_obj, val_data_obj, test_data_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews, books, read, user_genres, review_embeddings, review_sentiment = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.merge(reviews, review_embeddings, on=\"review_id\", how=\"inner\")  \n",
    "reviews = pd.merge(reviews, review_sentiment, on=\"review_id\", how=\"inner\") \n",
    "reviews  = reviews[['rating', 'user_id', 'book_id', 'confidence_score', 'embeddings']]   \n",
    "books = books[['book_id', 'title', 'authors', 'filtered_genres', 'average_rating']]\n",
    "data = pd.merge(books, reviews, on='book_id', how='inner')\n",
    "data = data.reset_index(drop=True)\n",
    "user_genres = user_genres.reset_index()\n",
    "user_genres = user_genres[user_genres['most_common_genres'].apply(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wil remove after svd\n",
    "data = data[data['rating'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(user_genres[['user_id', 'most_common_genres']], on='user_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_data(data, user_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training Set Size: {len(train_data)}')\n",
    "print(f'Validation Set Size: {len(val_data)}')\n",
    "print(f'Test Set Size: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_to_index, book_id_to_index = initialize_id_mappings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = balance_ratings(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, val_data, min_rating = normalise_ratings(train_data, test_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_genre_features, book_genre_features = user_book_features(data, user_id_to_index, book_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_obj, val_data_obj, test_data_obj = prepare_data_objects(\n",
    "    train_data, val_data, test_data, user_genre_features, book_genre_features, user_id_to_index, book_id_to_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../Pickle/user_id_to_index_gat.pkl', 'wb') as f:\n",
    "    pickle.dump(user_id_to_index, f)\n",
    "\n",
    "with open('../Pickle/book_id_to_index_gat.pkl', 'wb') as f:\n",
    "    pickle.dump(book_id_to_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads, edge_feature_dim, dropout_rate=0.1):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.dropout_rate = dropout_rate  # Dropout rate\n",
    "\n",
    "        # Define the GAT layers\n",
    "        self.gat1 = GATv2Conv(in_channels, hidden_channels, heads=num_heads, edge_dim=edge_feature_dim)\n",
    "        self.gat2 = GATv2Conv(hidden_channels * num_heads, hidden_channels, heads=num_heads, edge_dim=edge_feature_dim)\n",
    "        self.gat3 = GATv2Conv(hidden_channels * num_heads, hidden_channels, heads=num_heads, edge_dim=edge_feature_dim)\n",
    "        self.gat4 = GATv2Conv(hidden_channels * num_heads, hidden_channels, heads=num_heads, edge_dim=edge_feature_dim)\n",
    "        self.gat5 = GATv2Conv(hidden_channels * num_heads, out_channels, heads=num_heads, concat=False, edge_dim=edge_feature_dim)\n",
    "\n",
    "        # Batch Normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels * num_heads)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels * num_heads)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_channels * num_heads)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_channels * num_heads)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.gat1(x, edge_index, edge_attr)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = F.elu(x)  # Activation function\n",
    "        x = self.dropout(x)  # Dropout after activation\n",
    "\n",
    "        x = self.gat2(x, edge_index, edge_attr)\n",
    "        x = self.bn2(x)  \n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.gat3(x, edge_index, edge_attr)\n",
    "        x = self.bn3(x) \n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.gat4(x, edge_index, edge_attr)\n",
    "        x = self.bn4(x)  \n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.gat5(x, edge_index, edge_attr)  \n",
    "\n",
    "       \n",
    "        edge_outputs = torch.sum(x[edge_index[0]] * x[edge_index[1]], dim=-1)  # Index nodes by edge indices\n",
    "\n",
    "        return edge_outputs, x\n",
    "\n",
    "\n",
    "    def predict(self, x, edge_index, edge_attr):\n",
    "        self.eval()  \n",
    "        with torch.no_grad():  \n",
    "            edge_outputs, node_embeddings = self.forward(x, edge_index, edge_attr)\n",
    "            return node_embeddings, edge_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data_list = [train_data_obj]  \n",
    "test_data_list = [test_data_obj]\n",
    "val_data_list = [val_data_obj]\n",
    "\n",
    "train_loader = DataLoader(train_data_list, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_data_list, batch_size=8, shuffle=False)\n",
    "val_loader = DataLoader(val_data_list, batch_size=8, shuffle=False)\n",
    "\n",
    "all_embeddings = train_data['embeddings'].tolist() + test_data['embeddings'].tolist()\n",
    "\n",
    "all_embeddings = np.array(all_embeddings, dtype=object)\n",
    "\n",
    "embedding_size = len(all_embeddings[0])\n",
    "edge_feature_dim = 1 + 1 + embedding_size  # Rating + Confidence + Embedding Size\n",
    "\n",
    "model = GATModel(\n",
    "    in_channels=train_data_obj.x.shape[1],  # Input features per node\n",
    "    hidden_channels=35,\n",
    "    out_channels=1,\n",
    "    num_heads=30,\n",
    "    edge_feature_dim=edge_feature_dim  # Correct edge feature dimension\n",
    ").to(device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(predictions, true_values):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(true_values, predictions, alpha=0.5, color='blue', label=\"Predictions vs True\")\n",
    "    plt.plot([min(true_values), max(true_values)], [min(true_values), max(true_values)], 'r--', label='Perfect Prediction')\n",
    "    plt.xlabel('True Ratings')\n",
    "    plt.ylabel('Predicted Ratings')\n",
    "    plt.title('Predicted Ratings vs True Ratings')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, delta):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def __call__(self, val_loss, epoch):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_epoch = epoch\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def plot_loss(train_losses, test_losses):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training vs Test Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gat(model, train_loader, val_loader, test_loader, num_epochs, lr, device):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # Learning rate scheduler\n",
    "    criterion = nn.MSELoss()\n",
    "    early_stopping = EarlyStopping(patience=10, delta=0.0001)\n",
    "\n",
    "    all_true_values = []\n",
    "    all_predicted_values = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        num_train_batches = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            edge_outputs, embeddings = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            edge_outputs = edge_outputs.view(-1, 1)\n",
    "            target = batch.edge_attr[:, 0].view(-1, 1)\n",
    "\n",
    "            all_true_values.extend(target.cpu().numpy())\n",
    "            all_predicted_values.extend(edge_outputs.cpu().detach().numpy())\n",
    "\n",
    "            loss = criterion(edge_outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "\n",
    "        average_train_loss = total_train_loss / num_train_batches\n",
    "        train_losses.append(average_train_loss)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_train_loss:.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        num_val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                val_out, _ = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                val_out = val_out.view(-1, 1)\n",
    "                target = batch.edge_attr[:, 0].view(-1, 1)\n",
    "                val_loss = criterion(val_out, target)\n",
    "                total_val_loss += val_loss.item()\n",
    "                num_val_batches += 1\n",
    "\n",
    "        average_val_loss = total_val_loss / num_val_batches\n",
    "        val_losses.append(average_val_loss)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {average_val_loss:.4f}')\n",
    "\n",
    "        if average_val_loss < early_stopping.best_loss:\n",
    "            best_model_state = model.state_dict()\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(best_model_state, '../Pickle/gat_model.pth')\n",
    "\n",
    "        early_stopping(average_val_loss, epoch + 1)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}. Best model was at epoch {best_epoch}.\")\n",
    "            break\n",
    "\n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "    plot_loss(train_losses, val_losses)\n",
    "    model.load_state_dict(torch.load('../Pickle/gat_model.pth'))\n",
    "    print(f\"Loaded best model from epoch {best_epoch} for final evaluation.\")\n",
    "\n",
    "    # Evaluation phase using the test set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    all_embeddings = [] \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            test_out, embeddings = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            test_out = test_out.view(-1, 1)\n",
    "            target = batch.edge_attr[:, 0].view(-1, 1)\n",
    "            all_preds.append(test_out)\n",
    "            all_true.append(target)\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0).cpu().numpy()\n",
    "    all_true = torch.cat(all_true, dim=0).cpu().numpy()\n",
    "    all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "    mse = mean_squared_error(all_true, all_preds)\n",
    "    mae = mean_absolute_error(all_true, all_preds)\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "\n",
    "    return mse, mae, all_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, mae, all_embeddings = train_gat(model, train_loader, val_loader, test_loader, num_epochs=150, lr=0.00001, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings, predicted_ratings = model.predict(test_data_obj.x, test_data_obj.edge_index, test_data_obj.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/gat_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(all_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.asarray(predicted_ratings, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = denormalize_rating(predictions, min_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_recall_f1(user_true_ratings, user_predictions, k, threshold=4): # books rated four and above are considered relevant\n",
    "    precision_total, recall_total, f1_total = 0.0, 0.0, 0.0\n",
    "    num_users = len(user_true_ratings)\n",
    "\n",
    "    for user_id in user_true_ratings:\n",
    "        if user_id not in user_predictions:\n",
    "            continue  # Skip users without predictions \n",
    "\n",
    "        # Get top-k predicted books\n",
    "        top_k_pred = [book for book, _ in user_predictions[user_id][:k]]\n",
    "\n",
    "        # Get relevant books from the true ratings (those with rating >= threshold)\n",
    "        relevant_books = {book for book, rating in user_true_ratings[user_id] if rating >= threshold}\n",
    "\n",
    "        # True positives: Items correctly recommended\n",
    "        hits = sum(1 for book in top_k_pred if book in relevant_books)\n",
    "\n",
    "        precision = hits / k\n",
    "\n",
    "        recall = hits / len(relevant_books) if len(relevant_books) > 0 else 0.0\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "        # Sum up for averaging\n",
    "        precision_total += precision\n",
    "        recall_total += recall\n",
    "        f1_total += f1\n",
    "\n",
    "    # Average across users\n",
    "    precision_avg = precision_total / num_users\n",
    "    recall_avg = recall_total / num_users\n",
    "    f1_avg = f1_total / num_users\n",
    "\n",
    "    return precision_avg, recall_avg, f1_avg\n",
    "\n",
    "def compute_nDCG(user_true_ratings, user_predictions, k):\n",
    "    nDCG_total = 0.0\n",
    "    num_users = len(user_true_ratings)\n",
    "\n",
    "    for user_id in user_true_ratings:\n",
    "        if user_id not in user_predictions:\n",
    "            continue  # Skip users without predictions\n",
    "\n",
    "        # Get top-k predicted books\n",
    "        top_k_pred = [book for book, _ in user_predictions[user_id][:k]]\n",
    "\n",
    "        # Get actual ratings sorted by predicted order\n",
    "        true_ratings_sorted = [rating for book, rating in user_true_ratings[user_id] if book in top_k_pred]\n",
    "\n",
    "        # Compute DCG\n",
    "        dcg = sum((2 ** rating - 1) / np.log2(i + 2) for i, rating in enumerate(true_ratings_sorted))\n",
    "\n",
    "        # Compute ideal DCG (best possible ranking)\n",
    "        ideal_ratings_sorted = sorted([rating for _, rating in user_true_ratings[user_id]], reverse=True)[:k]\n",
    "        idcg = sum((2 ** rating - 1) / np.log2(i + 2) for i, rating in enumerate(ideal_ratings_sorted))\n",
    "\n",
    "        # Normalize\n",
    "        nDCG = dcg / idcg if idcg > 0 else 0.0\n",
    "        nDCG_total += nDCG\n",
    "\n",
    "    return nDCG_total / num_users\n",
    "\n",
    "def evaluate_metrics(user_predictions, user_true_ratings, k=10):\n",
    "    precision, recall, f1 = compute_precision_recall_f1(user_true_ratings, user_predictions, k)\n",
    "    nDCG_score = compute_nDCG(user_true_ratings, user_predictions, k)\n",
    "\n",
    "    print(f\"Precision@{k}: {precision:.4f}\")\n",
    "    print(f\"Recall@{k}: {recall:.4f}\")\n",
    "    print(f\"F1-score@{k}: {f1:.4f}\")\n",
    "    print(f\"nDCG@{k}: {nDCG_score:.4f}\")\n",
    "\n",
    "    return precision, recall, f1, nDCG_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = test_data_obj.edge_index[0].cpu().numpy()  # Users in test set\n",
    "book_ids = test_data_obj.edge_index[1].cpu().numpy()  # Books in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = test_data_obj.edge_attr[:, 0].cpu().numpy()  # True ratings from the test data\n",
    "true_values = np.expm1(true_values)  # Reverse np.log1p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_predictions = defaultdict(list)\n",
    "user_true_ratings = defaultdict(list)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    user_predictions[user_ids[i]].append((book_ids[i], predictions[i]))  # Store (book, predicted rating)\n",
    "    user_true_ratings[user_ids[i]].append((book_ids[i], true_values[i]))  # Store (book, actual rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_id in user_predictions:\n",
    "    user_predictions[user_id].sort(key=lambda x: x[1], reverse=True)  # Sort by predicted rating\n",
    "    user_true_ratings[user_id].sort(key=lambda x: x[1], reverse=True)  # Sort by actual rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1, nDCG_score = evaluate_metrics(user_predictions, user_true_ratings, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mrr(user_predictions, user_true_ratings, threshold=4.0, k=10):\n",
    "    mrr = 0.0\n",
    "    user_count = 0\n",
    "\n",
    "    for user_id in user_predictions:\n",
    "        pred_books = [book for book, _ in user_predictions[user_id][:k]]  # Get top-K book IDs\n",
    "        true_books = {book for book, rating in user_true_ratings[user_id] if rating >= threshold}  # Relevant books\n",
    "\n",
    "        for rank, book in enumerate(pred_books, start=1):\n",
    "            if book in true_books:  # Found a relevant book\n",
    "                mrr += 1 / rank\n",
    "                break  # Only consider the first relevant item\n",
    "        \n",
    "        user_count += 1  # Track valid users\n",
    "\n",
    "    return mrr / user_count if user_count > 0 else 0.0  # Avoid division by zero\n",
    "\n",
    "mrr_score = compute_mrr(user_predictions, user_true_ratings, threshold=4.5, k=5)\n",
    "print(f\"MRR@5: {mrr_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_prediction_histogram(predictions):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(predictions, bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title('Distribution of Predicted Ratings')\n",
    "    plt.xlabel('Predicted Ratings')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "plot_prediction_histogram(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.min().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_embeddings_with_clustering(data_obj, embed_type=\"node\", n_clusters=6):\n",
    "    if embed_type == \"node\":\n",
    "        embeddings = data_obj.x.cpu().detach().numpy()\n",
    "        title = \"PCA of Node Embeddings with Clusters\"\n",
    "    else:\n",
    "        embeddings = data_obj.edge_attr.cpu().detach().numpy()\n",
    "        title = \"t-SNE of Edge Embeddings with Clusters\"\n",
    "\n",
    "    if embeddings.shape[1] > 2:  # Apply dimensionality reduction if needed\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    else:\n",
    "        reduced_embeddings = embeddings\n",
    "\n",
    "    # Perform KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(reduced_embeddings)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings_with_clustering(train_data_obj, embed_type=\"edge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings_with_clustering(train_data_obj, embed_type=\"node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_node_embeddings(user_genre_features, book_genre_features):\n",
    "    # Convert user and book genre features into numpy arrays\n",
    "    all_user_features = torch.stack(list(user_genre_features.values())).cpu().numpy()\n",
    "    all_book_features = torch.stack(list(book_genre_features.values())).cpu().numpy()\n",
    "\n",
    "    # Combine user and book features for dimensionality reduction\n",
    "    all_features = np.concatenate([all_user_features, all_book_features], axis=0)\n",
    "\n",
    "    # Option 1: Use PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_features_pca = pca.fit_transform(all_features)\n",
    "\n",
    "    # Option 2: Use t-SNE for dimensionality reduction (can capture nonlinear relationships)\n",
    "    tsne = TSNE(n_components=2)\n",
    "    reduced_features_tsne = tsne.fit_transform(all_features)\n",
    "\n",
    "    # Plot PCA reduced features\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(reduced_features_pca[:, 0], reduced_features_pca[:, 1], c='blue', label='Embeddings')\n",
    "    plt.title(\"2D Projection of User and Book Features (PCA)\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot t-SNE reduced features\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(reduced_features_tsne[:, 0], reduced_features_tsne[:, 1], c='red', label='Embeddings')\n",
    "    plt.title(\"2D Projection of User and Book Features (t-SNE)\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_node_embeddings(user_genre_features, book_genre_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot the distribution of adjusted ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_data['rating'], bins=5)\n",
    "plt.title('Distribution of Ratings In GATv2Conv Train Data')\n",
    "plt.xlabel('Adjusted Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot the distribution of adjusted ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(test_data['rating'], bins=5)\n",
    "plt.title('Distribution of Ratings In GATv2Conv Test Data')\n",
    "plt.xlabel('Adjusted Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
