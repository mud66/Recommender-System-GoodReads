{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from surprise import Reader, Dataset, SVD, accuracy\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "\n",
    "def load_data(file_path):\n",
    "    return pd.read_pickle(file_path)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    users_with_only_zeros = df.groupby('user_id')['rating'].apply(lambda x: (x == 0).all())\n",
    "    users_with_only_zeros = users_with_only_zeros[users_with_only_zeros].index\n",
    "    df = df[~df['user_id'].isin(users_with_only_zeros)]\n",
    "    \n",
    "    rated_books = df[df['is_read'] != 0]\n",
    "    rated_books = rated_books[rated_books['rating'] != 0].reset_index(drop=True)\n",
    "    return rated_books\n",
    "\n",
    "def split_data(df):\n",
    "    user_counts = df['user_id'].value_counts()\n",
    "    book_counts = df['book_id'].value_counts()\n",
    "    single_interactions = df[df['user_id'].isin(user_counts[user_counts == 1].index) |\n",
    "                             df['book_id'].isin(book_counts[book_counts == 1].index)]\n",
    "    remaining_interactions = df[~df.index.isin(single_interactions.index)]\n",
    "    train_df, test_df = train_test_split(remaining_interactions, test_size=0.2, random_state=42, stratify=remaining_interactions['rating'])\n",
    "    train_df = pd.concat([train_df, single_interactions], ignore_index=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "def upsample_ratings(train_df):\n",
    "    rating_counts = train_df['rating'].value_counts()\n",
    "    majority_count = rating_counts.max()\n",
    "    modified_dfs = []\n",
    "    for rating, count in rating_counts.items():\n",
    "        class_df = train_df[train_df['rating'] == rating]\n",
    "        num_duplicates = int(majority_count / count)\n",
    "        duplicated_df = pd.concat([class_df] * num_duplicates, ignore_index=True)\n",
    "        duplicated_df['rating'] = duplicated_df['rating'] + np.random.uniform(-0.1, 0.1, size=duplicated_df.shape[0])\n",
    "        duplicated_df['rating'] = duplicated_df['rating'].clip(1, 5)\n",
    "        modified_dfs.append(duplicated_df)\n",
    "    return pd.concat(modified_dfs)\n",
    "\n",
    "def normalize_ratings(train_df, test_df):\n",
    "    min_rating = train_df['rating'].min()\n",
    "    train_df['rating'] = np.log1p(train_df['rating'])\n",
    "    test_df['rating'] = np.log1p(test_df['rating'])\n",
    "    return train_df, test_df, min_rating\n",
    "\n",
    "def denormalize_rating(log_scaled_ratings, min_rating=0):\n",
    "    original_ratings = np.expm1(np.asarray(log_scaled_ratings, dtype=float))\n",
    "    return np.clip(original_ratings + min_rating, 0, 5)\n",
    "\n",
    "def save_model(model, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def predict_batches(model, test_df, batch_size=5000):\n",
    "    reader = Reader(rating_scale=(test_df['rating'].min(), test_df['rating'].max()))\n",
    "    test_data = Dataset.load_from_df(test_df[['user_id', 'book_id', 'rating']], reader)\n",
    "    testset = test_data.build_full_trainset().build_testset()\n",
    "    predictions = []\n",
    "    for i in range(0, len(testset), batch_size):\n",
    "        batch = testset[i:i+batch_size]\n",
    "        predictions.extend(model.test(batch))\n",
    "    return predictions\n",
    "\n",
    "def precision_recall_ndcg_at_k(predictions, k=5, threshold=4.5):\n",
    "    def dcg_at_k(scores):\n",
    "        return sum((2**rel - 1) / log2(idx + 2) for idx, rel in enumerate(scores))\n",
    "    \n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "    for user_ratings in user_est_true.values():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        n_rel = sum(true_r >= threshold for _, true_r in user_ratings)\n",
    "        n_rec_k = sum(est >= threshold for est, _ in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum((true_r >= threshold) and (est >= threshold) for est, true_r in user_ratings[:k])\n",
    "        precision = n_rel_and_rec_k / n_rec_k if n_rec_k > 0 else 0\n",
    "        recall = n_rel_and_rec_k / n_rel if n_rel > 0 else 0\n",
    "        actual = [true_r for _, true_r in user_ratings[:k]]\n",
    "        ideal = sorted([true_r for _, true_r in user_ratings], reverse=True)[:k]\n",
    "        ndcg = dcg_at_k(actual) / dcg_at_k(ideal) if dcg_at_k(ideal) > 0 else 0\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "    return np.mean(precisions), np.mean(recalls), np.mean(ndcgs)\n",
    "\n",
    "def plot_histogram(data, title, xlabel):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(data, bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "def plot_residuals(true_ratings, estimated_ratings):\n",
    "    residuals = true_ratings - estimated_ratings\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(true_ratings, residuals, alpha=0.5, edgecolors='k')\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xlabel(\"True Ratings\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(\"Residual Plot\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('../Pickle/read.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = upsample_ratings(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, min_rating = normalize_ratings(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df), chunk_size):\n\u001b[0;32m     13\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i:i\u001b[38;5;241m+\u001b[39mchunk_size]\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     rows\u001b[38;5;241m.\u001b[39mextend(chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m     cols\u001b[38;5;241m.\u001b[39mextend(chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from surprise import Dataset, Reader\n",
    "\n",
    "ratings = coo_matrix((df['rating'], (df['user_id'], df['book_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset from the sparse matrix\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[['user_id', 'book_id', 'rating']], reader)\n",
    "trainset = data.build_full_trainset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m SVD(n_factors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, lr_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, reg_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m trainset \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mbuild_full_trainset()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "model = SVD(n_factors=200, n_epochs=60, lr_all=0.01, reg_all=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 61):\n",
    "    model.fit(trainset)\n",
    "    print(f\"Epoch {epoch}/{60} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, '../Pickle/svd_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_batches(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, ndcg = precision_recall_ndcg_at_k(predictions)\n",
    "print(f'Precision@5: {precision:.4f}, Recall@5: {recall:.4f}, nDCG@5: {ndcg:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
