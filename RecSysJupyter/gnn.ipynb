{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    reviews = pd.read_pickle('../Pickle/reviews.pkl')\n",
    "    books = pd.read_pickle('../Pickle/books.pkl')\n",
    "    read = pd.read_pickle('../Pickle/read.pkl')\n",
    "    review_embeddings = pd.read_pickle('../Pickle/review_embeddings.pkl')\n",
    "    user_genres = pd.read_pickle('../Pickle/user_most_common_genres.pkl')\n",
    "    return reviews, books, read, review_embeddings, user_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def split_data(data, book_id_to_index, test_size=0.2):\n",
    "    # Ensure that only books in the book_id_to_index are considered\n",
    "    available_books = set(book_id_to_index.keys())\n",
    "    data = data[data['book_id'].isin(available_books)]  # Filter books to only those in train\n",
    "\n",
    "    # Initialize lists to store train and test data\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "\n",
    "    # Group data by user_id\n",
    "    for user_id, user_data in data.groupby('user_id'):\n",
    "        # Split the books into training and testing sets for each user\n",
    "        books = list(user_data['book_id'].unique())\n",
    "        train_books, test_books = train_test_split(books, test_size=test_size, random_state=42)\n",
    "\n",
    "        # Use the pre-split books to filter the user data for train/test\n",
    "        user_train_data = user_data[user_data['book_id'].isin(train_books)]\n",
    "        user_test_data = user_data[user_data['book_id'].isin(test_books)]\n",
    "\n",
    "        # Append user data to corresponding lists\n",
    "        train_dfs.append(user_train_data)\n",
    "        test_dfs.append(user_test_data)\n",
    "\n",
    "    # Concatenate all the user-specific dataframes into one dataframe each for train and test\n",
    "    train_data = pd.concat(train_dfs, ignore_index=True)\n",
    "    test_data = pd.concat(test_dfs, ignore_index=True)\n",
    "\n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ratings(train_data, test_data):\n",
    "    # Calculate mean and standard deviation from the training set\n",
    "    mean_rating = train_data['rating'].mean()\n",
    "    std_rating = train_data['rating'].std()\n",
    "\n",
    "    # Normalize the ratings in both the training and test sets using the training set statistics\n",
    "    train_data['rating'] = (train_data['rating'] - mean_rating) / std_rating\n",
    "    test_data['rating'] = (test_data['rating'] - mean_rating) / std_rating\n",
    "\n",
    "    return train_data, test_data, mean_rating, std_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def upsample_lower_classes(data, target_column='rating', minority_classes=[0, 1, 2], n_samples=1000):\n",
    "    # Separate majority and minority class data\n",
    "    majority_class_data = data[~data[target_column].isin(minority_classes)]\n",
    "    \n",
    "    # Up-sample each minority class by n_samples\n",
    "    upsampled_minority_data = []\n",
    "    for class_label in minority_classes:\n",
    "        class_data = data[data[target_column] == class_label]\n",
    "        upsampled_class_data = resample(class_data, replace=True, n_samples=n_samples, random_state=42)\n",
    "        upsampled_minority_data.append(upsampled_class_data)\n",
    "\n",
    "    # Combine the majority class data with up-sampled minority class data\n",
    "    upsampled_data = pd.concat([majority_class_data] + upsampled_minority_data)\n",
    "\n",
    "    return upsampled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_id_mappings(combined_data):\n",
    "    # Combine the unique user IDs and book IDs from both train and test data\n",
    "    unique_user_ids = set(combined_data['user_id'])\n",
    "    unique_book_ids = set(combined_data['book_id'])\n",
    "\n",
    "    # Create mappings from user_id and book_id to indices\n",
    "    user_id_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "    book_id_to_index = {book_id: idx for idx, book_id in enumerate(unique_book_ids)}\n",
    "\n",
    "    return user_id_to_index, book_id_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_edge_index_ratings_attributes(df, user_id_to_index, book_id_to_index):\n",
    "    edge_index = []\n",
    "    ratings = []\n",
    "    edge_attrs = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        book_id = row['book_id']\n",
    "        rating = row['rating']\n",
    "        embedding = row['embeddings']  # Review embedding as edge attribute\n",
    "\n",
    "        # Map user_id and book_id to indices\n",
    "        user_idx = user_id_to_index.get(user_id)\n",
    "        book_idx = book_id_to_index.get(book_id)\n",
    "\n",
    "        if user_idx is None or book_idx is None:\n",
    "            continue\n",
    "\n",
    "        edge_index.append([user_idx, book_idx])\n",
    "        ratings.append(rating)\n",
    "        edge_attrs.append(embedding)\n",
    "\n",
    "    # Convert edge_attrs to numpy array before tensor conversion (if needed)\n",
    "    edge_attrs = np.array(edge_attrs)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    ratings_tensor = torch.tensor(ratings, dtype=torch.float32)\n",
    "    edge_attrs_tensor = torch.tensor(edge_attrs, dtype=torch.float32)\n",
    "\n",
    "    print(f\"Edge index shape: {edge_index.shape}\")\n",
    "    print(f\"Ratings tensor shape: {ratings_tensor.shape}\")\n",
    "    print(f\"Edge attributes tensor shape: {edge_attrs_tensor.shape}\")\n",
    "\n",
    "    return edge_index, ratings_tensor, edge_attrs_tensor\n",
    "\n",
    "def prepare_data_objects(train_data, test_data, user_id_to_index, book_id_to_index, user_embedding_dim=64, book_embedding_dim=64):\n",
    "    # Prepare edge index, ratings, and edge attributes for training and testing\n",
    "    train_edge_index, train_ratings_tensor, train_edge_attrs = prepare_edge_index_ratings_attributes(\n",
    "        train_data, user_id_to_index, book_id_to_index\n",
    "    )\n",
    "    test_edge_index, test_ratings_tensor, test_edge_attrs = prepare_edge_index_ratings_attributes(\n",
    "        test_data, user_id_to_index, book_id_to_index\n",
    "    )\n",
    "\n",
    "    # Initialize random embeddings for users and books (separate dimensions)\n",
    "    num_users = len(user_id_to_index)\n",
    "    num_books = len(book_id_to_index)\n",
    "    \n",
    "    # Generate random embeddings for users and books\n",
    "    user_embeddings = torch.randn(num_users, user_embedding_dim)  # Random embeddings for users\n",
    "    book_embeddings = torch.randn(num_books, book_embedding_dim)  # Random embeddings for books\n",
    "    \n",
    "    # Concatenate user and book embeddings\n",
    "    # Debug: Check the size of concatenated embeddings\n",
    "    print(f\"User embeddings shape: {user_embeddings.shape}\")\n",
    "    print(f\"Book embeddings shape: {book_embeddings.shape}\")\n",
    "\n",
    "# Concatenate user and book embeddings\n",
    "    node_embeddings = torch.cat([user_embeddings, book_embeddings], dim=0)  # Combined user and book embeddings\n",
    "\n",
    "# Debug: Verify concatenated shape\n",
    "    print(f\"Concatenated node embeddings shape: {node_embeddings.shape}\")\n",
    "\n",
    "    # Create Data objects with edge attributes included\n",
    "    train_data_obj = Data(\n",
    "        x=node_embeddings,  # Assign node features (separate user/book embeddings)\n",
    "        edge_index=train_edge_index,\n",
    "        y=train_ratings_tensor,\n",
    "        edge_attr=train_edge_attrs\n",
    "    )\n",
    "    test_data_obj = Data(\n",
    "        x=node_embeddings,  # Assign node features (separate user/book embeddings)\n",
    "        edge_index=test_edge_index,\n",
    "        y=test_ratings_tensor,\n",
    "        edge_attr=test_edge_attrs\n",
    "    )\n",
    "\n",
    "    return train_data_obj, test_data_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews, books, read, review_embeddings, user_genres = load_data()\n",
    "books = books[['book_id', 'title', 'authors', 'filtered_genres']]\n",
    "data = pd.merge(read, books, on='book_id')\n",
    "data = data[['user_id', 'book_id', 'rating', 'title', 'authors', 'filtered_genres']]\n",
    "data = data.reset_index(drop=True)\n",
    "user_genres = user_genres.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interaction_counts = data['user_id'].value_counts()\n",
    "eligible_users = user_interaction_counts[user_interaction_counts >= 5].index\n",
    "data = data[data['user_id'].isin(eligible_users)]\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_to_index, book_id_to_index = initialize_id_mappings(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_data(combined_data, book_id_to_index, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = upsample_lower_classes(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_obj, test_data_obj = prepare_data_objects(train_data, test_data, user_id_to_index, book_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "# Define the model parameters\n",
    "in_channels = train_data_obj.x.shape[1]  # Number of features per node\n",
    "out_channels = 1  # Single output for regression (e.g., predicted rating)\n",
    "hidden_channels = 12\n",
    "num_heads = 4\n",
    "lr = 0.0001\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, num_users, num_books, num_features):\n",
    "        super(GATModel, self).__init__()\n",
    "        \n",
    "        # Define embeddings for users and books\n",
    "        self.user_embedding = nn.Embedding(num_users, num_features)\n",
    "        self.book_embedding = nn.Embedding(num_books, num_features)\n",
    "        \n",
    "        # Define GAT layers\n",
    "        self.gat1 = GATConv(num_features, 8)  # First GAT layer (num_features -> 8)\n",
    "        self.gat2 = GATConv(8, 1)  # Second GAT layer (8 -> 1, for regression output)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Extract node features and edge index from the data object\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = x.long()  # Ensure the indices are of type Long\n",
    "        \n",
    "        # Apply embeddings to user and book\n",
    "        x_user = self.user_embedding(x[:len(user_id_to_index)])  # User embeddings\n",
    "        x_book = self.book_embedding(x[len(user_id_to_index):])  # Book embeddings\n",
    "        \n",
    "        # Concatenate user and book embeddings\n",
    "        x = torch.cat([x_user, x_book], dim=0)\n",
    "        \n",
    "        # Apply GAT layers\n",
    "        x = self.gat1(x, edge_index)  # First GAT layer\n",
    "        x = F.elu(x)\n",
    "        x = self.gat2(x, edge_index)  # Second GAT layer (output layer)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Model setup\n",
    "model = GATModel(in_channels, hidden_channels, out_channels=1, num_heads=num_heads)  # Output a single value for regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = torch.nn.MSELoss()  # MSE loss for regression\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "def train(model, train_data_obj, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure the input data to the model\n",
    "        edge_index = train_data_obj.edge_index\n",
    "        edge_attr = train_data_obj.edge_attr\n",
    "        x = train_data_obj.x  # Ensure features are present\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(x, edge_index, edge_attr)\n",
    "        print(f\"Output shape: {out.shape}\")\n",
    "\n",
    "        \n",
    "        # Ensure output and target (y) are the same shape\n",
    "        y = train_data_obj.y.view(-1, 1)  # Reshape target to match output shape\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "# Validate the model\n",
    "def validate(model, data):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss()  # Use MSELoss for regression\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)  # Forward pass with the entire graph\n",
    "        \n",
    "        # Debugging the shapes of out and y\n",
    "        print(f\"Validation output shape: {out.shape}\")\n",
    "        print(f\"Target shape: {data.y.view(-1, 1).shape}\")\n",
    "        \n",
    "        val_loss = criterion(out, data.y.view(-1, 1))  # Ensure y is the correct shape\n",
    "    return val_loss.item()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train(model, train_data_obj, optimizer, criterion, num_epochs)\n",
    "\n",
    "# Validate the model\n",
    "test_loss = validate(model, test_data_obj)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_predictions(predictions, mean_rating, std_rating):\n",
    "    # Rescale and clip\n",
    "    rescaled = (predictions * std_rating) + mean_rating\n",
    "    return np.clip(rescaled, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(r, k):\n",
    "    \"\"\"Calculates precision at k\"\"\"\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.sum(r) / k\n",
    "\n",
    "def recall_at_k(r, k, all_positives):\n",
    "    \"\"\"Calculates recall at k\"\"\"\n",
    "    r = np.asarray(r)[:k]\n",
    "    if all_positives == 0:\n",
    "        return 0\n",
    "    return np.sum(r) / all_positives\n",
    "\n",
    "def ndcg_at_k(actual_sorted, k):\n",
    "    actual_sorted_padded = np.pad(actual_sorted, (0, max(0, k - len(actual_sorted))), 'constant')\n",
    "    ideal_sorted = np.pad(np.sort(actual_sorted)[::-1], (0, max(0, k - len(actual_sorted))), 'constant')\n",
    "    ideal_dcg = np.sum(ideal_sorted[:k] / np.log2(np.arange(2, k + 2)))\n",
    "    dcg = np.sum(actual_sorted_padded[:k] / np.log2(np.arange(2, k + 2)))\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "def evaluate(data, model, target_ratings, k, mean_rating, std_rating):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Model prediction and rescaling\n",
    "        out = model.predict(data.x, data.edge_index).squeeze()\n",
    "        predicted_ratings = out[data.edge_index[0]].numpy()\n",
    "        rescaled_predictions = rescale_predictions(predicted_ratings, mean_rating, std_rating)\n",
    "        rounded_predictions = np.round(rescaled_predictions)\n",
    "\n",
    "        # Convert actual ratings to binary relevance\n",
    "        actual_ratings = (target_ratings >= 3).float().numpy()\n",
    "        user_ids = data.edge_index[0].numpy()\n",
    "\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        ndcg_scores = []\n",
    "\n",
    "        for user_id in np.unique(user_ids):\n",
    "            user_indices = user_ids == user_id\n",
    "            actual = actual_ratings[user_indices]\n",
    "            predicted = rounded_predictions[user_indices]\n",
    "\n",
    "            sorted_indices = np.argsort(predicted)[::-1]\n",
    "            actual_sorted = actual[sorted_indices]\n",
    "\n",
    "            precision = precision_at_k(actual_sorted, k)\n",
    "            recall = recall_at_k(actual_sorted, k, np.sum(actual))\n",
    "            ndcg = ndcg_at_k(actual_sorted, k)\n",
    "\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            ndcg_scores.append(ndcg)\n",
    "\n",
    "        mean_precision = np.mean(precision_scores)\n",
    "        mean_recall = np.mean(recall_scores)\n",
    "        mean_ndcg = np.mean(ndcg_scores)\n",
    "\n",
    "        print(f'Average Precision@{k}: {mean_precision:.4f}')\n",
    "        print(f'Average Recall@{k}: {mean_recall:.4f}')\n",
    "        print(f'Average NDCG@{k}: {mean_ndcg:.4f}')\n",
    "\n",
    "evaluate(test_data_obj, model, test_ratings_tensor, k=10, mean_rating=mean_rating, std_rating=std_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(data, rescaled_predictions, user_id, user_id_to_index, book_id_to_index, books, top_n):\n",
    "    # Map user ID to internal index\n",
    "    user_index = user_id_to_index[user_id]\n",
    "\n",
    "    # Get all the edges that correspond to the user_id in combined_data (i.e., user-item interactions)\n",
    "    user_edges = data.edge_index[0] == user_index  # Get all the user-item edges where the user is involved\n",
    "    user_rated_books = data.edge_index[1][user_edges].cpu().numpy()  # Get the book indices for those edges\n",
    "\n",
    "    # All books present in the dataset (combined_data)\n",
    "    all_books = list(book_id_to_index.keys())\n",
    "\n",
    "    # Find books that the user has not rated (i.e., not in the user_rated_books)\n",
    "    unread_books_indices = [\n",
    "        i for i, book in enumerate(all_books) if book_id_to_index[book] not in user_rated_books\n",
    "    ]\n",
    "    \n",
    "    if len(unread_books_indices) == 0:\n",
    "        return []  # Return empty list if no unread books\n",
    "\n",
    "    # Map book IDs to the corresponding indices for rescaled_predictions\n",
    "    unread_books_indices_mapped = [book_id_to_index[all_books[i]] for i in unread_books_indices]\n",
    "\n",
    "    # Ensure that unread_books_indices maps to actual indices of predictions\n",
    "    unread_books_ratings = rescaled_predictions[unread_books_indices_mapped]\n",
    "\n",
    "    # Sort unread books ratings\n",
    "    top_n_indices = np.argsort(unread_books_ratings)[::-1]\n",
    "    \n",
    "    recommended_books = []\n",
    "    count = 0\n",
    "    for i in top_n_indices:\n",
    "        if count >= top_n:\n",
    "            break\n",
    "        book_id = all_books[unread_books_indices[i]]\n",
    "        book_title = books.loc[books['book_id'] == book_id, 'title']\n",
    "        if not book_title.empty:\n",
    "            recommended_books.append((book_id, book_title.values[0], unread_books_ratings[i]))\n",
    "            count += 1\n",
    "\n",
    "    return recommended_books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    node_features = test_data_obj.x\n",
    "    edge_indices = test_data_obj.edge_index\n",
    "    predictions = model.predict(node_features, edge_indices)\n",
    "\n",
    "    rescaled_predictions = rescale_predictions(predictions, mean_rating, std_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(combined_data, rescaled_predictions, user_id=1, user_id_to_index=user_id_to_index, book_id_to_index=book_id_to_index, books=books, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(combined_data, rescaled_predictions, user_id=9, user_id_to_index=user_id_to_index, book_id_to_index=book_id_to_index, books=books, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
