{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    reviews = pd.read_pickle('../Pickle/reviews.pkl')\n",
    "    books = pd.read_pickle('../Pickle/books.pkl')\n",
    "    read = pd.read_pickle('../Pickle/read.pkl')\n",
    "    review_embeddings = pd.read_pickle('../Pickle/review_embeddings.pkl')\n",
    "    return reviews, books, read, review_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_review_embeddings(reviews, review_embeddings):\n",
    "    review_embeddings = pd.merge(reviews[['review_id', 'user_id', 'book_id']], review_embeddings, on='review_id', how='left')\n",
    "    return review_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(reviews, books, read, review_embeddings):\n",
    "    reviews = reviews[reviews['book_id'].isin(books['book_id'])]\n",
    "    read = read[read['book_id'].isin(books['book_id'])]\n",
    "    common_users = pd.merge(read[['user_id']], reviews[['user_id']], on='user_id')['user_id'].unique()\n",
    "    read = read[read['user_id'].isin(common_users)]\n",
    "    reviews = reviews[reviews['user_id'].isin(common_users)]\n",
    "    return reviews, books, read, review_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(read, reviews, review_embeddings):\n",
    "    train_read, test_read = train_test_split(read, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Filter reviews and review_embeddings to contain only users and books from the train and test sets\n",
    "    train_user_book_pairs = train_read[['user_id', 'book_id']]\n",
    "    test_user_book_pairs = test_read[['user_id', 'book_id']]\n",
    "\n",
    "    train_reviews = reviews[reviews[['user_id', 'book_id']].apply(tuple, axis=1).isin(train_user_book_pairs.apply(tuple, axis=1))]\n",
    "    test_reviews = reviews[reviews[['user_id', 'book_id']].apply(tuple, axis=1).isin(test_user_book_pairs.apply(tuple, axis=1))]\n",
    "    \n",
    "    train_review_embeddings = review_embeddings[review_embeddings[['user_id', 'book_id']].apply(tuple, axis=1).isin(train_user_book_pairs.apply(tuple, axis=1))]\n",
    "    test_review_embeddings = review_embeddings[review_embeddings[['user_id', 'book_id']].apply(tuple, axis=1).isin(test_user_book_pairs.apply(tuple, axis=1))]\n",
    "\n",
    "    return train_read, test_read, train_reviews, test_reviews, train_review_embeddings, test_review_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(train_read):\n",
    "    high_ratings = train_read[train_read['rating'] >= 4]\n",
    "    low_ratings = train_read[train_read['rating'] < 4]\n",
    "    low_ratings_upsampled = resample(low_ratings, replace=True, n_samples=len(high_ratings), random_state=42)\n",
    "    train_read = pd.concat([low_ratings_upsampled, high_ratings])\n",
    "    return train_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tensors(train_read, test_read, train_reviews, test_reviews, train_review_embeddings, test_review_embeddings, books):\n",
    "    train_read = balance_data(train_read)\n",
    "    train_ratings_tensor = torch.tensor(train_read['rating'].values, dtype=torch.float32)\n",
    "    test_ratings_tensor = torch.tensor(test_read['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    user_features = pd.concat([train_reviews[['user_id', 'Confidence Score']], test_reviews[['user_id', 'Confidence Score']]]).drop_duplicates().reset_index(drop=True)\n",
    "    book_features = books[['book_id', 'filtered_genres']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    merged_books_train = pd.merge(book_features, train_review_embeddings, on='book_id', how='left')\n",
    "    merged_books_test = pd.merge(book_features, test_review_embeddings, on='book_id', how='left')\n",
    "\n",
    "    # Drop rows with missing embeddings\n",
    "    merged_books_train = merged_books_train.dropna(subset=['embeddings'])\n",
    "    merged_books_test = merged_books_test.dropna(subset=['embeddings'])\n",
    "\n",
    "    # Determine the embedding dimension\n",
    "    if not merged_books_train['embeddings'].isnull().all():\n",
    "        embedding_dim = merged_books_train['embeddings'].dropna().iloc[0].shape[0]\n",
    "    elif not merged_books_test['embeddings'].isnull().all():\n",
    "        embedding_dim = merged_books_test['embeddings'].dropna().iloc[0].shape[0]\n",
    "    else:\n",
    "        raise ValueError(\"All embedding values are missing in both train and test datasets.\")\n",
    "\n",
    "    genre_encoder = OneHotEncoder(sparse_output=True)\n",
    "    book_genres_encoded = genre_encoder.fit_transform(books[['filtered_genres']])\n",
    "    book_genres_encoded_coo = book_genres_encoded.tocoo()\n",
    "\n",
    "    values = book_genres_encoded_coo.data\n",
    "    indices = np.vstack((book_genres_encoded_coo.row, book_genres_encoded_coo.col))\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = book_genres_encoded_coo.shape\n",
    "    book_features_tensor = torch.sparse_coo_tensor(i, v, torch.Size(shape)).coalesce()\n",
    "\n",
    "    book_embeddings_tensor_train = torch.tensor(np.stack(merged_books_train['embeddings'].values), dtype=torch.float32)\n",
    "    book_embeddings_tensor_test = torch.tensor(np.stack(merged_books_test['embeddings'].values), dtype=torch.float32)\n",
    "\n",
    "    book_features_tensor_dense = book_features_tensor.to_dense()\n",
    "    book_features_tensor_train = book_features_tensor_dense[:len(book_embeddings_tensor_train)].to_sparse().coalesce()\n",
    "    book_features_tensor_test = book_features_tensor_dense[:len(book_embeddings_tensor_test)].to_sparse().coalesce()\n",
    "\n",
    "    combined_book_features_tensor_train = torch.cat([book_features_tensor_train.to_dense(), book_embeddings_tensor_train], dim=1)\n",
    "    combined_book_features_tensor_test = torch.cat([book_features_tensor_test.to_dense(), book_embeddings_tensor_test], dim=1)\n",
    "\n",
    "    user_features_tensor = torch.tensor(user_features.values, dtype=torch.float32)\n",
    "    num_features_to_pad = combined_book_features_tensor_train.shape[1] - user_features_tensor.shape[1]\n",
    "    padding = torch.zeros((user_features_tensor.shape[0], num_features_to_pad))\n",
    "    user_features_tensor_padded = torch.cat([user_features_tensor, padding], dim=1)\n",
    "    user_features_coo = user_features_tensor_padded.to_sparse().coalesce()\n",
    "\n",
    "    user_id_to_index = {user_id: idx for idx, user_id in enumerate(user_features['user_id'].unique())}\n",
    "    book_id_to_index = {book_id: idx + len(user_id_to_index) for idx, book_id in enumerate(book_features['book_id'].unique())}\n",
    "\n",
    "    return train_read, test_read, train_ratings_tensor, test_ratings_tensor, user_features_coo, combined_book_features_tensor_train, combined_book_features_tensor_test, user_id_to_index, book_id_to_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_edge_index_and_ratings(df, user_id_to_index, book_id_to_index):\n",
    "    edge_index = []\n",
    "    ratings = []\n",
    "    for _, row in df.iterrows():\n",
    "        user_idx = user_id_to_index[row['user_id']]\n",
    "        book_idx = book_id_to_index[row['book_id']]\n",
    "        edge_index.append([user_idx, book_idx])\n",
    "        ratings.append(row['rating'])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    ratings_tensor = torch.tensor(ratings, dtype=torch.float)\n",
    "    return edge_index, ratings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_objects(train_read, test_read, user_features_coo, book_features_tensor_train, book_features_tensor_test, user_id_to_index, book_id_to_index):\n",
    "    train_edge_index, train_ratings_tensor = prepare_edge_index_and_ratings(train_read, user_id_to_index, book_id_to_index)\n",
    "    test_edge_index, test_ratings_tensor = prepare_edge_index_and_ratings(test_read, user_id_to_index, book_id_to_index)\n",
    "\n",
    "    user_indices = user_features_coo.indices()\n",
    "    book_indices_train = book_features_tensor_train.indices() + torch.tensor([[user_features_coo.shape[0]], [0]])\n",
    "    book_indices_test = book_features_tensor_test.indices() + torch.tensor([[user_features_coo.shape[0]], [0]])\n",
    "\n",
    "    combined_indices_train = torch.cat([user_indices, book_indices_train], dim=1)\n",
    "    combined_indices_test = torch.cat([user_indices, book_indices_test], dim=1)\n",
    "\n",
    "    train_data = Data(edge_index=combined_indices_train, y=train_ratings_tensor)\n",
    "    test_data = Data(edge_index=combined_indices_test, y=test_ratings_tensor)\n",
    "\n",
    "    return train_data, test_data, train_edge_index, test_edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews, books, read, review_embeddings = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_embeddings = align_review_embeddings(reviews, review_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "indices expected sparse coordinate tensor layout but got Strided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m train_read \u001b[38;5;241m=\u001b[39m balance_data(train_read)\n\u001b[0;32m      4\u001b[0m train_read, test_read, train_ratings_tensor, test_ratings_tensor, user_features_coo, combined_book_features_tensor_train, combined_book_features_tensor_test, user_id_to_index, book_id_to_index \u001b[38;5;241m=\u001b[39m prepare_tensors(train_read, test_read, train_reviews, test_reviews, train_review_embeddings, test_review_embeddings, books)\n\u001b[1;32m----> 5\u001b[0m train_data, test_data, train_edge_index, test_edge_index \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_features_coo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_book_features_tensor_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_book_features_tensor_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_id_to_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbook_id_to_index\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[37], line 6\u001b[0m, in \u001b[0;36mprepare_data_objects\u001b[1;34m(train_read, test_read, user_features_coo, book_features_tensor_train, book_features_tensor_test, user_id_to_index, book_id_to_index)\u001b[0m\n\u001b[0;32m      3\u001b[0m test_edge_index, test_ratings_tensor \u001b[38;5;241m=\u001b[39m prepare_edge_index_and_ratings(test_read, user_id_to_index, book_id_to_index)\n\u001b[0;32m      5\u001b[0m user_indices \u001b[38;5;241m=\u001b[39m user_features_coo\u001b[38;5;241m.\u001b[39mindices()\n\u001b[1;32m----> 6\u001b[0m book_indices_train \u001b[38;5;241m=\u001b[39m \u001b[43mbook_features_tensor_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[user_features_coo\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]], [\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m      7\u001b[0m book_indices_test \u001b[38;5;241m=\u001b[39m book_features_tensor_test\u001b[38;5;241m.\u001b[39mindices() \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[user_features_coo\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]], [\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m      9\u001b[0m combined_indices_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([user_indices, book_indices_train], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: indices expected sparse coordinate tensor layout but got Strided"
     ]
    }
   ],
   "source": [
    "reviews, books, read, review_embeddings = filter_data(reviews, books, read, review_embeddings)\n",
    "train_read, test_read, train_reviews, test_reviews, train_review_embeddings, test_review_embeddings = split_data(read, reviews, review_embeddings)\n",
    "train_read = balance_data(train_read)\n",
    "train_read, test_read, train_ratings_tensor, test_ratings_tensor, user_features_coo, combined_book_features_tensor_train, combined_book_features_tensor_test, user_id_to_index, book_id_to_index = prepare_tensors(train_read, test_read, train_reviews, test_reviews, train_review_embeddings, test_review_embeddings, books)\n",
    "train_data, test_data, train_edge_index, test_edge_index = prepare_data_objects(train_read, test_read, user_features_coo, combined_book_features_tensor_train, combined_book_features_tensor_test, user_id_to_index, book_id_to_index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels, heads=num_heads)\n",
    "        self.gat2 = GATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads)\n",
    "        self.gat3 = GATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads)\n",
    "        self.gat4 = GATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads)\n",
    "        self.gat5 = GATConv(hidden_channels * num_heads, out_channels, heads = num_heads, concat=False)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.gat3(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.gat4(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.gat4(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.gat5(x, edge_index)\n",
    "    \n",
    "        # Extract node embeddings and compute edge outputs\n",
    "        edge_outputs = x[edge_index[0]] * x[edge_index[1]]\n",
    "        edge_outputs = torch.sigmoid(edge_outputs.sum(dim=-1)) * 5  # Scale the output to the range [0, 5]\n",
    "        return edge_outputs\n",
    "\n",
    "    def predict(self, x, edge_index):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x, edge_index)\n",
    "\n",
    "\n",
    "# Define the model parameters\n",
    "in_channels = train_data.x.shape[1]\n",
    "hidden_channels = 16\n",
    "out_channels = 1\n",
    "num_heads = 8\n",
    "\n",
    "# Initialize the model\n",
    "model = GATModel(in_channels, hidden_channels, out_channels, num_heads)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = torch.nn.SmoothL1Loss() \n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.2)\n",
    "\n",
    "def train(model, train_data, val_data, criterion, optimizer, num_epochs, batch_size):\n",
    "    train_loader = DataLoader([train_data], batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader([val_data], batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index)\n",
    "\n",
    "            if out.shape != data.y.shape:\n",
    "                raise ValueError(f'Shape mismatch: output {out.shape}, target {data.y.shape}')\n",
    "            \n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                out = model(data.x, data.edge_index)\n",
    "                if out.shape != data.y.shape:\n",
    "                    raise ValueError(f'Shape mismatch: output {out.shape}, target {data.y.shape}')\n",
    "                \n",
    "                val_loss = criterion(out, data.y)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f\"                Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "trained_model = train(model, train_data, test_data, criterion, optimizer, num_epochs=25, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(r, k):\n",
    "    \"\"\"Calculates precision at k\"\"\"\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "def recall_at_k(r, k, all_positives):\n",
    "    \"\"\"Calculates recall at k\"\"\"\n",
    "    r = np.asarray(r)[:k]\n",
    "    if all_positives == 0:\n",
    "        return 0\n",
    "    return np.sum(r) / all_positives\n",
    "\n",
    "def ndcg_at_k(actual_sorted, k):\n",
    "    actual_sorted_padded = np.pad(actual_sorted, (0, max(0, k - len(actual_sorted))), 'constant')\n",
    "    ideal_sorted = np.pad(np.sort(actual_sorted)[::-1], (0, max(0, k - len(actual_sorted))), 'constant')\n",
    "    ideal_dcg = np.sum(ideal_sorted[:k] / np.log2(np.arange(2, k + 2)))\n",
    "    dcg = np.sum(actual_sorted_padded[:k] / np.log2(np.arange(2, k + 2)))\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, model, target_ratings, k):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model.predict(data.x, data.edge_index).squeeze()\n",
    "        predicted_ratings = out[data.edge_index[0]].cpu().numpy()\n",
    "        actual_ratings = (target_ratings >= 4).float().cpu().numpy()\n",
    "\n",
    "        user_ids = data.edge_index[0].cpu().numpy()\n",
    "        book_ids = data.edge_index[1].cpu().numpy()\n",
    "\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        ndcg_scores = []\n",
    "\n",
    "        for user_id in np.unique(user_ids):\n",
    "            user_indices = user_ids == user_id\n",
    "            actual = actual_ratings[user_indices]\n",
    "            predicted = predicted_ratings[user_indices]\n",
    "\n",
    "            sorted_indices = np.argsort(predicted)[::-1]\n",
    "            actual_sorted = actual[sorted_indices]\n",
    "\n",
    "            precision = precision_at_k(actual_sorted, k)\n",
    "            recall = recall_at_k(actual_sorted, k, np.sum(actual))\n",
    "            ndcg = ndcg_at_k(actual_sorted, k)\n",
    "\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            ndcg_scores.append(ndcg)\n",
    "\n",
    "        mean_precision = np.mean(precision_scores)\n",
    "        mean_recall = np.mean(recall_scores)\n",
    "        mean_ndcg = np.mean(ndcg_scores)\n",
    "\n",
    "        print(f'Precision@{k}: {mean_precision:.4f}')\n",
    "        print(f'Recall@{k}: {mean_recall:.4f}')\n",
    "        print(f'NDCG@{k}: {mean_ndcg:.4f}')\n",
    "\n",
    "evaluate(test_data, model, test_ratings_tensor, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(data, model, user_id, top_n):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_index = user_id_to_index[user_id]\n",
    "        book_indices = torch.arange(len(user_id_to_index), len(user_id_to_index) + len(book_id_to_index))\n",
    "        user_edge_index = torch.stack([torch.full_like(book_indices, user_index), book_indices], dim=0)\n",
    "        out = model.predict(data.x, user_edge_index)\n",
    "        predicted_ratings = out.squeeze().cpu().numpy()\n",
    "        read_books = set(read[read['user_id'] == user_id]['book_id'])\n",
    "        all_books = list(book_id_to_index.keys())\n",
    "        unread_books_indices = [i for i in range(len(all_books)) if all_books[i] not in read_books]\n",
    "        unread_books_ratings = predicted_ratings[unread_books_indices]\n",
    "        top_n_indices = np.argsort(unread_books_ratings)[::-1][:top_n]\n",
    "        recommended_books = [(all_books[i], books[books['book_id'] == all_books[i]]['title'].values[0], unread_books_ratings[i]) for i in top_n_indices]\n",
    "        return recommended_books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the edge indices for all data\n",
    "combined_edge_index = torch.cat([train_edge_index, test_edge_index], dim=1) \n",
    "# Prepare PyTorch Geometric Data object for all data \n",
    "combined_data = Data(x=all_features, edge_index=combined_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_books = recommend(combined_data, model, user_id=1, top_n=5)\n",
    "for book_id, title, rating in recommended_books:\n",
    "    print(f\"Book ID: {book_id}, Title: {title}, Predicted Rating: {rating:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_books = recommend(combined_data, model, user_id=4, top_n=5)\n",
    "for book_id, title, rating in recommended_books:\n",
    "    print(f\"Book ID: {book_id}, Title: {title}, Predicted Rating: {rating:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_books = recommend(combined_data, model, user_id=15, top_n=5)\n",
    "for book_id, title, rating in recommended_books:\n",
    "    print(f\"Book ID: {book_id}, Title: {title}, Predicted Rating: {rating:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_books = recommend(combined_data, model, user_id=67, top_n=5)\n",
    "for book_id, title, rating in recommended_books:\n",
    "    print(f\"Book ID: {book_id}, Title: {title}, Predicted Rating: {rating:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
