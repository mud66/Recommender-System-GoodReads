{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from umap import UMAP\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.sparse import csr_matrix\n",
    "import faiss  \n",
    "import hdbscan  \n",
    "\n",
    "sns.set_theme(style=\"white\", palette=\"muted\")\n",
    "\n",
    "# Load data\n",
    "def load_data(file_path):\n",
    "    books = pd.read_pickle(file_path)\n",
    "    books = books.drop_duplicates(subset='title', keep='first')\n",
    "    embedding_matrix = np.vstack(books['embeddings'].values)\n",
    "    return books, embedding_matrix\n",
    "\n",
    "# Standardizing embeddings using sparse matrices\n",
    "def standardize_embeddings(train_embeddings, test_embeddings):\n",
    "    scaler = StandardScaler(with_mean=False)  # Avoid modifying sparsity\n",
    "    return scaler.fit_transform(train_embeddings), scaler.transform(test_embeddings)\n",
    "\n",
    "# PCA with IncrementalPCA (memory efficient)\n",
    "def apply_pca(embeddings, n_components=50, batch_size=1000):\n",
    "    ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
    "    return ipca.fit_transform(embeddings)\n",
    "\n",
    "# UMAP with parallelization\n",
    "def apply_umap(embeddings, n_components=20, n_neighbors=200, min_dist=0.005):\n",
    "    umap_model = UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, metric='cosine', n_jobs=-1)\n",
    "    return umap_model.fit_transform(embeddings)\n",
    "\n",
    "def remove_outliers(embeddings, contamination=0.05, max_samples=0.2, random_state=42):\n",
    "    iso_forest = IsolationForest(contamination=contamination, max_samples=max_samples, random_state=random_state, n_jobs=-1)\n",
    "    outliers = iso_forest.fit_predict(embeddings)\n",
    "    valid_indices = np.where(outliers == 1)[0]  # Indices of non-outliers\n",
    "    return embeddings[valid_indices], valid_indices  # Return the cleaned embeddings and valid indices\n",
    "\n",
    "\n",
    "# Convert embeddings to sparse matrix\n",
    "def convert_to_sparse(embeddings):\n",
    "    return csr_matrix(embeddings)\n",
    "\n",
    "# **HDBSCAN Clustering**\n",
    "def perform_hdbscan_clustering(embeddings, min_cluster_size=500, min_samples=300):\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, metric='euclidean')\n",
    "    clusters = clusterer.fit_predict(embeddings)\n",
    "    return clusters\n",
    "\n",
    "# Efficient Recommendation Search with Precomputed Similarities\n",
    "import faiss\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def get_recommendations_by_cluster(book_id, books_df, embeddings, top_n=5, book_id_to_index=None):\n",
    "    if book_id not in book_id_to_index:\n",
    "        return []  # Return empty if book_id is not in book_id_to_index\n",
    "\n",
    "    book_idx = book_id_to_index[book_id]\n",
    "    input_book_title = books_df.loc[books_df['book_id'] == book_id, 'title'].values[0]\n",
    "\n",
    "    # Create a FAISS index (using L2 or IP depending on embeddings)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # Using L2 for non-normalized or IP for normalized\n",
    "    index.add(embeddings)  # Add all book embeddings to FAISS index\n",
    "\n",
    "    # Search for nearest neighbors\n",
    "    _, indices = index.search(np.array([embeddings[book_idx]]), top_n + 1)  # +1 to exclude itself\n",
    "\n",
    "    recommendations = []\n",
    "    for idx in indices[0][1:]:  # Exclude the book itself\n",
    "        recommended_book = books_df.iloc[idx]\n",
    "        recommendations.append({\"title\": recommended_book[\"title\"], \"authors\": recommended_book[\"authors\"]})\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# Correct cluster assignment for train and test sets\n",
    "def assign_clusters_to_books(books, indices, clusters, cluster_column=\"cluster\"):\n",
    "    books_copy = books.copy()\n",
    "    books_copy[cluster_column] = -1\n",
    "    books_copy.iloc[indices, books_copy.columns.get_loc(cluster_column)] = clusters\n",
    "    return books_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "books, embedding_matrix = load_data('../Pickle/books.pkl')\n",
    "\n",
    "# Remove outliers and get valid indices\n",
    "clean_embeddings, valid_indices = remove_outliers(embedding_matrix)\n",
    "\n",
    "# Filter the books DataFrame using the valid indices\n",
    "books = books.iloc[valid_indices]\n",
    "\n",
    "# Standardize embeddings (using the full clean_embeddings dataset now)\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(clean_embeddings)\n",
    "\n",
    "# Dimensionality reduction using PCA\n",
    "pca_embeddings = apply_pca(scaled_embeddings)\n",
    "\n",
    "# UMAP reduction\n",
    "umap_embeddings = apply_umap(pca_embeddings)\n",
    "\n",
    "# Apply HDBSCAN clustering\n",
    "full_clusters = perform_hdbscan_clustering(umap_embeddings, min_cluster_size=10, min_samples=5)\n",
    "\n",
    "# Ensure that indices match the embeddings used to generate clusters\n",
    "indices = np.arange(umap_embeddings.shape[0])\n",
    "\n",
    "# Assign clusters to the books\n",
    "books = assign_clusters_to_books(books, indices, full_clusters, cluster_column=\"cluster\")\n",
    "\n",
    "# Map book_id to index (make sure this map is consistent with the embeddings)\n",
    "book_id_to_index = {book_id: idx for idx, book_id in enumerate(books['book_id'])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check recommendations for a book\n",
    "recommendations = get_recommendations_by_cluster(40, books, umap_embeddings, book_id_to_index=book_id_to_index)\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recommendations = get_recommendations_by_cluster(36494299, books, umap_embeddings, book_id_to_index=book_id_to_index)\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting 3D\n",
    "def plot_3d_embeddings(embeddings, clusters):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter(embeddings[:, 0], embeddings[:, 1], embeddings[:, 2], \n",
    "                         c=clusters, cmap='tab10', s=50, alpha=0.6, edgecolor='w')\n",
    "\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    ax.set_zlabel('Component 3')\n",
    "    plt.title('3D Clustering of Books')\n",
    "    legend1 = ax.legend(*scatter.legend_elements(), title=\"Cluster\")\n",
    "    ax.add_artist(legend1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_embeddings(umap_embeddings[:, :3], full_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "dbi_score = davies_bouldin_score(umap_embeddings, full_clusters)\n",
    "print(f\"Davies-Bouldin Index: {dbi_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "ch_score = calinski_harabasz_score(umap_embeddings, full_clusters)\n",
    "print(f\"Calinski-Harabasz Index: {ch_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
