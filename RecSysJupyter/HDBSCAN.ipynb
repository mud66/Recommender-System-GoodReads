{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import faiss  \n",
    "import hdbscan  \n",
    "import pickle\n",
    "import umap\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from itertools import product \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  \n",
    "sns.set_theme(style=\"white\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    books_list = []\n",
    "\n",
    "    with open('../Pickle/books.pkl', 'rb') as file:\n",
    "        while True:\n",
    "            try:\n",
    "                chunk = pickle.load(file)\n",
    "                books_list.append(chunk)\n",
    "            except EOFError:\n",
    "                break  \n",
    "    books = pd.concat(books_list, ignore_index=True)\n",
    "    books = books.drop_duplicates(subset='title', keep='first')\n",
    "    embedding_matrix = np.vstack(books['embeddings'].values)\n",
    "    return books, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_umap(embeddings, n_components=10, n_neighbors=300, min_dist=0.0):\n",
    "    embeddings = np.asarray(embeddings, dtype=np.float32)  \n",
    "\n",
    "    umap_model = umap.UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric='cosine',\n",
    "        low_memory=True, \n",
    "        random_state = 42\n",
    "    )\n",
    "    \n",
    "    return umap_model.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters_to_books(books, indices, clusters, cluster_column=\"cluster\"):\n",
    "    books_copy = books.copy()\n",
    "    books_copy[cluster_column] = -1\n",
    "    books_copy.iloc[indices, books_copy.columns.get_loc(cluster_column)] = clusters\n",
    "    return books_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hdbscan_clustering(embeddings, alpha=0.5, beta=0.5, n_trials=5):\n",
    "    # L2-normalize embeddings so Euclidean â‰ˆ Cosine distance\n",
    "    embeddings_normalized = normalize(embeddings, norm='l2', axis=1)\n",
    "\n",
    "    # Define search space for hyperparameters\n",
    "    min_cluster_sizes = [30, 40]\n",
    "    min_samples_list = [15, 25]\n",
    "    cluster_selection_epsilons = [0.1]\n",
    "\n",
    "    # Generate all possible hyperparameter combinations\n",
    "    all_param_combinations = list(product(min_cluster_sizes, min_samples_list, cluster_selection_epsilons))\n",
    "\n",
    "    # Randomly sample n_trials parameter combinations\n",
    "    sampled_combinations = random.sample(all_param_combinations, min(n_trials, len(all_param_combinations)))\n",
    "\n",
    "    best_combined_score = float(\"-inf\")  # Higher is better\n",
    "    best_params = None\n",
    "    best_clusterer = None\n",
    "    best_clusters = None\n",
    "\n",
    "    for min_cluster_size, min_samples, cluster_selection_epsilon in sampled_combinations:\n",
    "        # Perform clustering with soft clustering\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            cluster_selection_epsilon=cluster_selection_epsilon, \n",
    "            metric='euclidean',\n",
    "            prediction_data=True,\n",
    "            core_dist_n_jobs=1,\n",
    "            cluster_selection_method='leaf'\n",
    "        )\n",
    "        clusterer.fit_predict(embeddings_normalized)\n",
    "\n",
    "        soft_clusters = hdbscan.prediction.all_points_membership_vectors(clusterer)\n",
    "        \n",
    "        hard_clusters = np.array([\n",
    "            -1 if max(membership) < 0.01 else np.argmax(membership)\n",
    "            for membership in soft_clusters\n",
    "        ])\n",
    "\n",
    "        if len(set(hard_clusters) - {-1}) > 1:\n",
    "            db_index = davies_bouldin_score(embeddings_normalized, hard_clusters)\n",
    "            ch_index = calinski_harabasz_score(embeddings_normalized, hard_clusters)\n",
    "        else:\n",
    "            db_index, ch_index = float(\"inf\"), 0  \n",
    "\n",
    "        combined_score = alpha * (1 / db_index) + beta * ch_index\n",
    "\n",
    "        print(f\"min_cluster_size={min_cluster_size}, min_samples={min_samples}, epsilon={cluster_selection_epsilon}, DB={db_index:.3f}, CH={ch_index:.3f}, Combined={combined_score:.3f}\")\n",
    "\n",
    "        if combined_score > best_combined_score:\n",
    "            best_combined_score = combined_score\n",
    "            best_params = (min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "            best_clusterer = clusterer\n",
    "            best_clusters = hard_clusters\n",
    "\n",
    "    print(\"\\nBest Params:\", best_params, \"Best Combined Score:\", best_combined_score)\n",
    "    return best_clusters, best_clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books, embedding_matrix = load_data()\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings = apply_umap(scaled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, clusterer = perform_hdbscan_clustering(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(umap_embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = assign_clusters_to_books(books, indices, clusters, cluster_column=\"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_to_index = {book_id: idx for idx, book_id in enumerate(books['book_id'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "umap_embeddings_2d = apply_umap(scaled_embeddings, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_indices = clusters != -1\n",
    "filtered_embeddings_2d = umap_embeddings_2d[filtered_indices]\n",
    "filtered_clusters = clusters[filtered_indices]\n",
    "df_plot_2d = pd.DataFrame({\n",
    "    'UMAP1': filtered_embeddings_2d[:, 0],\n",
    "    'UMAP2': filtered_embeddings_2d[:, 1],\n",
    "    'Cluster': filtered_clusters\n",
    "})\n",
    "fig = px.scatter(\n",
    "    df_plot_2d,\n",
    "    x='UMAP1',\n",
    "    y='UMAP2',\n",
    "    color='Cluster',\n",
    "    title='2D UMAP Embeddings Coloured by Cluster',\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='UMAP Dimension 1',\n",
    "    yaxis_title='UMAP Dimension 2'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    x=filtered_embeddings_2d[:, 0],\n",
    "    y=filtered_embeddings_2d[:, 1],\n",
    "    hue=filtered_clusters,\n",
    "    palette='tab10',\n",
    "    alpha=0.7,\n",
    "    legend = False\n",
    ")\n",
    "plt.title('2D Plot of Embeddings and Clusters', fontsize=16)\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.kdeplot(\n",
    "    x=filtered_embeddings_2d[:, 0],\n",
    "    y=filtered_embeddings_2d[:, 1],\n",
    "    fill=True,\n",
    "    cmap='viridis',  \n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.title('Density Plot of UMAP 2D Embeddings')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.kdeplot(\n",
    "    x=filtered_embeddings_2d[:, 0],\n",
    "    y=filtered_embeddings_2d[:, 1],\n",
    "    hue=filtered_clusters,\n",
    "    fill=True,\n",
    "    alpha=0.5,\n",
    "    palette='tab10',\n",
    "    legend=False \n",
    ")\n",
    "\n",
    "plt.title('Density Plot of UMAP 2D Embeddings by Cluster')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(filtered_clusters, return_counts=True)\n",
    "cluster_sizes = dict(zip(unique, counts))\n",
    "df = pd.DataFrame({\n",
    "    'Cluster': list(cluster_sizes.keys()),\n",
    "    'Count': list(cluster_sizes.values())\n",
    "})\n",
    "plt.figure(figsize=(50, 6))\n",
    "sns.barplot(data=df, x='Cluster', y='Count', hue='Cluster', palette='viridis', dodge=False, legend=False)\n",
    "plt.title('Cluster Sizes (With Outliers)', fontsize=16)\n",
    "plt.xlabel('Cluster Label', fontsize=14)\n",
    "plt.ylabel('Number of Points', fontsize=14)\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sizes = [np.sum(clusters == label) for label in set(clusters) if label != -1]\n",
    "plt.hist(cluster_sizes, bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Cluster Size Distribution')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "compactness = []\n",
    "for label in set(clusters):\n",
    "    if label != -1:\n",
    "        # Select all points belonging to the current cluster\n",
    "        cluster_points = umap_embeddings[clusters == label]\n",
    "        \n",
    "        # Compute the centroid of the cluster\n",
    "        centroid = cluster_points.mean(axis=0)\n",
    "        \n",
    "        # Calculate distances from each point to the centroid\n",
    "        distances = euclidean_distances(cluster_points, centroid.reshape(1, -1))\n",
    "        \n",
    "        # Append the average distance (compactness for this cluster)\n",
    "        compactness.append(np.mean(distances))\n",
    "\n",
    "# Now compute separation between cluster centroids\n",
    "separation = []\n",
    "cluster_centroids = [\n",
    "    umap_embeddings[clusters == label].mean(axis=0)\n",
    "    for label in set(clusters) if label != -1\n",
    "]\n",
    "\n",
    "for i in range(len(cluster_centroids)):\n",
    "    for j in range(i + 1, len(cluster_centroids)):\n",
    "        dist = euclidean_distances(\n",
    "            [cluster_centroids[i]], [cluster_centroids[j]]\n",
    "        )[0][0]\n",
    "        separation.append(dist)\n",
    "\n",
    "# Output the averages\n",
    "print(f\"Average Compactness: {np.mean(compactness):.4f}\")\n",
    "print(f\"Average Separation: {np.mean(separation):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_points = len(clusters)\n",
    "outlier_points = np.sum(clusters == -1)\n",
    "outlier_percentage = (outlier_points / total_points) * 100\n",
    "print(f\"Outliers: {outlier_points} / {total_points}\")\n",
    "print(f\"Percentage of Outliers: {outlier_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbi_score = davies_bouldin_score(umap_embeddings, clusters)\n",
    "print(f\"Davies-Bouldin Index: {dbi_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_score = calinski_harabasz_score(umap_embeddings, clusters)\n",
    "print(f\"Calinski-Harabasz Index: {ch_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = silhouette_score(umap_embeddings, clusters)\n",
    "print(f\"Silhouette Score: {sh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = umap_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/umap_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(umap_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(faiss_index, '../Pickle/faiss_index.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/book_id_to_index.pkl', 'wb') as f:\n",
    "    pickle.dump(book_id_to_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/clustered_books.pkl', 'wb') as f:\n",
    "    pickle.dump(books, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/clusters.pkl', 'wb') as f:\n",
    "    pickle.dump(clusters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_books(book_id, top_n=5):\n",
    "    # Check if the book ID exists\n",
    "    if book_id not in book_id_to_index:\n",
    "        print(f\"Book ID {book_id} not found.\")\n",
    "        return []\n",
    "    \n",
    "    # Get the index and cluster of the input book\n",
    "    book_idx = book_id_to_index[book_id]\n",
    "    book_cluster = clusters[book_idx]\n",
    "\n",
    "    # Get the embedding for the book\n",
    "    query_embedding = umap_embeddings[book_idx].reshape(1, -1).astype('float32')\n",
    "\n",
    "    # If the book is not an outlier\n",
    "    if book_cluster != -1:\n",
    "        # Get indices of books in the same cluster excluding itself\n",
    "        same_cluster_indices = np.where((clusters == book_cluster) & (np.arange(len(clusters)) != book_idx))[0]\n",
    "\n",
    "        # If there are other books in the cluster\n",
    "        if len(same_cluster_indices) > 0:\n",
    "            # Get embeddings of books in the same cluster\n",
    "            cluster_embeddings = umap_embeddings[same_cluster_indices]\n",
    "\n",
    "            # Compute distances to all books in the cluster\n",
    "            distances = np.linalg.norm(cluster_embeddings - query_embedding, axis=1)\n",
    "\n",
    "            # Get top_n closest books (they already exclude itself)\n",
    "            top_indices = np.argsort(distances)[:top_n]\n",
    "\n",
    "            # Map back to book IDs and return similarity scores\n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                similar_book_idx = same_cluster_indices[idx]\n",
    "                similar_book_id = books.iloc[similar_book_idx]['book_id']\n",
    "                similarity_score = 1 / (1 + distances[idx])  # Convert distance to similarity\n",
    "                results.append((similar_book_id, similarity_score))\n",
    "\n",
    "            return results\n",
    "\n",
    "    # If outlier or no other books in the cluster, do global FAISS search\n",
    "    print(\"Book is an outlier or has no cluster neighbors. Performing global search...\")\n",
    "    \n",
    "    D, I = faiss_index.search(query_embedding, top_n + 1)  # +1 because it includes itself as the closest neighbor\n",
    "    results = []\n",
    "    count = 0\n",
    "    for idx in I[0]:\n",
    "        if idx == book_idx:\n",
    "            continue  # Skip the query book itself\n",
    "        similar_book_id = books.iloc[idx]['book_id']\n",
    "        distance = D[0, count]\n",
    "        similarity_score = 1 / (1 + distance)\n",
    "        results.append((similar_book_id, similarity_score))\n",
    "        count += 1\n",
    "        if len(results) == top_n:\n",
    "            break\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommend_similar_books(book_id=86, top_n=5)\n",
    "for rec_id, score in recommendations:\n",
    "    title = books.loc[books['book_id'] == rec_id, 'title'].values[0]\n",
    "    print(f\"Recommended Book ID: {rec_id}, Title: {title}, Similarity Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.condensed_tree_.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
