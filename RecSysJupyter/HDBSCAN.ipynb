{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import faiss  \n",
    "import hdbscan  \n",
    "import pickle\n",
    "import umap\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from itertools import product \n",
    "import hdbscan.prediction \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # For NumPy-based randomness\n",
    "sns.set_theme(style=\"white\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data():\n",
    "\n",
    "# Load the pickled chunks and concatenate them\n",
    "    books_list = []\n",
    "\n",
    "    with open('../Pickle/books.pkl', 'rb') as file:\n",
    "        while True:\n",
    "            try:\n",
    "                chunk = pickle.load(file)\n",
    "                books_list.append(chunk)\n",
    "            except EOFError:\n",
    "                break  # Stop when end of file is reached\n",
    "    books = pd.concat(books_list, ignore_index=True)\n",
    "    books = books.drop_duplicates(subset='title', keep='first')\n",
    "    embedding_matrix = np.vstack(books['embeddings'].values)\n",
    "    return books, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(embeddings, n_components=50, batch_size=5000):\n",
    "    embeddings = np.asarray(embeddings, dtype=np.float32)  # Ensure efficient type\n",
    "\n",
    "    ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
    "    \n",
    "    # Fit on batches\n",
    "    for i in range(0, embeddings.shape[0], batch_size):\n",
    "        batch = embeddings[i:i + batch_size]\n",
    "        ipca.partial_fit(batch)\n",
    "\n",
    "    # Transform in batches\n",
    "    transformed = np.vstack([ipca.transform(embeddings[i:i + batch_size]) \n",
    "                             for i in range(0, embeddings.shape[0], batch_size)])\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "def apply_umap(embeddings, n_components=20, n_neighbors=100, min_dist=0.08):\n",
    "    embeddings = np.asarray(embeddings, dtype=np.float32)  \n",
    "\n",
    "    umap_model = umap.UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric='cosine',\n",
    "        low_memory=True,  # Optimized memory use\n",
    "        random_state = 42\n",
    "    )\n",
    "    \n",
    "    return umap_model.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "# Correct cluster assignment for train and test sets\n",
    "def assign_clusters_to_books(books, indices, clusters, cluster_column=\"cluster\"):\n",
    "    books_copy = books.copy()\n",
    "    books_copy[cluster_column] = -1\n",
    "    books_copy.iloc[indices, books_copy.columns.get_loc(cluster_column)] = clusters\n",
    "    return books_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hdbscan_clustering(embeddings, alpha=0.5, beta=0.5, n_trials=10):\n",
    "    # L2-normalize embeddings so Euclidean â‰ˆ Cosine distance\n",
    "    embeddings_normalized = normalize(embeddings, norm='l2', axis=1)\n",
    "\n",
    "    # Define search space for hyperparameters\n",
    "    min_cluster_sizes = [100, 200, 300, 400, 500, 600, 1000]\n",
    "    min_samples_list = [50, 100, 200, 300, 400, 500, 600, 1000]\n",
    "\n",
    "    # Generate all possible hyperparameter combinations\n",
    "    all_param_combinations = list(product(min_cluster_sizes, min_samples_list))\n",
    "\n",
    "    # Randomly sample n_trials parameter combinations\n",
    "    sampled_combinations = random.sample(all_param_combinations, min(n_trials, len(all_param_combinations)))\n",
    "\n",
    "    best_combined_score = float(\"-inf\")  # Higher is better\n",
    "    best_params = None\n",
    "    best_clusterer = None\n",
    "    best_clusters = None\n",
    "\n",
    "    for min_cluster_size, min_samples in sampled_combinations:\n",
    "        # Perform clustering with soft clustering enabled\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            metric='euclidean',\n",
    "            prediction_data=True,\n",
    "            core_dist_n_jobs=-1,\n",
    "        )\n",
    "        clusterer.fit_predict(embeddings_normalized)\n",
    "\n",
    "        # Compute soft cluster assignments\n",
    "        soft_clusters = hdbscan.prediction.all_points_membership_vectors(clusterer)\n",
    "        \n",
    "        # Assign each book to its most likely cluster, unless it has very low membership\n",
    "        hard_clusters = np.array([\n",
    "            -1 if max(membership) < 0.1 else np.argmax(membership)\n",
    "            for membership in soft_clusters\n",
    "        ])\n",
    "\n",
    "        # Compute DB and CH scores only if there's more than one valid cluster\n",
    "        if len(set(hard_clusters) - {-1}) > 1:\n",
    "            db_index = davies_bouldin_score(embeddings_normalized, hard_clusters)\n",
    "            ch_index = calinski_harabasz_score(embeddings_normalized, hard_clusters)\n",
    "        else:\n",
    "            db_index, ch_index = float(\"inf\"), 0  # Penalize poor clustering\n",
    "\n",
    "        # Compute the combined score\n",
    "        combined_score = alpha * (1 / db_index) + beta * ch_index\n",
    "\n",
    "        print(f\"min_cluster_size={min_cluster_size}, min_samples={min_samples}, DB={db_index:.3f}, CH={ch_index:.3f}, Combined={combined_score:.3f}\")\n",
    "\n",
    "        # Track best model based on the combined score\n",
    "        if combined_score > best_combined_score:\n",
    "            best_combined_score = combined_score\n",
    "            best_params = (min_cluster_size, min_samples)\n",
    "            best_clusterer = clusterer\n",
    "            best_clusters = hard_clusters\n",
    "\n",
    "    print(\"\\nBest Params:\", best_params, \"Best Combined Score:\", best_combined_score)\n",
    "    return best_clusters, best_clusterer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "books, embedding_matrix = load_data()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction using PCA\n",
    "pca_embeddings = apply_pca(scaled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP reduction\n",
    "umap_embeddings = apply_umap(pca_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HDBSCAN clustering\n",
    "clusters, clusterer = perform_hdbscan_clustering(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that indices match the embeddings used to generate clusters\n",
    "indices = np.arange(umap_embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign clusters to the books\n",
    "books = assign_clusters_to_books(books, indices, clusters, cluster_column=\"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map book_id to index \n",
    "book_id_to_index = {book_id: idx for idx, book_id in enumerate(books['book_id'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(book_id, books=books, umap_embeddings=umap_embeddings, top_n=5, book_id_to_index=book_id_to_index, clusters=clusters):\n",
    "    if book_id not in book_id_to_index:\n",
    "        return []  # Return empty if book_id is not found\n",
    "\n",
    "    book_idx = book_id_to_index[book_id]\n",
    "    input_cluster = clusters[book_idx]\n",
    "\n",
    "    # Get indices of books in the same cluster\n",
    "    cluster_indices = np.where(clusters == input_cluster)[0]\n",
    "    \n",
    "    if len(cluster_indices) <= 1:  \n",
    "        return []  # If the book is the only one in its cluster, no recommendations\n",
    "\n",
    "    # Create FAISS index for the same-cluster books\n",
    "    dimension = umap_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 for non-normalized vectors\n",
    "    cluster_embeddings = umap_embeddings[cluster_indices]\n",
    "    index.add(cluster_embeddings)\n",
    "\n",
    "    # Search for nearest neighbors **only within the same cluster**\n",
    "    distances, indices = index.search(np.array([umap_embeddings[book_idx]]), min(len(cluster_indices), top_n + 1))  \n",
    "\n",
    "    recommendations = []\n",
    "    for idx, dist in zip(indices[0][1:], distances[0][1:]):  # Exclude the book itself\n",
    "        recommended_book = books.iloc[cluster_indices[idx]]\n",
    "\n",
    "        # Compute similarity score\n",
    "        similarity = round(1 / (1 + dist), 3)\n",
    "\n",
    "        recommendations.append({\n",
    "            \"title\": recommended_book[\"title\"],\n",
    "            \"authors\": recommended_book[\"authors\"],\n",
    "            \"cluster\": input_cluster,\n",
    "            \"similarity\": similarity\n",
    "        })\n",
    "\n",
    "    # **Sort recommendations by similarity in descending order**\n",
    "    recommendations.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "\n",
    "    return recommendations[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = get_recommendations(36488099, books, umap_embeddings, book_id_to_index=book_id_to_index)\n",
    "for rec in recommendations:\n",
    "    print(f\"{rec['title']} by {rec['authors']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = get_recommendations(36483546, books, umap_embeddings, book_id_to_index=book_id_to_index)\n",
    "for rec in recommendations:\n",
    "    print(f\"{rec['title']} by {rec['authors']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = get_recommendations(36488099, books, umap_embeddings, book_id_to_index=book_id_to_index)\n",
    "for rec in recommendations:\n",
    "    print(f\"{rec['title']} by {rec['authors']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = get_recommendations(36491811, books, umap_embeddings, book_id_to_index=book_id_to_index)\n",
    "for rec in recommendations:\n",
    "    print(f\"{rec['title']} by {rec['authors']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = get_recommendations(36494299, books, umap_embeddings, book_id_to_index=book_id_to_index)\n",
    "for rec in recommendations:\n",
    "    print(f\"{rec['title']} by {rec['authors']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting 3D\n",
    "def plot_3d_embeddings(embeddings, clusters):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter(embeddings[:, 0], embeddings[:, 1], embeddings[:, 2], \n",
    "                         c=clusters, cmap='tab10', s=50, alpha=0.6, edgecolor='w')\n",
    "\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    ax.set_zlabel('Component 3')\n",
    "    plt.title('3D Clustering of Books')\n",
    "    legend1 = ax.legend(*scatter.legend_elements(), title=\"Cluster\")\n",
    "    ax.add_artist(legend1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_embeddings(umap_embeddings[:, :3], clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbi_score = davies_bouldin_score(umap_embeddings, clusters)\n",
    "print(f\"Davies-Bouldin Index: {dbi_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_score = calinski_harabasz_score(umap_embeddings, clusters)\n",
    "print(f\"Calinski-Harabasz Index: {ch_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = umap_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save UMAP embeddings\n",
    "with open('../Pickle/umap_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(umap_embeddings, f)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(faiss_index, '../Pickle/faiss_index.bin')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save book_id to index mapping\n",
    "with open('../Pickle/book_id_to_index.pkl', 'wb') as f:\n",
    "    pickle.dump(book_id_to_index, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['cluster'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
