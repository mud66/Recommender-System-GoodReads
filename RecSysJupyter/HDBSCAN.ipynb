{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import faiss  \n",
    "import hdbscan  \n",
    "import pickle\n",
    "import umap\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from itertools import product \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  \n",
    "sns.set_theme(style=\"white\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    books_list = []\n",
    "\n",
    "    with open('../Pickle/books.pkl', 'rb') as file:\n",
    "        while True:\n",
    "            try:\n",
    "                chunk = pickle.load(file)\n",
    "                books_list.append(chunk)\n",
    "            except EOFError:\n",
    "                break  \n",
    "    books = pd.concat(books_list, ignore_index=True)\n",
    "    books = books.drop_duplicates(subset='title', keep='first')\n",
    "    embedding_matrix = np.vstack(books['embeddings'].values)\n",
    "    return books, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_umap(embeddings, n_components=20, n_neighbors=100, min_dist=0.0):\n",
    "    embeddings = np.asarray(embeddings, dtype=np.float32)  \n",
    "\n",
    "    umap_model = umap.UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric='cosine',\n",
    "        low_memory=True, \n",
    "        random_state = 42\n",
    "    )\n",
    "    \n",
    "    return umap_model.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters_to_books(books, indices, clusters, cluster_column=\"cluster\"):\n",
    "    books_copy = books.copy()\n",
    "    books_copy[cluster_column] = -1\n",
    "    books_copy.iloc[indices, books_copy.columns.get_loc(cluster_column)] = clusters\n",
    "    return books_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hdbscan_clustering(embeddings, alpha=0.5, beta=0.5, n_trials=5):\n",
    "    # L2-normalize embeddings so Euclidean â‰ˆ Cosine distance\n",
    "    embeddings_normalized = normalize(embeddings, norm='l2', axis=1)\n",
    "\n",
    "    # Define search space for hyperparameters\n",
    "    min_cluster_sizes = [100,300]\n",
    "    min_samples_list = [100,300]\n",
    "    cluster_selection_epsilons = [0.1, 0.5]\n",
    "\n",
    "    # Generate all possible hyperparameter combinations\n",
    "    all_param_combinations = list(product(min_cluster_sizes, min_samples_list, cluster_selection_epsilons))\n",
    "\n",
    "    # Randomly sample n_trials parameter combinations\n",
    "    sampled_combinations = random.sample(all_param_combinations, min(n_trials, len(all_param_combinations)))\n",
    "\n",
    "    best_combined_score = float(\"-inf\")  # Higher is better\n",
    "    best_params = None\n",
    "    best_clusterer = None\n",
    "    best_clusters = None\n",
    "\n",
    "    for min_cluster_size, min_samples, cluster_selection_epsilon in sampled_combinations:\n",
    "        # Perform clustering with soft clustering\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            cluster_selection_epsilon=cluster_selection_epsilon, \n",
    "            metric='euclidean',\n",
    "            prediction_data=True,\n",
    "            core_dist_n_jobs=1,\n",
    "            cluster_selection_method='leaf'\n",
    "        )\n",
    "        clusterer.fit_predict(embeddings_normalized)\n",
    "\n",
    "        soft_clusters = hdbscan.prediction.all_points_membership_vectors(clusterer)\n",
    "        \n",
    "        hard_clusters = np.array([\n",
    "            -1 if max(membership) < 0.1 else np.argmax(membership)\n",
    "            for membership in soft_clusters\n",
    "        ])\n",
    "\n",
    "        if len(set(hard_clusters) - {-1}) > 1:\n",
    "            db_index = davies_bouldin_score(embeddings_normalized, hard_clusters)\n",
    "            ch_index = calinski_harabasz_score(embeddings_normalized, hard_clusters)\n",
    "        else:\n",
    "            db_index, ch_index = float(\"inf\"), 0  \n",
    "\n",
    "        combined_score = alpha * (1 / db_index) + beta * ch_index\n",
    "\n",
    "        print(f\"min_cluster_size={min_cluster_size}, min_samples={min_samples}, epsilon={cluster_selection_epsilon}, DB={db_index:.3f}, CH={ch_index:.3f}, Combined={combined_score:.3f}\")\n",
    "\n",
    "        if combined_score > best_combined_score:\n",
    "            best_combined_score = combined_score\n",
    "            best_params = (min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "            best_clusterer = clusterer\n",
    "            best_clusters = hard_clusters\n",
    "\n",
    "    print(\"\\nBest Params:\", best_params, \"Best Combined Score:\", best_combined_score)\n",
    "    return best_clusters, best_clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books, embedding_matrix = load_data()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings = apply_umap(scaled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, clusterer = perform_hdbscan_clustering(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign outliers to the nearest cluster\n",
    "outlier_indices = np.where(clusters == -1)[0]  # Find indices of outliers\n",
    "if len(outlier_indices) > 0:\n",
    "    print(f\"Assigning {len(outlier_indices)} outliers to the nearest cluster...\")\n",
    "    clusters[outlier_indices] = hdbscan.approximate_predict(clusterer, umap_embeddings[outlier_indices])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(umap_embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = assign_clusters_to_books(books, indices, clusters, cluster_column=\"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_to_index = {book_id: idx for idx, book_id in enumerate(books['book_id'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting 3D\n",
    "def plot_3d_embeddings(embeddings, clusters):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter(embeddings[:, 0], embeddings[:, 1], embeddings[:, 2], \n",
    "                         c=clusters, cmap='tab10', s=50, alpha=0.6, edgecolor='w')\n",
    "\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    ax.set_zlabel('Component 3')\n",
    "    plt.title('3D Clustering of Books')\n",
    "    legend1 = ax.legend(*scatter.legend_elements(), title=\"Cluster\")\n",
    "    ax.add_artist(legend1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_embeddings(umap_embeddings[:, :3], clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbi_score = davies_bouldin_score(umap_embeddings, clusters)\n",
    "print(f\"Davies-Bouldin Index: {dbi_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_score = calinski_harabasz_score(umap_embeddings, clusters)\n",
    "print(f\"Calinski-Harabasz Index: {ch_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = silhouette_score(umap_embeddings, clusters)\n",
    "print(f\"Silhouette Score: {sh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = umap_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/umap_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(umap_embeddings, f)\n",
    "\n",
    "faiss.write_index(faiss_index, '../Pickle/faiss_index.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/book_id_to_index.pkl', 'wb') as f:\n",
    "    pickle.dump(book_id_to_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/clustered_books.pkl', 'wb') as f:\n",
    "    pickle.dump(books, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_books(\n",
    "    book_id, \n",
    "    umap_embeddings, \n",
    "    clusters, \n",
    "    books, \n",
    "    book_id_to_index, \n",
    "    top_n=10, \n",
    "    min_cluster_size=5, \n",
    "    allow_outside_cluster=True\n",
    "):\n",
    "\n",
    "    # Check if the book exists in the mapping\n",
    "    if book_id not in book_id_to_index:\n",
    "        raise ValueError(f\"Book ID {book_id} not found in the dataset.\")\n",
    "\n",
    "    # Get the index of the input book\n",
    "    book_index = book_id_to_index[book_id]\n",
    "\n",
    "    # Get the cluster of the input book\n",
    "    book_cluster = clusters[book_index]\n",
    "\n",
    "    # Get indices of all books in the same cluster\n",
    "    cluster_indices = np.where(clusters == book_cluster)[0]\n",
    "\n",
    "    # Handle cases where the cluster is too small or contains only the input book\n",
    "    if len(cluster_indices) < min_cluster_size:\n",
    "        if allow_outside_cluster:\n",
    "            print(f\"Cluster {book_cluster} is too small (size: {len(cluster_indices)}). Looking outside the cluster.\")\n",
    "            # Look outside the cluster (e.g., consider all books)\n",
    "            cluster_indices = np.arange(len(clusters))\n",
    "        else:\n",
    "            print(f\"Cluster {book_cluster} is too small (size: {len(cluster_indices)}). No recommendations available.\")\n",
    "            return []\n",
    "\n",
    "    # Create a FAISS index for the selected cluster\n",
    "    cluster_embeddings = umap_embeddings[cluster_indices]\n",
    "    faiss_index_cluster = faiss.IndexFlatL2(cluster_embeddings.shape[1])\n",
    "    faiss_index_cluster.add(cluster_embeddings)\n",
    "\n",
    "    # Find the nearest neighbors (including the input book)\n",
    "    distances, indices = faiss_index_cluster.search(\n",
    "        umap_embeddings[book_index].reshape(1, -1), top_n + 1\n",
    "    )\n",
    "\n",
    "    # Exclude the input book from the results\n",
    "    recommendations = []\n",
    "    for i in range(1, len(indices[0])):  # Skip the first result (input book)\n",
    "        idx = cluster_indices[indices[0][i]]  # Map back to the original index\n",
    "        book_id_rec = books.iloc[idx]['book_id']\n",
    "        similarity_score = 1 / (1 + distances[0][i])  # Convert distance to similarity score\n",
    "        recommendations.append((book_id_rec, similarity_score))\n",
    "\n",
    "    return recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
