{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import faiss  \n",
    "import pickle\n",
    "import umap\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from itertools import product \n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  \n",
    "sns.set_theme(style=\"white\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    books_list = []\n",
    "\n",
    "    with open('../Pickle/books.pkl', 'rb') as file:\n",
    "        while True:\n",
    "            try:\n",
    "                chunk = pickle.load(file)\n",
    "                books_list.append(chunk)\n",
    "            except EOFError:\n",
    "                break  \n",
    "    books = pd.concat(books_list, ignore_index=True)\n",
    "    books = books.drop_duplicates(subset='title', keep='first')\n",
    "    embedding_matrix = np.vstack(books['embeddings'].values)\n",
    "    return books, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_umap(embeddings, n_components=10, n_neighbors=300, min_dist=0.0):\n",
    "    embeddings = np.asarray(embeddings, dtype=np.float32)  \n",
    "\n",
    "    umap_model = umap.UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric='cosine',\n",
    "        low_memory=True, \n",
    "        random_state = 42\n",
    "    )\n",
    "    \n",
    "    return umap_model.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters_to_books(books, soft_clusters, embeddings, top_n=5):\n",
    "    \"\"\"\n",
    "    Return a DataFrame with only book_id, embeddings, and top N cluster memberships.\n",
    "    \n",
    "    Parameters:\n",
    "    - books: Original books DataFrame. Only the 'book_id' column is used here.\n",
    "    - soft_clusters: Soft cluster membership vectors (probabilities for each cluster).\n",
    "    - embeddings: The UMAP normalized embeddings for each book.\n",
    "    - top_n: Number of top clusters to keep.\n",
    "    \n",
    "    Returns:\n",
    "    - clustered_books: A simplified DataFrame with book_id, embedding, and top_clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(soft_clusters, np.ndarray):\n",
    "        soft_clusters = np.array(soft_clusters)\n",
    "\n",
    "    top_clusters_list = []\n",
    "    for cluster_vector in soft_clusters:\n",
    "        top_indices = np.argsort(cluster_vector)[::-1][:top_n]\n",
    "        top_probs = cluster_vector[top_indices]\n",
    "        top_clusters = list(zip(top_indices, top_probs))\n",
    "        top_clusters_list.append(top_clusters)\n",
    "    \n",
    "    clustered_books = pd.DataFrame({\n",
    "        'book_id': books['book_id'].values,\n",
    "        'embedding': [embedding.tolist() for embedding in embeddings], \n",
    "        'top_clusters': top_clusters_list\n",
    "    })\n",
    "    \n",
    "    return clustered_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hdbscan_clustering(embeddings, alpha=0.5, beta=0.5, n_trials=5, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Perform HDBSCAN clustering on the given embeddings using soft clustering (probabilistic membership vectors).\n",
    "    This function avoids precomputing the distance matrix to save memory.\n",
    "\n",
    "    Args:\n",
    "        embeddings (numpy.ndarray): The normalized embeddings (e.g., UMAP embeddings) of the books or items to cluster.\n",
    "        alpha (float, optional): Weight for the Davies-Bouldin Index in the combined score. Default is 0.5.\n",
    "        beta (float, optional): Weight for the Calinski-Harabasz Index in the combined score. Default is 0.5.\n",
    "        n_trials (int, optional): The number of random hyperparameter combinations to try. Default is 5.\n",
    "        n_jobs (int, optional): The number of jobs to run in parallel. Default is -1 (all cores).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - best_soft_clusters (numpy.ndarray): The best soft clusters, each element contains the membership \n",
    "              probabilities for each cluster.\n",
    "            - best_hard_clusters (numpy.ndarray): The best hard cluster labels (assigned clusters).\n",
    "            - best_clusterer (hdbscan.HDBSCAN): The best fitted HDBSCAN model used to generate the soft clusters.\n",
    "            - best_db_score (float): The Davies-Bouldin score for the best clustering.\n",
    "            - best_ch_score (float): The Calinski-Harabasz score for the best clustering.\n",
    "            - best_combined_score (float): The combined score for the best clustering.\n",
    "            - best_params (dict): The hyperparameters that gave the best score.\n",
    "    \"\"\"\n",
    "    min_cluster_sizes = [30, 50, 100]\n",
    "    min_samples_list = [10, 50, 60]\n",
    "    cluster_selection_epsilons = [0.5]\n",
    "\n",
    "    all_param_combinations = list(product(min_cluster_sizes, min_samples_list, cluster_selection_epsilons))\n",
    "    sampled_combinations = random.sample(all_param_combinations, min(n_trials, len(all_param_combinations)))\n",
    "\n",
    "    def evaluate_params(min_cluster_size, min_samples, cluster_selection_epsilon):\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "            metric='euclidean', \n",
    "            prediction_data=True,\n",
    "            core_dist_n_jobs=n_jobs,  \n",
    "            cluster_selection_method='leaf'\n",
    "        )\n",
    "        clusterer.fit(embeddings)\n",
    "\n",
    "        soft_clusters = hdbscan.prediction.all_points_membership_vectors(clusterer)\n",
    "\n",
    "        if len(soft_clusters) > 0:\n",
    "            db_index = davies_bouldin_score(embeddings, clusterer.labels_)\n",
    "            ch_index = calinski_harabasz_score(embeddings, clusterer.labels_)\n",
    "        else:\n",
    "            db_index, ch_index = float(\"inf\"), 0\n",
    "\n",
    "        combined_score = alpha * (1 / db_index) + beta * ch_index\n",
    "\n",
    "        print(f\"min_cluster_size={min_cluster_size}, min_samples={min_samples}, epsilon={cluster_selection_epsilon}, DB={db_index:.3f}, CH={ch_index:.3f}, Combined={combined_score:.3f}\")\n",
    "\n",
    "        return combined_score, db_index, ch_index, soft_clusters, clusterer.labels_, clusterer, (min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "\n",
    "    # Evaluate all parameter combinations in parallel\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(evaluate_params)(min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "        for min_cluster_size, min_samples, cluster_selection_epsilon in sampled_combinations\n",
    "    )\n",
    "\n",
    "    best_index = np.argmax([result[0] for result in results])\n",
    "    best_combined_score, best_db_score, best_ch_score, best_soft_clusters, best_hard_clusters, best_clusterer, best_params = results[best_index]\n",
    "    print(\"\\nBest Hyperparameters:\")\n",
    "    print(f\"min_cluster_size={best_params[0]}, min_samples={best_params[1]}, epsilon={best_params[2]}\")\n",
    "    print(\"Best Combined Score:\", best_combined_score)\n",
    "    print(\"Best Davies-Bouldin Score:\", best_db_score)\n",
    "    print(\"Best Calinski-Harabasz Score:\", best_ch_score)\n",
    "\n",
    "    return best_soft_clusters, best_hard_clusters, best_clusterer, best_db_score, best_ch_score, best_combined_score, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "books, embedding_matrix = load_data()\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings = apply_umap(scaled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings_normalized = normalize(umap_embeddings, norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters:\n",
      "min_cluster_size=100, min_samples=50, epsilon=0.5\n",
      "Best Combined Score: 647.888597436619\n",
      "Best Davies-Bouldin Score: 0.7294188426377485\n",
      "Best Calinski-Harabasz Score: 1294.4062404345182\n"
     ]
    }
   ],
   "source": [
    "best_soft_clusters, best_hard_clusters, best_clusterer, best_db_score, best_ch_score, best_combined_score, best_params = perform_hdbscan_clustering(umap_embeddings_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(umap_embeddings_normalized.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_books = assign_clusters_to_books(books, best_soft_clusters, umap_embeddings_normalized, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of books with at least one cluster probability ≥ 0.01: 66.62%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def has_high_probability(cluster_list):\n",
    "    return any(prob >= 0.01 for _, prob in cluster_list)\n",
    "\n",
    "count_high_prob = clustered_books['top_clusters'].apply(has_high_probability).sum()\n",
    "\n",
    "total_books = len(clustered_books)\n",
    "\n",
    "percentage = (count_high_prob / total_books) * 100\n",
    "\n",
    "print(f\"Percentage of books with at least one cluster probability ≥ 0.01: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of books with at least one cluster probability ≥ 0.01: 19.74%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def has_high_probability(cluster_list):\n",
    "    return any(prob >= 0.1 for _, prob in cluster_list)\n",
    "\n",
    "count_high_prob = clustered_books['top_clusters'].apply(has_high_probability).sum()\n",
    "\n",
    "total_books = len(clustered_books)\n",
    "\n",
    "percentage = (count_high_prob / total_books) * 100\n",
    "\n",
    "print(f\"Percentage of books with at least one cluster probability ≥ 0.01: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_to_index = {book_id: idx for idx, book_id in enumerate(books['book_id'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = umap_embeddings_normalized.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(umap_embeddings_normalized)\n",
    "with open('../Pickle/umap_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(umap_embeddings_normalized, f)\n",
    "faiss.write_index(faiss_index, '../Pickle/faiss_index.bin')\n",
    "with open('../Pickle/book_id_to_index.pkl', 'wb') as f:\n",
    "    pickle.dump(book_id_to_index, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/clustered_books.pkl', 'wb') as f:\n",
    "    pickle.dump(clustered_books, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "compactness = []\n",
    "for label in set(best_hard_clusters):\n",
    "    if label != -1:\n",
    "        cluster_points = umap_embeddings_normalized[best_hard_clusters == label]\n",
    "        centroid = cluster_points.mean(axis=0)\n",
    "        distances = euclidean_distances(cluster_points, centroid.reshape(1, -1))\n",
    "        compactness.append(np.mean(distances))\n",
    "\n",
    "separation = []\n",
    "cluster_centroids = [\n",
    "    umap_embeddings_normalized[best_hard_clusters == label].mean(axis=0)\n",
    "    for label in set(best_hard_clusters) if label != -1\n",
    "]\n",
    "\n",
    "for i in range(len(cluster_centroids)):\n",
    "    for j in range(i + 1, len(cluster_centroids)):\n",
    "        dist = euclidean_distances(\n",
    "            [cluster_centroids[i]], [cluster_centroids[j]]\n",
    "        )[0][0]\n",
    "        separation.append(dist)\n",
    "\n",
    "print(f\"Average Compactness: {np.mean(compactness):.4f}\")\n",
    "print(f\"Average Separation: {np.mean(separation):.4f}\")\n",
    "\n",
    "total_points = len(best_hard_clusters)\n",
    "outlier_points = np.sum(best_hard_clusters == -1)\n",
    "outlier_percentage = (outlier_points / total_points) * 100\n",
    "print(f\"Outliers: {outlier_points} / {total_points}\")\n",
    "print(f\"Percentage of Outliers: {outlier_percentage:.2f}%\")\n",
    "dbi_score = davies_bouldin_score(umap_embeddings_normalized, best_hard_clusters)\n",
    "print(f\"Davies-Bouldin Index: {dbi_score}\")\n",
    "ch_score = calinski_harabasz_score(umap_embeddings_normalized, best_hard_clusters)\n",
    "print(f\"Calinski-Harabasz Index: {ch_score}\")\n",
    "sh = silhouette_score(umap_embeddings_normalized, best_hard_clusters)\n",
    "print(f\"Silhouette Score: {sh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "reduced_pca_embeddings = pca.fit_transform(umap_embeddings_normalized) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Create a DataFrame for 3D plotting\n",
    "df_plot_3d = pd.DataFrame({\n",
    "    'PCA1': reduced_pca_embeddings[:, 0],  # First principal component\n",
    "    'PCA2': reduced_pca_embeddings[:, 1],  # Second principal component\n",
    "    'PCA3': reduced_pca_embeddings[:, 2],  # Third principal component\n",
    "    'Cluster': best_hard_clusters  # Ensure this has the same length as the embeddings\n",
    "})\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    df_plot_3d,\n",
    "    x='PCA1',\n",
    "    y='PCA2',\n",
    "    z='PCA3',\n",
    "    color='Cluster',\n",
    "    title='3D PCA Embeddings Coloured by Cluster',\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='PCA Dimension 1',\n",
    "        yaxis_title='PCA Dimension 2',\n",
    "        zaxis_title='PCA Dimension 3'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
