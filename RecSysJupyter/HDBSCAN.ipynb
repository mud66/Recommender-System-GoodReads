{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import faiss  \n",
    "import hdbscan  \n",
    "import pickle\n",
    "import umap\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from itertools import product \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  \n",
    "sns.set_theme(style=\"white\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    books_list = []\n",
    "\n",
    "    with open('../Pickle/books.pkl', 'rb') as file:\n",
    "        while True:\n",
    "            try:\n",
    "                chunk = pickle.load(file)\n",
    "                books_list.append(chunk)\n",
    "            except EOFError:\n",
    "                break  \n",
    "    books = pd.concat(books_list, ignore_index=True)\n",
    "    books = books.drop_duplicates(subset='title', keep='first')\n",
    "    embedding_matrix = np.vstack(books['embeddings'].values)\n",
    "    return books, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_umap(embeddings, n_components=20, n_neighbors=200, min_dist=0.001):\n",
    "    embeddings = np.asarray(embeddings, dtype=np.float32)  \n",
    "\n",
    "    umap_model = umap.UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric='cosine',\n",
    "        low_memory=True, \n",
    "        random_state = 42\n",
    "    )\n",
    "    \n",
    "    return umap_model.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters_to_books(books, indices, clusters, cluster_column=\"cluster\"):\n",
    "    books_copy = books.copy()\n",
    "    books_copy[cluster_column] = -1\n",
    "    books_copy.iloc[indices, books_copy.columns.get_loc(cluster_column)] = clusters\n",
    "    return books_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hdbscan_clustering(embeddings, alpha=0.5, beta=0.5, n_trials=9):\n",
    "    # L2-normalize embeddings so Euclidean â‰ˆ Cosine distance\n",
    "    embeddings_normalized = normalize(embeddings, norm='l2', axis=1)\n",
    "\n",
    "    # Define search space for hyperparameters\n",
    "    min_cluster_sizes = [800, 900]\n",
    "    min_samples_list = [800, 900]\n",
    "\n",
    "    # Generate all possible hyperparameter combinations\n",
    "    all_param_combinations = list(product(min_cluster_sizes, min_samples_list))\n",
    "\n",
    "    # Randomly sample n_trials parameter combinations\n",
    "    sampled_combinations = random.sample(all_param_combinations, min(n_trials, len(all_param_combinations)))\n",
    "\n",
    "    best_combined_score = float(\"-inf\")  # Higher is better\n",
    "    best_params = None\n",
    "    best_clusterer = None\n",
    "    best_clusters = None\n",
    "\n",
    "    for min_cluster_size, min_samples in sampled_combinations:\n",
    "        # Perform clustering with soft clustering enabled\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            metric='euclidean',\n",
    "            prediction_data=True,\n",
    "            core_dist_n_jobs=1,\n",
    "            cluster_selection_method= 'leaf'\n",
    "        )\n",
    "        clusterer.fit_predict(embeddings_normalized)\n",
    "\n",
    "        soft_clusters = hdbscan.prediction.all_points_membership_vectors(clusterer)\n",
    "        \n",
    "        hard_clusters = np.array([\n",
    "            -1 if max(membership) < 0.1 else np.argmax(membership)\n",
    "            for membership in soft_clusters\n",
    "        ])\n",
    "\n",
    "        if len(set(hard_clusters) - {-1}) > 1:\n",
    "            db_index = davies_bouldin_score(embeddings_normalized, hard_clusters)\n",
    "            ch_index = calinski_harabasz_score(embeddings_normalized, hard_clusters)\n",
    "        else:\n",
    "            db_index, ch_index = float(\"inf\"), 0  \n",
    "\n",
    "        combined_score = alpha * (1 / db_index) + beta * ch_index\n",
    "\n",
    "        print(f\"min_cluster_size={min_cluster_size}, min_samples={min_samples}, DB={db_index:.3f}, CH={ch_index:.3f}, Combined={combined_score:.3f}\")\n",
    "\n",
    "        if combined_score > best_combined_score:\n",
    "            best_combined_score = combined_score\n",
    "            best_params = (min_cluster_size, min_samples)\n",
    "            best_clusterer = clusterer\n",
    "            best_clusters = hard_clusters\n",
    "\n",
    "    print(\"\\nBest Params:\", best_params, \"Best Combined Score:\", best_combined_score)\n",
    "    return best_clusters, best_clusterer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "books, embedding_matrix = load_data()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maddy\\anaconda3\\envs\\goodreads\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "umap_embeddings = apply_umap(scaled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, clusterer = perform_hdbscan_clustering(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(umap_embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = assign_clusters_to_books(books, indices, clusters, cluster_column=\"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_to_index = {book_id: idx for idx, book_id in enumerate(books['book_id'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting 3D\n",
    "def plot_3d_embeddings(embeddings, clusters):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter(embeddings[:, 0], embeddings[:, 1], embeddings[:, 2], \n",
    "                         c=clusters, cmap='tab10', s=50, alpha=0.6, edgecolor='w')\n",
    "\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    ax.set_zlabel('Component 3')\n",
    "    plt.title('3D Clustering of Books')\n",
    "    legend1 = ax.legend(*scatter.legend_elements(), title=\"Cluster\")\n",
    "    ax.add_artist(legend1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_embeddings(umap_embeddings[:, :3], clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbi_score = davies_bouldin_score(umap_embeddings, clusters)\n",
    "print(f\"Davies-Bouldin Index: {dbi_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_score = calinski_harabasz_score(umap_embeddings, clusters)\n",
    "print(f\"Calinski-Harabasz Index: {ch_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = umap_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/umap_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(umap_embeddings, f)\n",
    "\n",
    "faiss.write_index(faiss_index, '../Pickle/faiss_index.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/book_id_to_index.pkl', 'wb') as f:\n",
    "    pickle.dump(book_id_to_index, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustered = (hdbscan_labels >= 0)\n",
    "# (\n",
    "#     adjusted_rand_score(mnist.target[clustered], hdbscan_labels[clustered]),\n",
    "#     adjusted_mutual_info_score(mnist.target[clustered], hdbscan_labels[clustered])\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
