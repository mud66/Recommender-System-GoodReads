{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maddy\\anaconda3\\envs\\goodreads\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_list = []\n",
    "\n",
    "with open('../Pickle/books.pkl', 'rb') as file:\n",
    "    while True:\n",
    "        try:\n",
    "            chunk = pickle.load(file)\n",
    "            books_list.append(chunk)\n",
    "        except EOFError:\n",
    "            break\n",
    "books = pd.concat(books_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books[books['filtered_genres'].apply(lambda x: bool(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_language_codes = ['', ' ', 'eng', 'en-US', 'en-GB', '--', 'en-CA', 'en-IN']\n",
    "books = books[books['language_code'].isin(include_language_codes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = LanguageDetectorBuilder.from_all_languages().build()\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detects whether the input text is in English using the LanguageDetector.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The text to detect language for.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the text is in English, False otherwise.\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return False\n",
    "    text = text[:250]\n",
    "\n",
    "    try:\n",
    "        detected_lang = detector.detect_language_of(text)\n",
    "        return detected_lang == Language.ENGLISH\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "chunk_size = 5000\n",
    "save_every = 10\n",
    "output_pickle = \"../Pickle/books_filtered.pkl\"\n",
    "\n",
    "if os.path.exists(output_pickle):\n",
    "    books_filtered = pd.read_pickle(output_pickle)\n",
    "    processed_books = set(books_filtered[\"book_id\"])\n",
    "else:\n",
    "    books_filtered = pd.DataFrame()\n",
    "    processed_books = set()\n",
    "\n",
    "books_to_process = books[~books[\"book_id\"].isin(processed_books)]\n",
    "\n",
    "if books_to_process.empty:\n",
    "    print(\"all books processed\")\n",
    "else:\n",
    "    print(f\"Processing {len(books_to_process)}\")\n",
    "\n",
    "    buffer = []\n",
    "    for i, start in enumerate(tqdm(range(0, len(books_to_process), chunk_size), desc=\"processing\")):\n",
    "        end = min(start + chunk_size, len(books_to_process))\n",
    "        books_chunk = books_to_process.iloc[start:end].copy()\n",
    "        books_chunk[\"is_english\"] = books_chunk[\"description\"].progress_apply(detect_language)\n",
    "        books_chunk = books_chunk[books_chunk[\"is_english\"]].drop(columns=[\"is_english\"])\n",
    "        buffer.append(books_chunk)\n",
    "\n",
    "        if (i + 1) % save_every == 0 or (i + 1) == len(range(0, len(books_to_process), chunk_size)):\n",
    "            buffer_df = pd.concat(buffer, ignore_index=True)\n",
    "            books_filtered = pd.concat([books_filtered, buffer_df], ignore_index=True)\n",
    "            books_filtered.to_pickle(output_pickle)\n",
    "            buffer = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_books = pd.read_pickle(\"../Pickle/books_filtered.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = eng_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['combined_features'] = books.apply(\n",
    "    lambda row: f\"{row['title']} by {row['authors']}, \" +\n",
    "                f\"Description: {row['description']}, \" +\n",
    "                f\"Shelves: {row['expanded_shelves']}\" +\n",
    "                f\"Genres: {row['filtered_genres']}\",\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def save_embeddings_incrementally(books_df, model, interval=100):\n",
    "    \"\"\"\n",
    "    Generates and saves sentence embeddings for book descriptions incrementally.\n",
    "    \n",
    "    Parameters:\n",
    "    - books_df (pd.DataFrame): DataFrame containing book_id and combined_features columns.\n",
    "    - model (SentenceTransformer): Preloaded SentenceTransformer model used for encoding.\n",
    "    - interval (int): Number of new embeddings after which to save progress.\n",
    "    \n",
    "    Returns:\n",
    "    - None. Saves embeddings incrementally to a pickle file.\n",
    "    \"\"\"\n",
    "    embeddings_file = '../Pickle/embeddings.pkl'\n",
    "    \n",
    "    if os.path.exists(embeddings_file):\n",
    "        embeddings_df = pd.read_pickle(embeddings_file)\n",
    "    else:\n",
    "        embeddings_df = pd.DataFrame(columns=['book_id', 'embeddings'])\n",
    "    \n",
    "    books_df = books_df.dropna(subset=['combined_features']).reset_index(drop=True)\n",
    "    new_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(len(books_df)), desc=\"processing\"):\n",
    "        book_id = books_df.at[i, 'book_id']\n",
    "        if book_id in embeddings_df['book_id'].values:\n",
    "            continue\n",
    "        \n",
    "        embedding = model.encode(books_df.at[i, 'combined_features'])\n",
    "        new_row = {'book_id': book_id, 'embeddings': embedding}\n",
    "        new_embeddings.append(new_row)\n",
    "        \n",
    "        if len(new_embeddings) % interval == 0:\n",
    "            new_embeddings_df = pd.DataFrame(new_embeddings)\n",
    "            embeddings_df = pd.concat([embeddings_df, new_embeddings_df], ignore_index=True)\n",
    "            embeddings_df.to_pickle(embeddings_file)\n",
    "            new_embeddings = []\n",
    "    \n",
    "    if new_embeddings:\n",
    "        new_embeddings_df = pd.DataFrame(new_embeddings)\n",
    "        embeddings_df = pd.concat([embeddings_df, new_embeddings_df], ignore_index=True)\n",
    "        embeddings_df.to_pickle(embeddings_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings_incrementally(books, model, interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.read_pickle('../Pickle/embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df=embeddings_df.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577082/577082 [00:02<00:00, 210510.98it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_df.set_index('book_id', inplace=True)\n",
    "\n",
    "def get_embedding(book_id):\n",
    "    try:\n",
    "        return embeddings_df.at[book_id, 'embeddings']\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "books['embeddings'] = books['book_id'].progress_apply(get_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577082/577082 [03:29<00:00, 2760.12it/s] \n"
     ]
    }
   ],
   "source": [
    "chunk_size = 10000\n",
    "num_chunks = len(books) // chunk_size + 1\n",
    "progress_bar = tqdm(total=len(books))\n",
    "with open('../Pickle/books.pkl', 'wb') as file:\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size\n",
    "        chunk = books.iloc[start_idx:end_idx]\n",
    "        for _, row in chunk.iterrows():\n",
    "            progress_bar.update(1)\n",
    "        if i == 0:\n",
    "            pickle.dump(chunk, file)\n",
    "        else:\n",
    "            pickle.dump(chunk, file)\n",
    "progress_bar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
