{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_pickle('../Pickle/reviews.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses input text by performing the following steps:\n",
    "    1. Removes any URLs from the text.\n",
    "    2. Tokenizes the text into individual words.\n",
    "    3. Filters out non-alphanumeric words and stop words (defined by the `stop_words` list).\n",
    "    4. Joins the filtered words back into a single string of text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    str: The cleaned and filtered text.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    cleaned_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text):\n",
    "    \"\"\"\n",
    "    Truncates the input text to a maximum length of 512 tokens using a tokenizer. \n",
    "    The text is tokenized, truncated to the specified length, and then decoded back to a string.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be truncated.\n",
    "\n",
    "    Returns:\n",
    "    str: The truncated text.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    truncated_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
    "    return truncated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['cleaned_text'] = reviews['review_text'].progress_apply(preprocess_text)\n",
    "reviews['truncated_text'] = reviews['cleaned_text'].progress_apply(truncate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.dropna(subset=['truncated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sentiment_incrementally(reviews_df, sentiment_pipeline, interval=300):\n",
    "    \"\"\"\n",
    "    Processes the reviews dataframe and applies sentiment analysis incrementally in batches.\n",
    "    The results are saved in a pickle file to avoid recomputation.\n",
    "\n",
    "    Parameters:\n",
    "    reviews_df (pd.DataFrame): The dataframe containing the reviews to be processed, including a column 'truncated_text'.\n",
    "    sentiment_pipeline (callable): A sentiment analysis pipeline that returns sentiment labels and confidence scores.\n",
    "    interval (int): The batch size to process and save at a time (default is 300).\n",
    "\n",
    "    Returns:\n",
    "    None: The function saves the processed sentiment analysis results to a pickle file.\n",
    "    \"\"\"\n",
    "    sentiment_file = '../Pickle/review_score.pkl'\n",
    "    \n",
    "    if os.path.exists(sentiment_file):\n",
    "        reviews_with_sentiment = pd.read_pickle(sentiment_file)\n",
    "    else:\n",
    "        reviews_with_sentiment = pd.DataFrame(columns=['review_id', 'sentiment', 'confidence'])\n",
    "    \n",
    "    reviews_df = reviews_df.dropna(subset=['truncated_text']).reset_index(drop=True)\n",
    "\n",
    "    processed_review_ids = set(reviews_with_sentiment['review_id'].values)\n",
    "    \n",
    "    new_sentiments = []\n",
    "\n",
    "    for i in tqdm(range(len(reviews_df)), desc=\"Processing\"):\n",
    "        review_id = reviews_df.at[i, 'review_id']\n",
    "        \n",
    "        if review_id in processed_review_ids:\n",
    "            continue\n",
    "        \n",
    "        review_text = reviews_df.at[i, 'truncated_text']\n",
    "        sentiment_result = sentiment_pipeline(review_text)[0]\n",
    "        sentiment = sentiment_result['label']\n",
    "        confidence = sentiment_result['score']\n",
    "        \n",
    "        new_sentiments.append({'review_id': review_id, 'sentiment': sentiment, 'confidence': confidence})\n",
    "        \n",
    "        processed_review_ids.add(review_id)\n",
    "        \n",
    "        if len(new_sentiments) % interval == 0:\n",
    "            new_sentiments_df = pd.DataFrame(new_sentiments)\n",
    "            reviews_with_sentiment = pd.concat([reviews_with_sentiment, new_sentiments_df], ignore_index=True)\n",
    "            reviews_with_sentiment.to_pickle(sentiment_file)\n",
    "            new_sentiments = [] \n",
    "            print(f\"saved batch {i + 1}/{len(reviews_df)}.\")\n",
    "\n",
    "    if new_sentiments:\n",
    "        new_sentiments_df = pd.DataFrame(new_sentiments)\n",
    "        reviews_with_sentiment = pd.concat([reviews_with_sentiment, new_sentiments_df], ignore_index=True)\n",
    "        reviews_with_sentiment.to_pickle(sentiment_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sentiment = pd.read_pickle('../Pickle/review_score.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adjusts the confidence score in the 'review_sentiment' dataframe based on the sentiment label.\n",
    "\n",
    "For rows where the sentiment label is 0, the confidence score is inverted (1 - confidence).\n",
    "Otherwise, the confidence score remains unchanged.\n",
    "\n",
    "Parameters:\n",
    "- review_sentiment (pd.DataFrame): A dataframe containing the 'sentiment' and 'confidence' columns.\n",
    "\n",
    "Returns:\n",
    "- pd.DataFrame: The dataframe with an updated 'confidence_score' column.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "review_sentiment['confidence_score'] = [\n",
    "    1 - row['confidence'] if row['sentiment'] == 0 else row ['confidence']\n",
    "    for _,row in review_sentiment.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sentiment.to_pickle('../Pickle/review_score.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
