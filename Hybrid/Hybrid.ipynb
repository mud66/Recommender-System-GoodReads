{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import joblib\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary data\n",
    "def load_data():\n",
    "    books_list = []\n",
    "    with open('../Pickle/books.pkl', 'rb') as file:\n",
    "        while True:\n",
    "            try:\n",
    "                chunk = pickle.load(file)\n",
    "                books_list.append(chunk)\n",
    "            except EOFError:\n",
    "                break\n",
    "    books = pd.concat(books_list, ignore_index=True).drop_duplicates(subset='title', keep='first')\n",
    "\n",
    "    interactions = pd.read_pickle('../Pickle/interactions.pkl')\n",
    "    read = pd.read_pickle('../Pickle/read.pkl')\n",
    "    reviews = pd.read_pickle('../Pickle/reviews.pkl')\n",
    "    \n",
    "    with open('../Pickle/umap_embeddings.pkl', 'rb') as f:\n",
    "        umap_embeddings = pickle.load(f)\n",
    "    faiss_index = faiss.read_index('../Pickle/faiss_index.bin')\n",
    "\n",
    "    with open('../Pickle/book_id_to_index.pkl', 'rb') as f:\n",
    "        book_id_to_index = pickle.load(f)\n",
    "    with open('../Pickle/user_id_to_index_gat.pkl', 'rb') as f:\n",
    "        user_id_to_index_gat = pickle.load(f)\n",
    "    with open('../Pickle/book_id_to_index_gat.pkl', 'rb') as f:\n",
    "        book_id_to_index_gat = pickle.load(f)\n",
    "    with open('../Pickle/gat_embeddings.pkl', 'rb') as f:\n",
    "        all_embeddings = pickle.load(f)\n",
    "    \n",
    "        # Filter read and interactions for valid books\n",
    "    read = read[read['is_read'] == 1]\n",
    "    valid_book_ids = set(books['book_id'])\n",
    "    interactions = interactions[interactions['book_id'].isin(valid_book_ids)]\n",
    "    read = read[read['book_id'].isin(valid_book_ids)]\n",
    "    reviews = reviews[reviews['book_id'].isin(valid_book_ids)]\n",
    "\n",
    "    return books, interactions, read, reviews, umap_embeddings, faiss_index, book_id_to_index, user_id_to_index_gat, book_id_to_index_gat, all_embeddings, reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = pd.read_pickle('../Pickle/read.pkl')\n",
    "interactions = pd.read_pickle('../Pickle/interactions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GAT model\n",
    "def load_gat_model():\n",
    "    from Gatv2Conv import GATModel\n",
    "    model = GATModel(\n",
    "        in_channels=32,  # Input features per node\n",
    "        hidden_channels=25,\n",
    "        out_channels=1,\n",
    "        num_heads=25,\n",
    "        edge_feature_dim=386  # Edge feature dimension\n",
    "    )\n",
    "    model.load_state_dict(torch.load('../RecSysJupyter/gat_model.pth'))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation function for HDBSCAN (Content-based)\n",
    "def recommend_books_HDBSCAN(book_id, books, umap_embeddings, faiss_index, book_id_to_index, top_n=5):\n",
    "    if book_id not in book_id_to_index:\n",
    "        return []\n",
    "\n",
    "    book_idx = book_id_to_index[book_id]\n",
    "    distances, indices = faiss_index.search(np.array([umap_embeddings[book_idx]]), top_n + 1)\n",
    "    recommendations = []\n",
    "\n",
    "    for idx, dist in zip(indices[0][1:], distances[0][1:]):  # Exclude the book itself\n",
    "        if idx >= len(books):\n",
    "            continue  # Skip out-of-bounds indices\n",
    "\n",
    "        recommended_book = books.iloc[idx]\n",
    "        explanation = f\"Similarity Score: {round(1 / (1 + dist), 3)}\"\n",
    "        recommendations.append({\n",
    "            \"book_id\": recommended_book[\"book_id\"],\n",
    "            \"title\": recommended_book[\"title\"],\n",
    "            \"authors\": ', '.join(recommended_book[\"authors\"]) if isinstance(recommended_book[\"authors\"], list) else recommended_book[\"authors\"],\n",
    "            \"predicted_rating\": \"N/A\",\n",
    "            \"explanation\": explanation\n",
    "        })\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_NMF(nmf_model, interactions, user_id, books_read, books, n_recommendations=5):\n",
    "    all_books = interactions['book_id'].unique()  # Include all books, no exclusions\n",
    "\n",
    "    # Predict ratings for all books\n",
    "    user_predictions = [\n",
    "        (book_id, nmf_model.predict(uid=user_id, iid=book_id).est) for book_id in all_books\n",
    "    ]\n",
    "\n",
    "    user_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_books = user_predictions[:n_recommendations]\n",
    "\n",
    "    recommendations = []\n",
    "    for book_id, rating in top_books:\n",
    "        book_info = books.loc[books['book_id'] == book_id, ['title', 'authors']].values[0]\n",
    "        recommendations.append({\n",
    "            \"book_id\": book_id,\n",
    "            \"title\": book_info[0],\n",
    "            \"authors\": ', '.join(book_info[1]) if isinstance(book_info[1], list) else book_info[1],\n",
    "            \"predicted_rating\": rating,\n",
    "            \"explanation\": \"N/A\"  # Add an explanation if needed\n",
    "        })\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_GAT(user_id, unread_book_ids, all_embeddings, user_id_to_index, book_id_to_index, books_df, top_n=5):\n",
    "    all_books = unread_book_ids  # Include all books (no exclusion logic)\n",
    "\n",
    "    user_index = user_id_to_index.get(user_id)\n",
    "    if user_index is None:\n",
    "        raise ValueError(f\"User ID {user_id} not found in index mappings.\")\n",
    "\n",
    "    user_embedding = all_embeddings[user_index]\n",
    "    predictions = []\n",
    "\n",
    "    for book_id in all_books:\n",
    "        book_index = book_id_to_index.get(book_id)\n",
    "        if book_index is None:\n",
    "            continue\n",
    "\n",
    "        book_embedding = all_embeddings[book_index]\n",
    "        predicted_rating = np.dot(user_embedding, book_embedding)\n",
    "        predicted_rating = np.expm1(predicted_rating)  # Denormalize\n",
    "\n",
    "        book_title = books_df.loc[books_df['book_id'] == book_id, 'title'].values[0]\n",
    "        predictions.append({\n",
    "            \"book_id\": book_id,\n",
    "            \"title\": book_title,\n",
    "            \"authors\": ', '.join(books_df.loc[books_df['book_id'] == book_id, 'authors'].values[0]) if isinstance(books_df.loc[books_df['book_id'] == book_id, 'authors'].values[0], list) else books_df.loc[books_df['book_id'] == book_id, 'authors'].values[0],\n",
    "            \"predicted_rating\": predicted_rating,\n",
    "            \"explanation\": \"N/A\"  # Add an explanation if needed\n",
    "        })\n",
    "\n",
    "    top_recommendations = sorted(predictions, key=lambda x: x[\"predicted_rating\"], reverse=True)[:top_n]\n",
    "    return top_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for weighted merge\n",
    "def merge_recommendations_weighted(gat_recommendations, nmf_recommendations, final_size=5, gat_priority=3):\n",
    "    combined = []\n",
    "    seen_books = set()\n",
    "\n",
    "    # Step 1: Add top GAT recommendations (prioritised)\n",
    "    for rec in gat_recommendations:\n",
    "        if rec not in seen_books:\n",
    "            combined.append(rec)\n",
    "            seen_books.add(rec)\n",
    "        if len(combined) == gat_priority:\n",
    "            break\n",
    "\n",
    "    # Step 2: Add top NMF recommendations to balance\n",
    "    for rec in nmf_recommendations:\n",
    "        if rec not in seen_books:\n",
    "            combined.append(rec)\n",
    "            seen_books.add(rec)\n",
    "        if len(combined) == final_size:\n",
    "            break\n",
    "\n",
    "    # Step 3: If still not enough, fill from remaining unique recommendations\n",
    "    all_recs = gat_recommendations + nmf_recommendations\n",
    "    for rec in all_recs:\n",
    "        if rec not in seen_books:\n",
    "            combined.append(rec)\n",
    "            seen_books.add(rec)\n",
    "        if len(combined) == final_size:\n",
    "            break\n",
    "\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main recommendation logic with user validity checks\n",
    "def recommend_for_user(user_id, books, interactions, read, umap_embeddings, faiss_index, book_id_to_index, user_id_to_index_gat, book_id_to_index_gat, all_embeddings, reviews):\n",
    "    user_books_read = read[read['user_id'] == user_id]\n",
    "    user_num_books = user_books_read['book_id'].nunique()\n",
    "    user_reviews = reviews[reviews['user_id'] == user_id]\n",
    "    user_num_reviews = user_reviews.shape[0]\n",
    "\n",
    "    all_books = interactions['book_id'].unique()\n",
    "    unread_books = list(set(all_books) - set(user_books_read['book_id']))\n",
    "\n",
    "    # --- Case 1: Content-Based Filtering (HDBSCAN) ---\n",
    "    if user_num_books < 5:\n",
    "        print(f\"User {user_id} has read {user_num_books} books. Using content-based filtering.\")\n",
    "        recommendations = []\n",
    "        for book_id in user_books_read['book_id']:\n",
    "            book_recommendations = recommend_books_HDBSCAN(book_id, books, umap_embeddings, faiss_index, book_id_to_index)\n",
    "            recommendations.extend(book_recommendations)\n",
    "\n",
    "        # Remove duplicates by title and return\n",
    "        unique_recommendations = {rec['title']: rec for rec in recommendations}.values()\n",
    "        return list(unique_recommendations)[:5]\n",
    "\n",
    "    # --- Case 2: Collaborative Filtering (NMF) ---\n",
    "    if user_num_books >= 5 and user_num_reviews < 5:\n",
    "        print(f\"User {user_id} has fewer reviews. Using collaborative filtering (NMF).\")\n",
    "        best_nmf = joblib.load('../Pickle/best_nmf_model.pkl')\n",
    "        book_recommendations = recommend_books_NMF(best_nmf, interactions, user_id, user_books_read['book_id'], books)\n",
    "        return book_recommendations[:5]\n",
    "\n",
    "    # --- Case 3: Hybrid Filtering (NMF + GAT) ---\n",
    "    if user_num_books >= 5 and user_num_reviews >= 5:\n",
    "        print(f\"User {user_id} has more than 5 books and reviews. Using hybrid filtering (NMF + GAT).\")\n",
    "        nmf_recommendations = recommend_books_NMF(best_nmf, interactions, user_id, user_books_read['book_id'], books)\n",
    "        gat_recommendations = recommend_books_GAT(user_id, unread_books, all_embeddings, user_id_to_index_gat, book_id_to_index_gat, books)\n",
    "\n",
    "        # Merge the recommendations giving more weight to GAT\n",
    "        recommendations = merge_recommendations_weighted(gat_recommendations, nmf_recommendations, final_size=5, gat_priority=3)\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books, interactions, read, reviews, umap_embeddings, faiss_index, book_id_to_index, user_id_to_index_gat, book_id_to_index_gat, all_embeddings, reviews = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(ratings_data, test_size=0.1, val_size=0.15, random_state=42):\n",
    "\n",
    "    # Identify users and books that appear only once in the dataset\n",
    "    user_counts = ratings_data['user_id'].value_counts()\n",
    "    book_counts = ratings_data['book_id'].value_counts()\n",
    "\n",
    "    # Find interactions where user or book appears only once\n",
    "    single_interactions = ratings_data[\n",
    "        ratings_data['user_id'].isin(user_counts[user_counts == 1].index) | \n",
    "        ratings_data['book_id'].isin(book_counts[book_counts == 1].index)\n",
    "    ]\n",
    "\n",
    "    # Remove those interactions from the main dataset\n",
    "    remaining_interactions = ratings_data[~ratings_data.index.isin(single_interactions.index)]\n",
    "\n",
    "    # Split the remaining interactions into train, validation, and test\n",
    "    train_df, temp_data = train_test_split(remaining_interactions, test_size=test_size+val_size, random_state=random_state)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=test_size/(test_size+val_size), random_state=random_state)\n",
    "\n",
    "    # Add the single interactions to the training set\n",
    "    train_data = pd.concat([train_df, single_interactions], ignore_index=True)\n",
    "\n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_inter, test_data_inter = split_data(interactions)\n",
    "train_data_rev, test_data_rev = split_data(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using train_data_inter, test_data_inter call case 2 for valid users and eval\n",
    "train_data_rev, test_data_rev call case 3 for valid users and eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommend_for_user(1 , books, interactions, read, umap_embeddings, faiss_index, book_id_to_index, user_id_to_index_gat, book_id_to_index_gat, all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rec in recommendations:\n",
    "    print(f\"Book ID: {rec['book_id']}\")\n",
    "    print(f\"Title: {rec['title']}\")\n",
    "    \n",
    "    # Check if 'authors' is a list or a single string\n",
    "    print(f\"Authors: {rec['authors']}\")\n",
    "    print(f\"Predicted Rating: {rec['predicted_rating']:.4f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, ndcg_score, mean_reciprocal_rank\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to calculate NDCG\n",
    "def ndcg_at_k(recommended, relevant, k=5):\n",
    "    \"\"\"\n",
    "    Calculates NDCG (Normalized Discounted Cumulative Gain) at rank k.\n",
    "    recommended: list of recommended book ids\n",
    "    relevant: list of relevant book ids (from ground truth)\n",
    "    k: rank at which NDCG is calculated\n",
    "    \"\"\"\n",
    "    recommended_at_k = recommended[:k]\n",
    "    relevant_at_k = [1 if book in recommended_at_k else 0 for book in relevant]\n",
    "\n",
    "    dcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(relevant_at_k)])\n",
    "    idcg = sum([1 / np.log2(i + 2) for i in range(min(k, len(relevant)))])  # Ideal DCG\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "# Metrics calculation function\n",
    "def evaluate_recommendations(test_data, recommendations, k=5):\n",
    "    \"\"\"\n",
    "    Evaluates recommendations using Precision, Recall, NDCG, and MRR.\n",
    "    test_data: The ground truth (users, books they interacted with, and ratings)\n",
    "    recommendations: The recommended books for each user\n",
    "    \"\"\"\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    ndcg_scores = []\n",
    "    mrr_scores = []\n",
    "    diversity_scores = []\n",
    "    surprise_scores = []\n",
    "    novelty_scores = []\n",
    "\n",
    "    for user_id, recs in recommendations.items():\n",
    "        # Get the ground truth for the user from test data\n",
    "        ground_truth = test_data[test_data['user_id'] == user_id]\n",
    "        relevant_books = set(ground_truth['book_id'])\n",
    "        \n",
    "        # Recommended books\n",
    "        recommended_books = [rec['book_id'] for rec in recs]\n",
    "\n",
    "        # Precision\n",
    "        precision = len(relevant_books & set(recommended_books)) / len(recommended_books)\n",
    "        precision_scores.append(precision)\n",
    "\n",
    "        # Recall\n",
    "        recall = len(relevant_books & set(recommended_books)) / len(relevant_books)\n",
    "        recall_scores.append(recall)\n",
    "\n",
    "        # NDCG\n",
    "        ndcg = ndcg_at_k(recommended_books, list(relevant_books), k)\n",
    "        ndcg_scores.append(ndcg)\n",
    "\n",
    "        # MRR (Mean Reciprocal Rank)\n",
    "        ranks = [i + 1 for i, rec in enumerate(recommended_books) if rec in relevant_books]\n",
    "        mrr = np.mean([1 / rank for rank in ranks]) if ranks else 0\n",
    "        mrr_scores.append(mrr)\n",
    "\n",
    "        # Diversity (intra-list similarity) - Assuming cosine similarity or distance between embeddings\n",
    "        diversity = calculate_diversity(recommended_books)  # You would need to define a function to calculate this\n",
    "        diversity_scores.append(diversity)\n",
    "\n",
    "        # Surprise (out-of-the-box recommendations)\n",
    "        surprise = calculate_surprise(recommended_books)  # Define surprise based on rarity, etc.\n",
    "        surprise_scores.append(surprise)\n",
    "\n",
    "        # Novelty (recommendation of rare books, inverse of popularity)\n",
    "        novelty = calculate_novelty(recommended_books)  # Define novelty based on item rarity\n",
    "        novelty_scores.append(novelty)\n",
    "\n",
    "    # Average the metrics\n",
    "    return {\n",
    "        \"precision\": np.mean(precision_scores),\n",
    "        \"recall\": np.mean(recall_scores),\n",
    "        \"ndcg\": np.mean(ndcg_scores),\n",
    "        \"mrr\": np.mean(mrr_scores),\n",
    "        \"diversity\": np.mean(diversity_scores),\n",
    "        \"surprise\": np.mean(surprise_scores),\n",
    "        \"novelty\": np.mean(novelty_scores),\n",
    "    }\n",
    "\n",
    "# Helper function to calculate diversity, surprise, and novelty\n",
    "def calculate_diversity(recommended_books):\n",
    "    # Example: Measure diversity using embeddings (e.g., cosine similarity)\n",
    "    pass  # Define how to measure diversity between the recommended books\n",
    "\n",
    "def calculate_surprise(recommended_books):\n",
    "    # Example: Measure surprise based on rare items (e.g., inverse frequency)\n",
    "    pass  # Define how to calculate surprise\n",
    "\n",
    "def calculate_novelty(recommended_books):\n",
    "    # Example: Measure novelty based on book popularity\n",
    "    pass  # Define how to calculate novelty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with early check for user validity before calling recommend_for_user\n",
    "def evaluate_case_2_and_3(train_data_inter, test_data_inter, train_data_rev, test_data_rev, books, interactions, read, umap_embeddings, faiss_index, book_id_to_index, user_id_to_index_gat, book_id_to_index_gat, all_embeddings, reviews):\n",
    "    \n",
    "    # Initialize results containers\n",
    "    case_2_results = []\n",
    "    case_3_results = []\n",
    "\n",
    "    # Case 2: Collaborative Filtering (NMF) evaluation\n",
    "    for user_id in test_data_inter['user_id'].unique():\n",
    "        # Check user eligibility for Case 2 (Collaborative Filtering)\n",
    "        user_books_read = read[read['user_id'] == user_id]\n",
    "        user_num_books = user_books_read['book_id'].nunique()\n",
    "\n",
    "        if user_num_books >= 5:\n",
    "            recommendations = recommend_for_user(\n",
    "                user_id, books, interactions, read, umap_embeddings, faiss_index, \n",
    "                book_id_to_index, user_id_to_index_gat, book_id_to_index_gat, all_embeddings, reviews\n",
    "            )\n",
    "            actual_books = test_data_inter[test_data_inter['user_id'] == user_id]['book_id'].values\n",
    "\n",
    "            precision, recall, ndcg, mrr, diversity, surprise, novelty = evaluate_recommendations(recommendations, actual_books)\n",
    "\n",
    "            # Collect metrics into a dictionary (or tuple)\n",
    "            case_2_results.append({\n",
    "                'user_id': user_id,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'ndcg': ndcg,\n",
    "                'mrr': mrr,\n",
    "                'diversity': diversity,\n",
    "                'surprise': surprise,\n",
    "                'novelty': novelty\n",
    "            })\n",
    "\n",
    "    # Case 3: Hybrid Filtering (NMF + GAT) evaluation\n",
    "    for user_id in test_data_rev['user_id'].unique():\n",
    "        # Check user eligibility for Case 3 (Hybrid Filtering)\n",
    "        user_books_read = read[read['user_id'] == user_id]\n",
    "        user_num_books = user_books_read['book_id'].nunique()\n",
    "        user_reviews = reviews[reviews['user_id'] == user_id]\n",
    "        user_num_reviews = user_reviews.shape[0]\n",
    "\n",
    "        if user_num_books >= 5 and user_num_reviews >= 5:\n",
    "            recommendations = recommend_for_user(\n",
    "                user_id, books, interactions, read, umap_embeddings, faiss_index, \n",
    "                book_id_to_index, user_id_to_index_gat, book_id_to_index_gat, all_embeddings, reviews\n",
    "            )\n",
    "            actual_books = test_data_rev[test_data_rev['user_id'] == user_id]['book_id'].values\n",
    "\n",
    "            precision, recall, ndcg, mrr, diversity, surprise, novelty = evaluate_recommendations(recommendations, actual_books)\n",
    "\n",
    "            # Collect metrics into a dictionary (or tuple)\n",
    "            case_3_results.append({\n",
    "                'user_id': user_id,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'ndcg': ndcg,\n",
    "                'mrr': mrr,\n",
    "                'diversity': diversity,\n",
    "                'surprise': surprise,\n",
    "                'novelty': novelty\n",
    "            })\n",
    "\n",
    "    # Return both cases results\n",
    "    return {\n",
    "        'case_2_results': case_2_results,\n",
    "        'case_3_results': case_3_results\n",
    "    }\n",
    "\n",
    "# Call the evaluation function and store results\n",
    "results = evaluate_case_2_and_3(\n",
    "    train_data_inter, test_data_inter, train_data_rev, test_data_rev, books, \n",
    "    interactions, read, umap_embeddings, faiss_index, book_id_to_index, \n",
    "    user_id_to_index_gat, book_id_to_index_gat, all_embeddings, reviews\n",
    ")\n",
    "\n",
    "# If you want to check results after the call\n",
    "print(\"Case 2 Results:\", results['case_2_results'][:5])  # print first 5 as example\n",
    "print(\"Case 3 Results:\", results['case_3_results'][:5])  # print fir\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
