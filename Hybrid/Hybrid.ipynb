{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_list = []\n",
    "with open('../Pickle/books.pkl', 'rb') as file:\n",
    "    while True:\n",
    "        try:\n",
    "            books_list.append(pickle.load(file))\n",
    "        except EOFError:\n",
    "            break\n",
    "books = pd.concat(books_list, ignore_index=True).drop_duplicates(subset='title', keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_pickle('../Pickle/interactions.pkl')\n",
    "read = pd.read_pickle('../Pickle/read.pkl')\n",
    "reviews = pd.read_pickle('../Pickle/reviews.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction = interactions[interactions['rating'] != 0]\n",
    "read = read[read['rating'] != 0]\n",
    "reviews = reviews[reviews['rating'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings = pd.read_pickle('../Pickle/umap_embeddings.pkl')\n",
    "faiss_index = faiss.read_index('../Pickle/faiss_index.bin')\n",
    "book_id_to_index = pd.read_pickle('../Pickle/book_id_to_index.pkl')\n",
    "user_id_to_index_gat = pd.read_pickle('../Pickle/user_id_to_index_gat.pkl')\n",
    "book_id_to_index_gat = pd.read_pickle('../Pickle/book_id_to_index_gat.pkl')\n",
    "all_embeddings = pd.read_pickle('../Pickle/gat_embeddings.pkl')\n",
    "clustered_books = pd.read_pickle('../Pickle/clustered_books.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_rating(log_scaled_ratings, min_rating):\n",
    "    \"\"\"\n",
    "    Denormalizes log-scaled ratings to their original rating scale and applies an optional minimum rating offset.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    log_scaled_ratings (list or array-like): The log-transformed ratings that need to be denormalized.\n",
    "    min_rating (float): The minimum rating value to be added back to the denormalized ratings.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray: The denormalized ratings, clipped to a range of [0, 5].\n",
    "    \"\"\"\n",
    "    log_scaled_ratings = np.asarray(log_scaled_ratings, dtype=float)\n",
    "    original_ratings = np.expm1(log_scaled_ratings)\n",
    "    if min_rating:\n",
    "        original_ratings += min_rating\n",
    "    return np.clip(original_ratings, 0, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gat_model():\n",
    "    from Gatv2Conv import GATModel\n",
    "    model = GATModel(\n",
    "        in_channels=32,\n",
    "        hidden_channels=30,\n",
    "        out_channels=1,\n",
    "        num_heads=25,\n",
    "        edge_feature_dim=386\n",
    "    )\n",
    "    model.load_state_dict(torch.load('../Pickle/gat_model.pth'))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nmf_model_with_joblib(joblib_file_path):\n",
    "    return joblib.load(joblib_file_path)\n",
    "joblib_file_path = '../Pickle/best_nmf_model.pkl' \n",
    "nmf_model = load_nmf_model_with_joblib(joblib_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_HDBSCAN_new_user(clustered_books, faiss_index, books, top_n=5, cluster_threshold=0.01, random_cluster_count=3):\n",
    "    \"\"\"\n",
    "    Recommends books for a new user based on HDBSCAN clustering and FAISS index search. \n",
    "    If no relevant clusters are found, it falls back to using the FAISS global search or random recommendations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    clustered_books (DataFrame): A DataFrame containing books that have been clustered with HDBSCAN, \n",
    "                                  including book embeddings and top clusters.\n",
    "    faiss_index (faiss.Index): A FAISS index for fast nearest neighbor search, used if no relevant books are found \n",
    "                               in the HDBSCAN clusters.\n",
    "    books (DataFrame): A DataFrame containing metadata of books, including 'book_id', 'title', and 'authors'.\n",
    "    top_n (int): The number of recommendations to return (default is 5).\n",
    "    cluster_threshold (float): The minimum probability threshold for considering a book relevant to a cluster (default is 0.01).\n",
    "    random_cluster_count (int): The number of random clusters to sample for finding relevant books (default is 3).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list: A list of recommended books with their IDs, titles, authors, and similarity scores. If no relevant books are found,\n",
    "          fallback recommendations are provided based on FAISS or random sampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_clusters = {cluster_id for row in clustered_books.itertuples() for cluster_id, prob in row.top_clusters}\n",
    "    \n",
    "    sampled_clusters = random.sample(sorted(all_clusters), min(random_cluster_count, len(all_clusters)))\n",
    "\n",
    "    relevant_books = []\n",
    "    embeddings = []\n",
    "    book_ids = []\n",
    "\n",
    "    for row in clustered_books.itertuples():\n",
    "        shared_clusters = {cid for cid, prob in row.top_clusters if cid in sampled_clusters and prob >= cluster_threshold}\n",
    "        if shared_clusters:\n",
    "            relevant_books.append(row.book_id)\n",
    "            embeddings.append(np.array(row.embedding).astype('float32'))\n",
    "            book_ids.append(row.book_id)\n",
    "\n",
    "    if relevant_books:\n",
    "        embeddings = np.array(embeddings)\n",
    "        distances = np.linalg.norm(embeddings, axis=1)\n",
    "        sorted_indices = np.argsort(distances)\n",
    "\n",
    "        recommendations = []\n",
    "        for idx in sorted_indices[:top_n]:\n",
    "            book_id = book_ids[idx]\n",
    "            book_info = books.loc[books['book_id'] == book_id].iloc[0]\n",
    "            title, authors = book_info['title'], book_info['authors']\n",
    "            similarity_score = 1 / (1 + distances[idx])\n",
    "            recommendations.append({\n",
    "                \"Book ID\": book_id,\n",
    "                \"Title\": title,\n",
    "                \"Authors\": authors,\n",
    "                \"Similarity Score\": round(similarity_score, 4)\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "    if faiss_index is not None:\n",
    "        print(\"No relevant books found in random clusters. Falling back to FAISS global search.\")\n",
    "        random_book = random.choice(clustered_books['book_id'].values)\n",
    "        book_row = clustered_books[clustered_books['book_id'] == random_book].iloc[0]\n",
    "        query_embedding = np.array(book_row['embedding']).astype('float32').reshape(1, -1)\n",
    "        D, I = faiss_index.search(query_embedding, top_n + 1)\n",
    "\n",
    "        recommendations = []\n",
    "        for idx in I[0]:\n",
    "            similar_book_id = clustered_books.iloc[idx]['book_id']\n",
    "            if similar_book_id == random_book:\n",
    "                continue\n",
    "            distance = D[0][np.where(I[0] == idx)[0][0]]\n",
    "            similarity_score = 1 / (1 + distance)\n",
    "\n",
    "            book_info = books.loc[books['book_id'] == similar_book_id].iloc[0]\n",
    "            title, authors = book_info['title'], book_info['authors']\n",
    "            \n",
    "            recommendations.append({\n",
    "                \"Book ID\": similar_book_id,\n",
    "                \"Title\": title,\n",
    "                \"Authors\": authors,\n",
    "                \"Similarity Score\": round(similarity_score, 4)\n",
    "            })\n",
    "\n",
    "            if len(recommendations) == top_n:\n",
    "                break\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "    print(\"No relevant books found from either method, using fallback recommendations.\")\n",
    "    \n",
    "    random_books = random.sample(clustered_books['book_id'].values.tolist(), top_n)\n",
    "    recommendations = []\n",
    "    for book_id in random_books:\n",
    "        book_info = books.loc[books['book_id'] == book_id].iloc[0]\n",
    "        title = book_info['title']\n",
    "        authors = book_info['authors']\n",
    "        recommendations.append({\n",
    "            \"Book ID\": book_id,\n",
    "            \"Title\": title,\n",
    "            \"Authors\": authors,\n",
    "            \"Similarity Score\": 'N/A'\n",
    "        })\n",
    "    \n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_HDBSCAN(book_id, clustered_books, faiss_index, books, top_n=5, cluster_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Recommends books for a given book based on shared clusters found using HDBSCAN clustering. \n",
    "    If no relevant books are found based on clustering, it falls back to using FAISS global search.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    book_id (int): The ID of the book for which recommendations are to be made.\n",
    "    clustered_books (DataFrame): A DataFrame containing books that have been clustered with HDBSCAN, \n",
    "                                  including book embeddings and top clusters.\n",
    "    faiss_index (faiss.Index): A FAISS index for fast nearest neighbor search, used if no relevant books are found \n",
    "                               in the HDBSCAN clusters.\n",
    "    books (DataFrame): A DataFrame containing metadata of books, including 'book_id', 'title', and 'authors'.\n",
    "    top_n (int): The number of recommendations to return (default is 5).\n",
    "    cluster_threshold (float): The minimum probability threshold for considering a book relevant to a cluster (default is 0.01).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list: A list of recommended books with their IDs, titles, authors, and similarity scores. If no relevant books are found,\n",
    "          fallback recommendations are provided based on FAISS search.\n",
    "    \"\"\"\n",
    "    \n",
    "    if book_id not in clustered_books['book_id'].values:\n",
    "        print(f\"Book ID {book_id} not found.\")\n",
    "        return []\n",
    "    \n",
    "    book_row = clustered_books[clustered_books['book_id'] == book_id].iloc[0]\n",
    "    query_embedding = np.array(book_row['embedding']).astype('float32')\n",
    "    top_clusters = dict(book_row['top_clusters'])\n",
    "\n",
    "    clustered_books['shared_clusters'] = clustered_books['top_clusters'].apply(lambda clusters: \n",
    "        [cid for cid, prob in clusters if cid in top_clusters and prob >= cluster_threshold])\n",
    "\n",
    "    relevant_books = clustered_books[clustered_books['shared_clusters'].apply(len) > 0]\n",
    "\n",
    "    if not relevant_books.empty:\n",
    "        embeddings = np.array(relevant_books['embedding'].tolist())\n",
    "        distances = np.linalg.norm(embeddings - query_embedding, axis=1)\n",
    "\n",
    "        sorted_indices = np.argsort(distances)\n",
    "\n",
    "        recommendations = []\n",
    "        for idx in sorted_indices[:top_n]:\n",
    "            book_id = relevant_books.iloc[idx]['book_id']\n",
    "            book_info = books[books['book_id'] == book_id].iloc[0]\n",
    "            title = book_info['title']\n",
    "            authors = book_info['authors']\n",
    "            recommendations.append({\n",
    "                'book_id': book_id,\n",
    "                'title': title,\n",
    "                'authors': authors,\n",
    "                'similarity_score': round(1 / (1 + distances[idx]), 3)\n",
    "            })\n",
    "\n",
    "        return recommendations\n",
    "    \n",
    "    if faiss_index is not None:\n",
    "        print(\"No relevant books found in top clusters. Falling back to FAISS global search.\")\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "        D, I = faiss_index.search(query_embedding, top_n + 1)\n",
    "        \n",
    "        recommendations = []\n",
    "        for idx in I[0]:\n",
    "            similar_book_id = clustered_books.iloc[idx]['book_id']\n",
    "            if similar_book_id == book_id:\n",
    "                continue\n",
    "            distance = D[0][np.where(I[0] == idx)[0][0]]\n",
    "            similarity_score = round(1 / (1 + distance), 3)\n",
    "\n",
    "            book_info = books[books['book_id'] == similar_book_id].iloc[0]\n",
    "            title = book_info['title']\n",
    "            authors = book_info['authors']\n",
    "            \n",
    "            recommendations.append({\n",
    "                'book_id': similar_book_id,\n",
    "                'title': title,\n",
    "                'authors': authors,\n",
    "                'similarity_score': similarity_score\n",
    "            })\n",
    "\n",
    "            if len(recommendations) == top_n:\n",
    "                break\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "    print(\"No recommendations found.\")\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_nmf(\n",
    "    nmf_model, \n",
    "    interactions, \n",
    "    user_id, \n",
    "    books_read, \n",
    "    books, \n",
    "    min_rating, \n",
    "    n_recommendations=5, \n",
    "    top_n_factors=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Recommends books for a given user using the NMF model, along with explanations of the top latent factors contributing \n",
    "    to the predictions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    nmf_model (surprise.NMF): The trained NMF model used for predicting ratings.\n",
    "    interactions (DataFrame): A DataFrame containing interactions (e.g., user-book ratings).\n",
    "    user_id (int): The ID of the user for whom recommendations are to be made.\n",
    "    books_read (list): A list of book IDs that the user has already read.\n",
    "    books (DataFrame): A DataFrame containing metadata of books, including 'book_id' and 'title'.\n",
    "    min_rating (float): The minimum rating to adjust the denormalized predictions.\n",
    "    n_recommendations (int): The number of recommendations to return (default is 5).\n",
    "    top_n_factors (int): The number of top latent factors to explain in the recommendations (default is 1).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list: A list of recommended books with their predicted ratings and explanations of the top latent factors.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_books = interactions['book_id'].unique()\n",
    "    candidate_books = [book_id for book_id in all_books if book_id not in books_read]\n",
    "\n",
    "    user_predictions = [\n",
    "        (book_id, nmf_model.predict(uid=user_id, iid=book_id).est)\n",
    "        for book_id in candidate_books\n",
    "    ]\n",
    "    \n",
    "    top_books = sorted(user_predictions, key=lambda x: x[1], reverse=True)[:n_recommendations]\n",
    "    \n",
    "    user_inner_id = nmf_model.trainset.to_inner_uid(user_id)\n",
    "    pu = nmf_model.pu\n",
    "    qi = nmf_model.qi\n",
    "    user_factors = pu[user_inner_id]\n",
    "\n",
    "    recommendations_with_explanations = []\n",
    "\n",
    "    for book_id, raw_pred_rating in top_books:\n",
    "        try:\n",
    "            item_inner_id = nmf_model.trainset.to_inner_iid(book_id)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        item_factors = qi[item_inner_id]\n",
    "\n",
    "        contributions = user_factors * item_factors\n",
    "        predicted_rating = contributions.sum()\n",
    "\n",
    "        top_factors_idx = np.argsort(np.abs(contributions))[::-1][:top_n_factors]\n",
    "\n",
    "        book_title = books.loc[books['book_id'] == book_id, 'title'].values[0]\n",
    "        \n",
    "        denormed_rating = denormalize_rating([predicted_rating], min_rating)[0]\n",
    "\n",
    "        explanations = []\n",
    "        \n",
    "        for rank, i in enumerate(top_factors_idx, 1):\n",
    "            explanation = {\n",
    "                'latent_factor': int(i + 1),\n",
    "                'user_affinity': round(user_factors[i], 3),\n",
    "                'item_relevance': round(item_factors[i], 3),\n",
    "                'contribution': round(contributions[i], 3)\n",
    "            }\n",
    "            explanations.append(explanation)\n",
    "\n",
    "        recommendations_with_explanations.append({\n",
    "            'book_id': book_id,\n",
    "            'title': book_title,\n",
    "            'predicted_rating': round(denormed_rating, 2),\n",
    "            'top_latent_factors': explanations\n",
    "        })\n",
    "\n",
    "    return recommendations_with_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_GAT(user_id, unread_book_ids, all_embeddings, user_id_to_index, book_id_to_index, books_df, min_rating, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommends books for a given user using the Graph Attention Network (GAT) model based on the user's and books' embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_id (int): The ID of the user for whom recommendations are to be made.\n",
    "    unread_book_ids (list): A list of book IDs that the user has not read.\n",
    "    all_embeddings (np.ndarray): A matrix containing embeddings for both users and books.\n",
    "    user_id_to_index (dict): A dictionary mapping user IDs to their corresponding index in the embeddings matrix.\n",
    "    book_id_to_index (dict): A dictionary mapping book IDs to their corresponding index in the embeddings matrix.\n",
    "    books_df (DataFrame): A DataFrame containing metadata of books, including 'book_id' and 'title'.\n",
    "    min_rating (float): The minimum rating to adjust the denormalized predictions.\n",
    "    top_n (int): The number of book recommendations to return (default is 5).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list: A list of recommended books with their predicted ratings.\n",
    "    \"\"\"\n",
    "    user_index = user_id_to_index.get(user_id)\n",
    "    if user_index is None:\n",
    "        raise ValueError(f\"User ID {user_id} not found in index mappings.\")\n",
    "\n",
    "    user_embedding = all_embeddings[user_index]\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for book_id in unread_book_ids:\n",
    "        book_index = book_id_to_index.get(book_id)\n",
    "        if book_index is None:\n",
    "            continue\n",
    "\n",
    "        book_embedding = all_embeddings[book_index]\n",
    "\n",
    "        predicted_rating = np.expm1(np.dot(user_embedding, book_embedding))\n",
    "\n",
    "        denormed_rating = denormalize_rating([predicted_rating], min_rating)[0] if min_rating else predicted_rating\n",
    "\n",
    "        book_title = books_df.loc[books_df['book_id'] == book_id, 'title'].values[0] if 'book_id' in books_df.columns else None\n",
    "\n",
    "        predictions.append({\n",
    "            \"book_id\": book_id,\n",
    "            \"title\": book_title,\n",
    "            \"predicted_rating\": denormed_rating\n",
    "        })\n",
    "\n",
    "    sorted_books = sorted(predictions, key=lambda x: x[\"predicted_rating\"], reverse=True)\n",
    "\n",
    "    return sorted_books[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_recommendations_ranked(gat_recs, nmf_recs, books_df, final_size, gat_weight=0.4, nmf_weight=0.6):\n",
    "    \"\"\"\n",
    "    Merges book recommendations from two different models (GAT and NMF) and ranks them based on weighted scores.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    gat_recs (list): List of recommendations from the GAT model, each containing 'book_id', 'predicted_rating'.\n",
    "    nmf_recs (list): List of recommendations from the NMF model, each containing 'book_id', 'predicted_rating'.\n",
    "    books_df (DataFrame): DataFrame containing metadata of books, including 'book_id' and 'title'.\n",
    "    final_size (int): The final number of recommendations to return after merging.\n",
    "    gat_weight (float): Weight assigned to the GAT model in the merged score (default is 0.4).\n",
    "    nmf_weight (float): Weight assigned to the NMF model in the merged score (default is 0.6).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list: A list of the final top book recommendations with their titles and predicted ratings.\n",
    "    \"\"\"\n",
    "    gat_recs_sorted = sorted(gat_recs, key=lambda x: x['predicted_rating'], reverse=True)\n",
    "    nmf_recs_sorted = sorted(nmf_recs, key=lambda x: x['predicted_rating'], reverse=True)\n",
    "\n",
    "    combined_recs = []\n",
    "    seen_books = set()\n",
    "\n",
    "    for rank, rec in enumerate(gat_recs_sorted, start=1):\n",
    "        if rec['book_id'] not in seen_books:\n",
    "            combined_recs.append({\n",
    "                'book_id': rec['book_id'],\n",
    "                'rank': rank,\n",
    "                'score': gat_weight * rank,\n",
    "                'predicted_rating': rec['predicted_rating']\n",
    "            })\n",
    "            seen_books.add(rec['book_id'])\n",
    "\n",
    "    for rank, rec in enumerate(nmf_recs_sorted, start=1):\n",
    "        if rec['book_id'] not in seen_books:\n",
    "            combined_recs.append({\n",
    "                'book_id': rec['book_id'],\n",
    "                'rank': rank,\n",
    "                'score': nmf_weight * rank,\n",
    "                'predicted_rating': rec['predicted_rating']\n",
    "            })\n",
    "            seen_books.add(rec['book_id'])\n",
    "\n",
    "    combined_recs.sort(key=lambda x: x['score'])\n",
    "\n",
    "    final_recs = []\n",
    "    for rec in combined_recs:\n",
    "        if len(final_recs) < final_size:\n",
    "            book_id = rec['book_id']\n",
    "            book_title = books_df.loc[books_df['book_id'] == book_id, 'title'].values[0]\n",
    "            final_recs.append({\n",
    "                'book_id': book_id,\n",
    "                'title': book_title,\n",
    "                'predicted_rating': rec['predicted_rating'],\n",
    "            })\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return final_recs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_books = set(books['book_id'])\n",
    "reviews = reviews[reviews['book_id'].isin(valid_books)].copy()\n",
    "interactions = interactions[interactions['book_id'].isin(valid_books)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_stats(user_id, interactions, reviews):\n",
    "    \"\"\"\n",
    "    Retrieves statistics about the user's interaction with the system, such as the number of books read and reviews written.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_id (int): The ID of the user.\n",
    "    interactions (DataFrame): DataFrame containing user-item interactions (book ratings).\n",
    "    reviews (DataFrame): DataFrame containing user reviews.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: A tuple containing the number of books read and the number of reviews written by the user.\n",
    "    \"\"\"\n",
    "    books_read = interactions[interactions['user_id'] == user_id]['book_id'].nunique()\n",
    "    reviews_written = reviews[reviews['user_id'] == user_id].shape[0]\n",
    "    return books_read, reviews_written\n",
    "\n",
    "\n",
    "def recommend_for_user(user_id, interactions, reviews, faiss_index, book_id_to_index, clustered_books, nmf_model, all_embeddings, user_id_to_index_gat, book_id_to_index_gat, books, min_rating, top_n=5, final_size=5, gat_weight=0.6, nmf_weight=0.5):\n",
    "    \"\"\"\n",
    "    Generates book recommendations for a user based on their activity, including ratings, reviews, and interactions with books.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_id (int): The ID of the user for whom to generate recommendations.\n",
    "    interactions (DataFrame): DataFrame of user-item interactions (book ratings).\n",
    "    reviews (DataFrame): DataFrame of user reviews.\n",
    "    faiss_index (Index): FAISS index for similarity-based search.\n",
    "    book_id_to_index (dict): Mapping of book IDs to indices for clustering.\n",
    "    clustered_books (DataFrame): DataFrame of clustered books with embeddings and cluster information.\n",
    "    nmf_model (NMF): Pre-trained NMF model for recommendation.\n",
    "    all_embeddings (ndarray): Array of book embeddings for the GAT model.\n",
    "    user_id_to_index_gat (dict): Mapping of user IDs to indices for the GAT model.\n",
    "    book_id_to_index_gat (dict): Mapping of book IDs to indices for the GAT model.\n",
    "    books (DataFrame): DataFrame containing book metadata (book_id, title, etc.).\n",
    "    min_rating (float): Minimum rating threshold for predictions.\n",
    "    top_n (int): Number of recommendations to generate (default is 5).\n",
    "    final_size (int): Number of final recommendations to return (default is 5).\n",
    "    gat_weight (float): Weight assigned to the GAT model in the final recommendation (default is 0.6).\n",
    "    nmf_weight (float): Weight assigned to the NMF model in the final recommendation (default is 0.5).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list: A list of top book recommendations for the user, each with book ID, title, and predicted rating.\n",
    "    \"\"\"\n",
    "    books_read, reviews_written = get_user_stats(user_id, interactions, reviews)\n",
    "    print(f\"User {user_id} stats: {books_read} books rated, {reviews_written} reviews written.\")\n",
    "    \n",
    "    if books_read == 0:\n",
    "        recommendations = recommend_HDBSCAN_new_user(clustered_books, faiss_index, books, top_n)\n",
    "        return recommendations\n",
    "\n",
    "    elif books_read <= 5:\n",
    "        books_read_list = interactions[interactions['user_id'] == user_id]['book_id'].unique()\n",
    "        all_recommendations = []\n",
    "\n",
    "        for book_id in books_read_list:\n",
    "            recommendations = recommend_HDBSCAN(book_id, clustered_books, faiss_index, books, top_n=5) \n",
    "            if recommendations:\n",
    "                all_recommendations.extend(recommendations)\n",
    "\n",
    "        seen_books = set()\n",
    "        unique_recommendations = []\n",
    "        for rec in all_recommendations:\n",
    "            if rec['book_id'] not in seen_books:\n",
    "                seen_books.add(rec['book_id'])\n",
    "                unique_recommendations.append(rec)\n",
    "\n",
    "        unique_recommendations.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        unique_recommendations = unique_recommendations[:5]\n",
    "        return unique_recommendations\n",
    "\n",
    "    elif books_read > 5 and reviews_written <= 5:\n",
    "        unread_books = interactions[interactions['user_id'] == user_id]['book_id'].unique()\n",
    "        recommendations = recommend_nmf(nmf_model, interactions, user_id, unread_books, books, min_rating, top_n)\n",
    "        return recommendations\n",
    "\n",
    "    elif books_read > 5 and reviews_written > 5:\n",
    "        unread_books = interactions[interactions['user_id'] == user_id]['book_id'].unique()\n",
    "        nmf_recs = recommend_nmf(nmf_model, interactions, user_id, unread_books, books, min_rating, top_n)\n",
    "        gat_recs = recommend_GAT(user_id, unread_books, all_embeddings, user_id_to_index_gat, book_id_to_index_gat, books, min_rating, top_n)\n",
    "        recommendations = merge_recommendations_ranked(gat_recs, nmf_recs, books, final_size, gat_weight, nmf_weight)\n",
    "        return recommendations\n",
    "\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_counts = interactions['user_id'].value_counts()\n",
    "valid_user_ids = user_counts[(user_counts == 100)].index\n",
    "filtered_interactions = interactions[interactions['user_id'].isin(valid_user_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recs_hybrid(interactions, reviews, faiss_index, book_id_to_index, \n",
    "                                          clustered_books, nmf_model, all_embeddings, user_id_to_index_gat, \n",
    "                                          book_id_to_index_gat, books, min_rating=1, top_n=5, \n",
    "                                          final_size=5, gat_weight=0.6, nmf_weight=0.4):\n",
    "    \"\"\"\n",
    "    Generates recommendations for eligible users without merging true ratings.\n",
    "    \n",
    "    Args:\n",
    "    interactions (pd.DataFrame): User-item interaction data.\n",
    "    reviews (pd.DataFrame): User reviews data.\n",
    "    faiss_index (Index): FAISS index for fast nearest neighbor search.\n",
    "    book_id_to_index (dict): Mapping of book IDs to indices.\n",
    "    clustered_books (dict): Predefined clusters of books.\n",
    "    nmf_model (model): Pre-trained NMF model for recommendation.\n",
    "    all_embeddings (dict): Book embeddings for recommendation.\n",
    "    user_id_to_index_gat (dict): Mapping of user IDs to indices for GAT model.\n",
    "    book_id_to_index_gat (dict): Mapping of book IDs to indices for GAT model.\n",
    "    books (pd.DataFrame): DataFrame containing book information.\n",
    "    min_rating (int): Minimum rating threshold for valid recommendations.\n",
    "    top_n (int): Number of top recommendations to generate.\n",
    "    final_size (int): Final number of recommendations to return.\n",
    "    gat_weight (float): Weight for GAT-based recommendations.\n",
    "    nmf_weight (float): Weight for NMF-based recommendations.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with recommendations.\n",
    "    \"\"\"\n",
    "    interactions_count = interactions['user_id'].value_counts()\n",
    "    reviews_count = reviews['user_id'].value_counts()\n",
    "\n",
    "    eligible_users = interactions_count.reindex(reviews_count.index, fill_value=0)\n",
    "    eligible_users = eligible_users[(eligible_users > 5) & (reviews_count > 5)].index\n",
    "    eligible_users_gat = [user_id for user_id in eligible_users if user_id in user_id_to_index_gat]\n",
    "    print(f\"Eligible users : {len(eligible_users_gat)}\")\n",
    "    recommendations = []\n",
    "    for user_id in eligible_users_gat:\n",
    "        recs = recommend_for_user(\n",
    "            user_id, interactions, reviews, faiss_index, book_id_to_index, clustered_books, \n",
    "            nmf_model, all_embeddings, user_id_to_index_gat, book_id_to_index_gat, books, \n",
    "            min_rating=min_rating, top_n=top_n, final_size=final_size, gat_weight=gat_weight, nmf_weight=nmf_weight\n",
    "        )\n",
    "        \n",
    "        for rec in recs:\n",
    "            recommendations.append({'user_id': user_id, **rec})  \n",
    "        \n",
    "        print(f\"Recommendations for user {user_id}: {recs}\")\n",
    "    \n",
    "    print(f\"Recommendations list size: {len(recommendations)}\")\n",
    "    df_recommendations = pd.DataFrame(recommendations)\n",
    "    return df_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recs_nmf(interactions, reviews, faiss_index, book_id_to_index, \n",
    "                      clustered_books, nmf_model, all_embeddings, user_id_to_index_gat, \n",
    "                      book_id_to_index_gat, books, min_rating=1, top_n=5, \n",
    "                      final_size=5, gat_weight=0.6, nmf_weight=0.4):\n",
    "    \"\"\"\n",
    "    Generates recommendations for eligible users without merging true ratings.\n",
    "    \n",
    "    Args:\n",
    "    interactions (pd.DataFrame): User-item interaction data.\n",
    "    reviews (pd.DataFrame): User reviews data.\n",
    "    faiss_index (Index): FAISS index for fast nearest neighbor search.\n",
    "    book_id_to_index (dict): Mapping of book IDs to indices.\n",
    "    clustered_books (dict): Predefined clusters of books.\n",
    "    nmf_model (model): Pre-trained NMF model for recommendation.\n",
    "    all_embeddings (dict): Book embeddings for recommendation.\n",
    "    user_id_to_index_gat (dict): Mapping of user IDs to indices for GAT model.\n",
    "    book_id_to_index_gat (dict): Mapping of book IDs to indices for GAT model.\n",
    "    books (pd.DataFrame): DataFrame containing book information.\n",
    "    min_rating (int): Minimum rating threshold for valid recommendations.\n",
    "    top_n (int): Number of top recommendations to generate.\n",
    "    final_size (int): Final number of recommendations to return.\n",
    "    gat_weight (float): Weight for GAT-based recommendations.\n",
    "    nmf_weight (float): Weight for NMF-based recommendations.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with recommendations.\n",
    "    \"\"\"\n",
    "    interactions_count = interactions['user_id'].value_counts()\n",
    "    reviews_count = reviews['user_id'].value_counts()\n",
    "\n",
    "    eligible_users = interactions_count.reindex(reviews_count.index, fill_value=0)\n",
    "    eligible_users = eligible_users[(eligible_users > 5) & (reviews_count <= 5)].index\n",
    "    print(f\"Eligible users: {len(eligible_users)}\")\n",
    "\n",
    "    recommendations = []\n",
    "    \n",
    "    for user_id in eligible_users:\n",
    "        books_read = interactions[interactions['user_id'] == user_id]['book_id'].nunique()\n",
    "        reviews_written = reviews[reviews['user_id'] == user_id].shape[0]\n",
    "        \n",
    "        if books_read > 5 and reviews_written <= 5:\n",
    "            print(f\"User {user_id}: More than 5 books rated but less than or equal to 5 reviews, using NMF-based recommendations.\")\n",
    "            unread_books = interactions[interactions['user_id'] == user_id]['book_id'].unique()\n",
    "            recs = recommend_nmf(nmf_model, interactions, user_id, unread_books, books, min_rating, top_n)\n",
    "            \n",
    "            for rec in recs:\n",
    "                recommendations.append({'user_id': user_id, **rec}) \n",
    "            \n",
    "        else:\n",
    "            recs = recommend_for_user(\n",
    "                user_id, interactions, reviews, faiss_index, book_id_to_index, clustered_books, \n",
    "                nmf_model, all_embeddings, user_id_to_index_gat, book_id_to_index_gat, books, \n",
    "                min_rating=min_rating, top_n=top_n, final_size=final_size, gat_weight=gat_weight, nmf_weight=nmf_weight\n",
    "            )\n",
    "            \n",
    "            for rec in recs:\n",
    "                recommendations.append({'user_id': user_id, **rec})  \n",
    "        print(f\"Recommendations for user {user_id}: {recs}\")\n",
    "        print(f\"Recommendations list size: {len(recommendations)}\")\n",
    "    df_recommendations = pd.DataFrame(recommendations)\n",
    "    return df_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recs_gat(interactions, reviews, faiss_index, book_id_to_index, \n",
    "                      clustered_books, nmf_model, all_embeddings, user_id_to_index_gat, \n",
    "                      book_id_to_index_gat, books, min_rating=1, top_n=5, \n",
    "                      final_size=5, gat_weight=0.6, nmf_weight=0.4):\n",
    "    \"\"\"\n",
    "    Generates recommendations for eligible users using GAT-based recommendations only.\n",
    "    \n",
    "    Args:\n",
    "    interactions (pd.DataFrame): User-item interaction data.\n",
    "    reviews (pd.DataFrame): User reviews data.\n",
    "    faiss_index (Index): FAISS index for fast nearest neighbor search.\n",
    "    book_id_to_index (dict): Mapping of book IDs to indices.\n",
    "    clustered_books (dict): Predefined clusters of books.\n",
    "    nmf_model (model): (Unused here) Pre-trained NMF model for recommendation.\n",
    "    all_embeddings (dict): Book and user embeddings for recommendation.\n",
    "    user_id_to_index_gat (dict): Mapping of user IDs to indices for GAT model.\n",
    "    book_id_to_index_gat (dict): Mapping of book IDs to indices for GAT model.\n",
    "    books (pd.DataFrame): DataFrame containing book information.\n",
    "    min_rating (int): Minimum rating threshold for valid recommendations.\n",
    "    top_n (int): Number of top recommendations to generate.\n",
    "    final_size (int): Final number of recommendations to return (unused here).\n",
    "    gat_weight (float): (Unused here)\n",
    "    nmf_weight (float): (Unused here)\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with GAT-based recommendations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count user interactions and reviews\n",
    "    interactions_count = interactions['user_id'].value_counts()\n",
    "    reviews_count = reviews['user_id'].value_counts()\n",
    "\n",
    "    # Define eligible users (based on interactions + reviews count)\n",
    "    eligible_users = interactions_count.reindex(reviews_count.index, fill_value=0)\n",
    "    eligible_users = eligible_users[(eligible_users > 5) & (reviews_count > 6)].index\n",
    "    print(f\"Eligible users: {len(eligible_users)}\")\n",
    "\n",
    "    recommendations = []\n",
    "    \n",
    "    for user_id in eligible_users:\n",
    "        print(f\"Generating GAT-based recommendations for user {user_id}...\")\n",
    "\n",
    "        # Get books the user has read (to exclude)\n",
    "        unread_books = interactions[interactions['user_id'] == user_id]['book_id'].unique()\n",
    "\n",
    "        # Call recommend_GAT for each eligible user\n",
    "        recs = recommend_GAT(\n",
    "            user_id,\n",
    "            unread_books,\n",
    "            all_embeddings,\n",
    "            user_id_to_index_gat,\n",
    "            book_id_to_index_gat,\n",
    "            books,\n",
    "            min_rating=min_rating,\n",
    "            top_n=top_n\n",
    "        )\n",
    "\n",
    "        for rec in recs:\n",
    "            recommendations.append({'user_id': user_id, **rec})\n",
    "\n",
    "        print(f\"Recommendations for user {user_id}: {recs}\")\n",
    "        print(f\"Total recommendations so far: {len(recommendations)}\")\n",
    "    \n",
    "    # Convert the recommendations to a DataFrame\n",
    "    df_recommendations = pd.DataFrame(recommendations)\n",
    "    return df_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(df_recs_pred, k, item_popularity, num_users, books, threshold=4, top_n=5):\n",
    "    \"\"\"\n",
    "    Evaluate the recommendations using multiple metrics: NDCG, MAP, Novelty, Hit Rate, User Coverage, \n",
    "    Item Coverage, and Diversity Ratio.\n",
    "    \n",
    "    Args:\n",
    "    - df_recs_pred (pandas.DataFrame): A DataFrame containing recommended books with columns 'user_id', 'book_id', and 'predicted_rating'.\n",
    "    - k (int): The number of top recommendations to evaluate.\n",
    "    - item_popularity (dict): A dictionary with item popularity (book_id -> number of ratings).\n",
    "    - num_users (int): The number of unique users in the dataset.\n",
    "    - books (pd.DataFrame): A DataFrame containing the catalog of all books (must contain 'book_id').\n",
    "    - threshold (float): The threshold above which a recommendation is considered relevant (default is 4).\n",
    "    - top_n (int): The number of top recommendations to evaluate (default is 5).\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing evaluation metrics, including:\n",
    "        - 'ndcg': Mean Normalized Discounted Cumulative Gain at k\n",
    "        - 'map': Mean Mean Average Precision at k\n",
    "        - 'novelty': Mean Novelty score (1 - average popularity of recommended items)\n",
    "        - 'hit_rate': Mean Hit Rate (whether there is at least one relevant recommendation)\n",
    "        - 'user_coverage': Proportion of users receiving at least one relevant recommendation\n",
    "        - 'item_coverage': Proportion of unique recommended items relative to total unique items\n",
    "        - 'diversity_ratio': Ratio of unique recommended books to max possible unique recommendations\n",
    "    \"\"\"\n",
    "\n",
    "    ndcg_list = []\n",
    "    map_list = []\n",
    "    novelty_list = []\n",
    "    hit_rate_list = []\n",
    "    users_with_recs = 0  \n",
    "    recommended_book_ids_all_users = set() \n",
    "    relevant_users = 0 \n",
    "\n",
    "    for user_id, group in df_recs_pred.groupby('user_id'):\n",
    "        top_recommended_books = group.drop_duplicates(subset=['book_id']).head(top_n)\n",
    "        recommended_book_ids = top_recommended_books['book_id'].tolist()\n",
    "        recommended_book_ids_all_users.update(recommended_book_ids)\n",
    "        \n",
    "        dcg = sum((1 if row['predicted_rating'] >= threshold else 0) / np.log2(idx + 2) for idx, row in top_recommended_books.iterrows())\n",
    "        idcg = sum(1 / np.log2(idx + 2) for idx in range(min(len(top_recommended_books), k)))\n",
    "        ndcg = dcg / idcg if idcg != 0 else 0\n",
    "        ndcg_list.append(ndcg)\n",
    "\n",
    "        sum_precisions = 0\n",
    "        for idx, row in top_recommended_books.iterrows():\n",
    "            if row['predicted_rating'] >= threshold:\n",
    "                sum_precisions += (idx + 1) / (idx + 1)  \n",
    "        map_score = sum_precisions / min(len(top_recommended_books), k) if len(top_recommended_books) > 0 else 0\n",
    "        map_list.append(map_score)\n",
    "\n",
    "        novelty = np.mean([1 - (item_popularity.get(book, 1) / num_users) for book in recommended_book_ids])\n",
    "        novelty_list.append(novelty)\n",
    "\n",
    "        hit_rate = 1 if any(row['predicted_rating'] >= threshold for idx, row in top_recommended_books.iterrows()) else 0\n",
    "        hit_rate_list.append(hit_rate)\n",
    "\n",
    "        if any(row['predicted_rating'] >= threshold for idx, row in top_recommended_books.iterrows()):\n",
    "            users_with_recs += 1\n",
    "            relevant_users += 1  \n",
    "\n",
    "    total_books_in_catalog = books['book_id'].nunique()  \n",
    "    unique_recommended_books = len(recommended_book_ids_all_users)\n",
    "    item_coverage = (unique_recommended_books / total_books_in_catalog) * 100\n",
    "    diversity_ratio = (unique_recommended_books / (num_users * top_n)) * 100\n",
    "\n",
    "    user_coverage = relevant_users / num_users if num_users > 0 else 0\n",
    "\n",
    "    evaluation_metrics = {\n",
    "        'ndcg': np.mean(ndcg_list),\n",
    "        'map': np.mean(map_list),\n",
    "        'novelty': np.mean(novelty_list),\n",
    "        'hit_rate': np.mean(hit_rate_list),\n",
    "        'user_coverage': user_coverage,\n",
    "        'item_coverage': f'{item_coverage}%', \n",
    "        'diversity_ratio': f'{diversity_ratio:}%' \n",
    "    }\n",
    "\n",
    "    return evaluation_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs_hybrid = generate_recs_hybrid(\n",
    "    interactions, reviews, faiss_index, book_id_to_index, clustered_books, \n",
    "    nmf_model, all_embeddings, user_id_to_index_gat, book_id_to_index_gat, books, \n",
    "    min_rating=1, top_n=5, final_size=5, gat_weight=0.4, nmf_weight=0.6\n",
    ")\n",
    "\n",
    "evaluate_recommendations(\n",
    "    recs_hybrid, k=5, \n",
    "    item_popularity=recs_hybrid['book_id'].value_counts().to_dict(), \n",
    "    num_users=len(interactions['user_id'].unique()), books = books, top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs_nmf = generate_recs_nmf(\n",
    "    interactions, reviews, faiss_index, book_id_to_index, clustered_books, \n",
    "    nmf_model, all_embeddings, user_id_to_index_gat, book_id_to_index_gat, books, \n",
    "    min_rating=1, top_n=5, final_size=5, gat_weight=0.4, nmf_weight=0.6\n",
    ")\n",
    "\n",
    "evaluate_recommendations(\n",
    "    recs_nmf, k=5, \n",
    "    item_popularity=recs_nmf['book_id'].value_counts().to_dict(), \n",
    "    num_users=len(interactions['user_id'].unique()), books = books, top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs_gat = generate_recs_gat(\n",
    "    interactions, reviews, faiss_index, book_id_to_index, clustered_books, \n",
    "    nmf_model, all_embeddings, user_id_to_index_gat, book_id_to_index_gat, books, \n",
    "    min_rating=1, top_n=5, final_size=5, gat_weight=0.4, nmf_weight=0.6\n",
    ")\n",
    "\n",
    "evaluate_recommendations(\n",
    "    recs_gat, k=5, \n",
    "    item_popularity=recs_gat['book_id'].value_counts().to_dict(), \n",
    "    num_users=len(interactions['user_id'].unique()), books = books, top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recs_hdbscan(interactions, reviews, faiss_index, book_id_to_index, \n",
    "                                          clustered_books, nmf_model, all_embeddings, user_id_to_index_gat, \n",
    "                                          book_id_to_index_gat, books, min_rating=1, top_n=5, \n",
    "                                          final_size=5, gat_weight=0.6, nmf_weight=0.4):\n",
    "    \"\"\"\n",
    "    Generates HDBSCAN recommendations for users who have read fewer than 5 books.\n",
    "\n",
    "    Args:\n",
    "    interactions (pd.DataFrame): User-item interaction data.\n",
    "    reviews (pd.DataFrame): User reviews data.\n",
    "    faiss_index (Index): FAISS index for fast nearest neighbor search.\n",
    "    book_id_to_index (dict): Mapping of book IDs to indices.\n",
    "    clustered_books (dict): Predefined clusters of books.\n",
    "    nmf_model (model): Pre-trained NMF model for recommendation.\n",
    "    all_embeddings (dict): Book embeddings for recommendation.\n",
    "    user_id_to_index_gat (dict): Mapping of user IDs to indices for GAT model.\n",
    "    book_id_to_index_gat (dict): Mapping of book IDs to indices for GAT model.\n",
    "    books (pd.DataFrame): DataFrame containing book information.\n",
    "    min_rating (int): Minimum rating threshold for valid recommendations.\n",
    "    top_n (int): Number of top recommendations to generate.\n",
    "    final_size (int): Final number of recommendations to return.\n",
    "    gat_weight (float): Weight for GAT-based recommendations.\n",
    "    nmf_weight (float): Weight for NMF-based recommendations.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with recommendations.\n",
    "    \"\"\"\n",
    "    \n",
    "    interactions_count = interactions['user_id'].value_counts()\n",
    "    eligible_users = interactions_count[interactions_count < 5].index\n",
    "    print(f\"Eligible users with less than 5 books read: {len(eligible_users)}\")\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    for user_id in eligible_users:\n",
    "        recs = recommend_for_user(\n",
    "            user_id, interactions, reviews, faiss_index, book_id_to_index, clustered_books, \n",
    "            nmf_model, all_embeddings, user_id_to_index_gat, book_id_to_index_gat, books, \n",
    "            min_rating=min_rating, top_n=top_n, final_size=final_size, gat_weight=gat_weight, nmf_weight=nmf_weight\n",
    "        )\n",
    "\n",
    "        for rec in recs:\n",
    "            recommendations.append({'user_id': user_id, **rec})\n",
    "\n",
    "        print(f\"Recommendations for user {user_id}: {recs}\")\n",
    "    print(f\"Total recommendations generated: {len(recommendations)}\")\n",
    "    df_recommendations = pd.DataFrame(recommendations)\n",
    "\n",
    "    return df_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(recommendations_list, books, interactions, recs_per_user=5):\n",
    "    \"\"\"\n",
    "    Evaluate recommendations for new users using item coverage, novelty, and diversity.\n",
    "\n",
    "    Args:\n",
    "    - recommendations_list (list of lists): List of recommendation lists (one list per new user).\n",
    "    - books (pd.DataFrame): All available books (must contain 'book_id').\n",
    "    - interactions (pd.DataFrame, optional): User-item interactions for popularity analysis.\n",
    "    - recs_per_user (int): Number of recommendations returned per user (used to calculate max possible unique books).\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    flat_recommendations = []\n",
    "    for user_idx, user_recs in enumerate(recommendations_list):\n",
    "        user_id = f\"new_user_{user_idx + 1}\"\n",
    "        for rec in user_recs:\n",
    "            flat_recommendations.append({\n",
    "                \"user_id\": user_id,\n",
    "                \"book_id\": rec['book_id']\n",
    "            })\n",
    "\n",
    "    df_recs_pred = pd.DataFrame(flat_recommendations)\n",
    "\n",
    "    recommended_book_ids = set(df_recs_pred['book_id'])\n",
    "    total_books = books['book_id'].nunique()\n",
    "    unique_recommended_books = len(recommended_book_ids)\n",
    "    item_coverage = (unique_recommended_books / total_books) * 100\n",
    "\n",
    "    num_users = len(recommendations_list)\n",
    "    max_possible_unique_recs = num_users * recs_per_user\n",
    "    print(f\"\\nUsers evaluated: {num_users}\")\n",
    "    print(f\"Max possible unique recommended books: {max_possible_unique_recs}\")\n",
    "    print(f\"Unique recommended books: {unique_recommended_books}\")\n",
    "    print(f\"Item Coverage: {item_coverage}% ({unique_recommended_books}/{total_books} books recommended)\")\n",
    "\n",
    "    diversity_ratio = (unique_recommended_books / max_possible_unique_recs) * 100\n",
    "    print(f\"Diversity Ratio: {diversity_ratio}% of max possible unique recs\")\n",
    "\n",
    "    avg_popularity = None\n",
    "    novelty = None\n",
    "    if interactions is not None:\n",
    "        book_popularity = interactions['book_id'].value_counts()\n",
    "        pop_scores = [book_popularity.get(book_id, 0) for book_id in recommended_book_ids]\n",
    "\n",
    "        if pop_scores:\n",
    "            avg_popularity = sum(pop_scores) / len(pop_scores)\n",
    "            print(f\"Average Popularity (lower = more novel): {avg_popularity} interactions per book\")\n",
    "\n",
    "            total_interactions = interactions['book_id'].count()\n",
    "            novelty = 1 - (sum(pop_scores) / total_interactions) if total_interactions > 0 else 0\n",
    "            print(f\"Novelty (higher = more novel): {novelty}\")\n",
    "        else:\n",
    "            print(\"No popularity data available for recommended books.\")\n",
    "\n",
    "    metrics = {\n",
    "        \"Item Coverage (%)\": (item_coverage, 2),\n",
    "        \"Unique Recommended Books\": unique_recommended_books,\n",
    "        \"Total Books in Catalog\": total_books,\n",
    "        \"Users Evaluated\": num_users,\n",
    "        \"Max Possible Unique Recommendations\": max_possible_unique_recs,\n",
    "        \"Diversity Ratio (%)\": (diversity_ratio, 2)\n",
    "    }\n",
    "\n",
    "    if avg_popularity is not None:\n",
    "        metrics[\"Average Popularity\"] = round(avg_popularity, 2)\n",
    "    if novelty is not None:\n",
    "        metrics[\"Novelty\"] = round(novelty, 2)\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendations = generate_recs_hdbscan(\n",
    "    interactions=interactions,\n",
    "    reviews=reviews,\n",
    "    faiss_index=faiss_index,\n",
    "    book_id_to_index=book_id_to_index,\n",
    "    clustered_books=clustered_books,\n",
    "    nmf_model=nmf_model,\n",
    "    all_embeddings=all_embeddings,\n",
    "    user_id_to_index_gat=user_id_to_index_gat,\n",
    "    book_id_to_index_gat=book_id_to_index_gat,\n",
    "    books=books,\n",
    "    min_rating=1,\n",
    "    top_n=5,\n",
    "    final_size=5,\n",
    "    gat_weight=0.6,\n",
    "    nmf_weight=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_list = []\n",
    "for user_id, group in df_recommendations.groupby('user_id'):\n",
    "    user_recs = group.to_dict(orient='records') \n",
    "    recommendations_list.append(user_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "user_similarity = df_recommendations.groupby('user_id')['similarity_score'].mean().reset_index()\n",
    "user_similarity = user_similarity.sort_values(by='similarity_score', ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(user_similarity['similarity_score'], bins=20, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Average Similarity Scores Across Users')\n",
    "plt.xlabel('Average Similarity Score')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_recommendations(recommendations_list, books, interactions, recs_per_user=5)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recs_hdbscan_new_users(clustered_books, faiss_index, books, interactions, \n",
    "                                     nmf_model, all_embeddings, user_id_to_index_gat, \n",
    "                                     book_id_to_index_gat, reviews, top_n, \n",
    "                                     sample_size, gat_weight, nmf_weight):\n",
    "    \"\"\"\n",
    "    Generates recommendations for a set of completely new users with no reading history,\n",
    "    using `recommend_for_user` for personalized recommendations based on the user's activity.\n",
    "\n",
    "    Args:\n",
    "    - clustered_books (pd.DataFrame): Must contain 'book_id', 'embedding', and 'top_clusters' columns.\n",
    "    - faiss_index (faiss.Index): FAISS index for fallback global search.\n",
    "    - books (pd.DataFrame): DataFrame containing book information.\n",
    "    - interactions (pd.DataFrame): User-item interactions to determine the highest existing user ID.\n",
    "    - nmf_model (NMF model): Trained NMF model for recommendation generation.\n",
    "    - all_embeddings (ndarray): Embeddings of books for the GAT model.\n",
    "    - user_id_to_index_gat (dict): Dictionary mapping user IDs to indices in the GAT model.\n",
    "    - book_id_to_index_gat (dict): Dictionary mapping book IDs to indices in the GAT model.\n",
    "    - reviews (pd.DataFrame): DataFrame containing user review data.\n",
    "    - top_n (int): Number of recommendations per user.\n",
    "    - sample_size (int): Number of new users to generate recommendations for.\n",
    "    - gat_weight (float): Weight for the GAT model's recommendations.\n",
    "    - nmf_weight (float): Weight for the NMF model's recommendations.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with recommendations for new users.\n",
    "    \"\"\"\n",
    "\n",
    "    max_user_id = interactions['user_id'].max() if not interactions.empty else 0\n",
    "    next_user_id = max_user_id + 1\n",
    "\n",
    "    recommendations = []\n",
    "    for i in range(sample_size):\n",
    "        user_id = f\"new_user_{next_user_id + i}\" \n",
    "        \n",
    "        user_recommendations = recommend_for_user(\n",
    "            user_id=user_id,\n",
    "            interactions=interactions,\n",
    "            reviews=reviews,\n",
    "            faiss_index=faiss_index,\n",
    "            book_id_to_index=book_id_to_index,\n",
    "            clustered_books=clustered_books,\n",
    "            nmf_model=nmf_model,\n",
    "            all_embeddings=all_embeddings,\n",
    "            user_id_to_index_gat=user_id_to_index_gat,\n",
    "            book_id_to_index_gat=book_id_to_index_gat,\n",
    "            books=books,\n",
    "            min_rating=1, \n",
    "            top_n=top_n,\n",
    "            final_size=top_n,  \n",
    "            gat_weight=gat_weight,\n",
    "            nmf_weight=nmf_weight\n",
    "        )\n",
    "\n",
    "        for rec in user_recommendations:\n",
    "            rec['user_id'] = user_id\n",
    "        recommendations.append(user_recommendations)\n",
    "        print(f\"Recommendations generated for {user_id}\")\n",
    "\n",
    "    flattened_recommendations = [rec for user_recs in recommendations for rec in user_recs]\n",
    "    df_recommendations = pd.DataFrame(flattened_recommendations)\n",
    "\n",
    "    print(f\"Total new users processed: {sample_size}\")\n",
    "    print(f\"Total recommendations generated: {len(flattened_recommendations)}\")\n",
    "\n",
    "    return df_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_recs = generate_recs_hdbscan_new_users(clustered_books, faiss_index, books, interactions, \n",
    "                                     nmf_model, all_embeddings, user_id_to_index_gat, \n",
    "                                     book_id_to_index_gat, reviews, top_n=5, \n",
    "                                     sample_size=350, gat_weight=0.6, nmf_weight=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(recommendations_list, books, interactions, recs_per_user=5):\n",
    "    \"\"\"\n",
    "    Evaluate recommendations for new users using item coverage, novelty, and diversity.\n",
    "\n",
    "    Args:\n",
    "    - recommendations_list (list of lists): List of recommendation lists (one list per new user).\n",
    "    - books (pd.DataFrame): All available books (must contain 'book_id').\n",
    "    - interactions (pd.DataFrame, optional): User-item interactions for popularity analysis.\n",
    "    - recs_per_user (int): Number of recommendations returned per user (used to calculate max possible unique books).\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    flat_recommendations = []\n",
    "    for user_idx, user_recs in enumerate(recommendations_list):\n",
    "        user_id = f\"new_user_{user_idx + 1}\"\n",
    "        for rec in user_recs:\n",
    "            flat_recommendations.append({\n",
    "                \"user_id\": user_id,\n",
    "                \"book_id\": rec['Book ID']\n",
    "            })\n",
    "\n",
    "    df_recs_pred = pd.DataFrame(flat_recommendations)\n",
    "\n",
    "    recommended_book_ids = set(df_recs_pred['book_id'])\n",
    "    total_books = books['book_id'].nunique()\n",
    "    unique_recommended_books = len(recommended_book_ids)\n",
    "    item_coverage = (unique_recommended_books / total_books) * 100\n",
    "\n",
    "    num_users = len(recommendations_list)\n",
    "    max_possible_unique_recs = num_users * recs_per_user\n",
    "    print(f\"\\nUsers evaluated: {num_users}\")\n",
    "    print(f\"Max possible unique recommended books: {max_possible_unique_recs}\")\n",
    "    print(f\"Unique recommended books: {unique_recommended_books}\")\n",
    "    print(f\"Item Coverage: {item_coverage}% ({unique_recommended_books}/{total_books} books recommended)\")\n",
    "\n",
    "    diversity_ratio = (unique_recommended_books / max_possible_unique_recs) * 100\n",
    "    print(f\"Diversity Ratio: {diversity_ratio}% of max possible unique recs\")\n",
    "\n",
    "    avg_popularity = None\n",
    "    novelty = None\n",
    "    if interactions is not None:\n",
    "        book_popularity = interactions['book_id'].value_counts()\n",
    "        pop_scores = [book_popularity.get(book_id, 0) for book_id in recommended_book_ids]\n",
    "\n",
    "        if pop_scores:\n",
    "            avg_popularity = sum(pop_scores) / len(pop_scores)\n",
    "            print(f\"Average Popularity (lower = more novel): {avg_popularity} interactions per book\")\n",
    "\n",
    "            total_interactions = interactions['book_id'].count()\n",
    "            novelty = 1 - (sum(pop_scores) / total_interactions) if total_interactions > 0 else 0\n",
    "            print(f\"Novelty (higher = more novel): {novelty}\")\n",
    "        else:\n",
    "            print(\"No popularity data available for recommended books.\")\n",
    "\n",
    "    metrics = {\n",
    "        \"Item Coverage (%)\": (item_coverage),\n",
    "        \"Unique Recommended Books\": unique_recommended_books,\n",
    "        \"Total Books in Catalog\": total_books,\n",
    "        \"Users Evaluated\": num_users,\n",
    "        \"Max Possible Unique Recommendations\": max_possible_unique_recs,\n",
    "        \"Diversity Ratio (%)\": (diversity_ratio)\n",
    "    }\n",
    "\n",
    "    if avg_popularity is not None:\n",
    "        metrics[\"Average Popularity\"] = (avg_popularity)\n",
    "    if novelty is not None:\n",
    "        metrics[\"Novelty\"] = (novelty)\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_list_new_users = []\n",
    "for user_id, group in new_recs.groupby('user_id'):\n",
    "    user_recs = group.to_dict(orient='records')\n",
    "    recommendations_list_new_users.append(user_recs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_new_users = evaluate_recommendations(\n",
    "    recommendations_list=recommendations_list_new_users,\n",
    "    books=books,\n",
    "    interactions=interactions\n",
    ")\n",
    "\n",
    "for metric, value in metrics_new_users.items():\n",
    "    print(f\"{metric}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def genre_distribution_plot(predictions, books, top_k=5):\n",
    "    user_predictions = {}\n",
    "\n",
    "    # Loop through all predictions (user-item pairs)\n",
    "    for prediction in predictions:\n",
    "        for item in prediction:\n",
    "            uid = item['user_id']\n",
    "            iid = item['book_id']\n",
    "            if uid not in user_predictions:\n",
    "                user_predictions[uid] = []\n",
    "            user_predictions[uid].append(iid)\n",
    "\n",
    "    top_k_predictions = {}\n",
    "\n",
    "    for uid, items in user_predictions.items():\n",
    "        # Get top-k items for each user (assuming predictions are already sorted by similarity score)\n",
    "        top_k_predictions[uid] = items[:top_k]\n",
    "\n",
    "    # Genres for top-k recommended books\n",
    "    predicted_book_genres = []\n",
    "\n",
    "    # Loop through top-k recommendations\n",
    "    for uid, top_items in top_k_predictions.items():\n",
    "        for iid in top_items:\n",
    "            book = books[books['book_id'] == iid]  # Match book ID to the books dataset\n",
    "            if not book.empty:\n",
    "                genres = book.iloc[0]['filtered_genres'].split(',')  # assuming comma-separated\n",
    "                predicted_book_genres.extend([genre.strip() for genre in genres])\n",
    "\n",
    "    # Count genres in top-k recommendations\n",
    "    predicted_genre_counts = Counter(predicted_book_genres)\n",
    "\n",
    "    # Get genre counts from the training data\n",
    "    genre_list = []\n",
    "    for genres in books['filtered_genres']:\n",
    "        genre_list.extend([genre.strip() for genre in genres.split(',')])\n",
    "\n",
    "    train_genre_counts_series = pd.Series(genre_list).value_counts()\n",
    "    train_genre_counts_df = train_genre_counts_series.reset_index()\n",
    "    train_genre_counts_df.columns = ['Genre', 'Count']\n",
    "\n",
    "    # Align genres for both plots by using the training genre order\n",
    "    genre_order = train_genre_counts_df['Genre'].tolist()\n",
    "\n",
    "    # Reindex predicted genre counts to match training genre order\n",
    "    predicted_genre_counts_ordered = [predicted_genre_counts.get(genre, 0) for genre in genre_order]\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot for training data\n",
    "    ax[0].bar(genre_order, train_genre_counts_df['Count'], color='lightblue')\n",
    "    ax[0].set_xlabel('Genre')\n",
    "    ax[0].set_ylabel('Count')\n",
    "    ax[0].set_title('Genre Distribution in All Books')\n",
    "    ax[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Plot for predicted (top-k) data\n",
    "    ax[1].bar(genre_order, predicted_genre_counts_ordered, color='lightgreen')\n",
    "    ax[1].set_xlabel('Genre')\n",
    "    ax[1].set_ylabel('Count')\n",
    "    ax[1].set_title(f'Genre Distribution of Top-{top_k} Recommended Books')\n",
    "    ax[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Overlap calculation\n",
    "    overlap = sum(\n",
    "        min(train_genre_counts_series.get(genre, 0), predicted_genre_counts.get(genre, 0)) \n",
    "        for genre in train_genre_counts_series.index\n",
    "    )\n",
    "    total_train_genres = train_genre_counts_series.sum()\n",
    "    total_predicted_genres = sum(predicted_genre_counts.values())\n",
    "\n",
    "    # Count total unique genres\n",
    "    unique_train_genres = train_genre_counts_series.index.nunique()\n",
    "    unique_predicted_genres = len(predicted_genre_counts)\n",
    "\n",
    "    print(f\"Overlap of genres: {overlap}\")\n",
    "    print(f\"Total genres in training data (counts): {total_train_genres}\")\n",
    "    print(f\"Total genres in top-{top_k} recommendations (counts): {total_predicted_genres}\")\n",
    "    print(f\"Total unique genres in training data: {unique_train_genres}\")\n",
    "    print(f\"Total unique genres in top-{top_k} recommendations: {unique_predicted_genres}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_distribution_plot(recommendations_list, books, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reorganized_data = {}\n",
    "\n",
    "for rec in new_recs[['user_id', 'Title', 'Authors', 'Similarity Score', 'Book ID']].to_dict(orient='records'):\n",
    "    rec = {k.lower(): v for k, v in rec.items()}\n",
    "    rec['book_id'] = rec.pop('book id') \n",
    "    if isinstance(rec['user_id'], str) and rec['user_id'].startswith('new_user_'):\n",
    "        try:\n",
    "            rec['user_id'] = int(rec['user_id'].replace('new_user_', ''))\n",
    "        except ValueError:\n",
    "            print(f\"Invalid user_id format in record: {rec}\")\n",
    "    if isinstance(rec['book_id'], int):\n",
    "        rec['book_id'] = rec.pop('book_id')\n",
    "    else:\n",
    "        try:\n",
    "            rec['book_id'] = int(rec['book_id'])\n",
    "        except ValueError:\n",
    "            print(f\"Invalid book_id format in record: {rec}\")\n",
    "    if rec['user_id'] not in final_reorganized_data:\n",
    "        final_reorganized_data[rec['user_id']] = []\n",
    "    final_reorganized_data[rec['user_id']].append(rec)\n",
    "\n",
    "final_reorganized_data_list = list(final_reorganized_data.values())\n",
    "\n",
    "genre_distribution_plot(final_reorganized_data_list, books, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = [\n",
    "    \n",
    "    [{'user_id': int(user_id), 'book_id': int(row['book_id'])} for _, row in group.iterrows()]\n",
    "    for user_id, group in recs_nmf.groupby('user_id')\n",
    "]\n",
    "flattened_data = [\n",
    "    [{'user_id': int(user_data[0]['user_id']), 'book_id': int(entry['book_id'])} \n",
    "     for entry in user_data] \n",
    "    for user_data in grouped_data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def genre_distribution_plot(predictions, books, top_k=5):\n",
    "    # Create a dictionary of user predictions from the DataFrame\n",
    "    user_predictions = {}\n",
    "\n",
    "    # Loop through all predictions (user-item pairs)\n",
    "    for _, row in predictions.iterrows():\n",
    "        uid = row['user_id']\n",
    "        iid = row['book_id']\n",
    "        if uid not in user_predictions:\n",
    "            user_predictions[uid] = []\n",
    "        user_predictions[uid].append(iid)\n",
    "\n",
    "    top_k_predictions = {}\n",
    "\n",
    "    for uid, items in user_predictions.items():\n",
    "        # Get top-k items for each user (assuming predictions are already sorted by similarity score)\n",
    "        top_k_predictions[uid] = items[:top_k]\n",
    "\n",
    "    # Genres for top-k recommended books\n",
    "    predicted_book_genres = []\n",
    "\n",
    "    # Loop through top-k recommendations\n",
    "    for uid, top_items in top_k_predictions.items():\n",
    "        for iid in top_items:\n",
    "            book = books[books['book_id'] == iid]  # Match book ID to the books dataset\n",
    "            if not book.empty:\n",
    "                genres = book.iloc[0]['filtered_genres'].split(',')  # assuming comma-separated\n",
    "                predicted_book_genres.extend([genre.strip() for genre in genres])\n",
    "\n",
    "    # Count genres in top-k recommendations\n",
    "    predicted_genre_counts = Counter(predicted_book_genres)\n",
    "\n",
    "    # Get genre counts from the training data\n",
    "    genre_list = []\n",
    "    for genres in books['filtered_genres']:\n",
    "        genre_list.extend([genre.strip() for genre in genres.split(',')])\n",
    "\n",
    "    train_genre_counts_series = pd.Series(genre_list).value_counts()\n",
    "    train_genre_counts_df = train_genre_counts_series.reset_index()\n",
    "    train_genre_counts_df.columns = ['Genre', 'Count']\n",
    "\n",
    "    # Align genres for both plots by using the training genre order\n",
    "    genre_order = train_genre_counts_df['Genre'].tolist()\n",
    "\n",
    "    # Reindex predicted genre counts to match training genre order\n",
    "    predicted_genre_counts_ordered = [predicted_genre_counts.get(genre, 0) for genre in genre_order]\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot for training data\n",
    "    ax[0].bar(genre_order, train_genre_counts_df['Count'], color='lightblue')\n",
    "    ax[0].set_xlabel('Genre')\n",
    "    ax[0].set_ylabel('Count')\n",
    "    ax[0].set_title('Genre Distribution in All Data')\n",
    "    ax[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Plot for predicted (top-k) data\n",
    "    ax[1].bar(genre_order, predicted_genre_counts_ordered, color='lightgreen')\n",
    "    ax[1].set_xlabel('Genre')\n",
    "    ax[1].set_ylabel('Count')\n",
    "    ax[1].set_title(f'Genre Distribution of Top-{top_k} Recommended Books')\n",
    "    ax[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Overlap calculation\n",
    "    overlap = sum(\n",
    "        min(train_genre_counts_series.get(genre, 0), predicted_genre_counts.get(genre, 0)) \n",
    "        for genre in train_genre_counts_series.index\n",
    "    )\n",
    "    total_train_genres = train_genre_counts_series.sum()\n",
    "    total_predicted_genres = sum(predicted_genre_counts.values())\n",
    "\n",
    "    # Count total unique genres\n",
    "    unique_train_genres = train_genre_counts_series.index.nunique()\n",
    "    unique_predicted_genres = len(predicted_genre_counts)\n",
    "\n",
    "    print(f\"Overlap of genres: {overlap}\")\n",
    "    print(f\"Total genres in training data (counts): {total_train_genres}\")\n",
    "    print(f\"Total genres in top-{top_k} recommendations (counts): {total_predicted_genres}\")\n",
    "    print(f\"Total unique genres in training data: {unique_train_genres}\")\n",
    "    print(f\"Total unique genres in top-{top_k} recommendations: {unique_predicted_genres}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_distribution_plot(recs_nmf, books, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_distribution_plot(recs_gat, books, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_distribution_plot(recs_hybrid, books, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
