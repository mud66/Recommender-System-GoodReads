{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load books in chunks\n",
    "    books_list = []\n",
    "    with open('../Pickle/books.pkl', 'rb') as file:\n",
    "        while True:\n",
    "            try:\n",
    "                books_list.append(pickle.load(file))\n",
    "            except EOFError:\n",
    "                break\n",
    "    books = pd.concat(books_list, ignore_index=True).drop_duplicates(subset='title', keep='first')\n",
    "\n",
    "    # Load other datasets\n",
    "    interactions = pd.read_pickle('../Pickle/interactions.pkl')\n",
    "    read = pd.read_pickle('../Pickle/read.pkl')\n",
    "    reviews = pd.read_pickle('../Pickle/reviews.pkl')\n",
    "    \n",
    "    # Load embeddings and indexes\n",
    "    umap_embeddings = pd.read_pickle('../Pickle/umap_embeddings.pkl')\n",
    "    faiss_index = faiss.read_index('../Pickle/faiss_index.bin')\n",
    "    book_id_to_index = pd.read_pickle('../Pickle/book_id_to_index.pkl')\n",
    "    user_id_to_index_gat = pd.read_pickle('../Pickle/user_id_to_index_gat.pkl')\n",
    "    book_id_to_index_gat = pd.read_pickle('../Pickle/book_id_to_index_gat.pkl')\n",
    "    all_embeddings = pd.read_pickle('../Pickle/gat_embeddings.pkl')\n",
    "    \n",
    "    # Filter datasets to valid book ids\n",
    "    valid_book_ids = set(books['book_id'])\n",
    "    read = read[(read['is_read'] == 1) & (read['book_id'].isin(valid_book_ids))]\n",
    "    interactions = interactions[interactions['book_id'].isin(valid_book_ids)]\n",
    "    reviews = reviews[reviews['book_id'].isin(valid_book_ids)]\n",
    "\n",
    "    return (\n",
    "        books, interactions, read, reviews,\n",
    "        umap_embeddings, faiss_index, book_id_to_index,\n",
    "        user_id_to_index_gat, book_id_to_index_gat, all_embeddings\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_rating(log_scaled_ratings, min_rating):\n",
    "    log_scaled_ratings = np.asarray(log_scaled_ratings, dtype=float)\n",
    "\n",
    "    # Reverse log1p transformation\n",
    "    original_ratings = np.expm1(log_scaled_ratings)\n",
    "\n",
    "    # Adjust for minimum rating\n",
    "    if min_rating:\n",
    "        original_ratings += min_rating\n",
    "\n",
    "    # Clip values between 0 and 5\n",
    "    return np.clip(original_ratings, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gat_model():\n",
    "    from Gatv2Conv import GATModel\n",
    "    model = GATModel(\n",
    "        in_channels=32,\n",
    "        hidden_channels=25,\n",
    "        out_channels=1,\n",
    "        num_heads=25,\n",
    "        edge_feature_dim=386\n",
    "    )\n",
    "    model.load_state_dict(torch.load('../RecSysJupyter/gat_model.pth'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_HDBSCAN(book_id, books, umap_embeddings, faiss_index, book_id_to_index, predicted_ratings, top_n=5):\n",
    "    if book_id not in book_id_to_index:\n",
    "        return []\n",
    "\n",
    "    book_idx = book_id_to_index[book_id]\n",
    "    distances, indices = faiss_index.search(np.array([umap_embeddings[book_idx]]), top_n + 1)\n",
    "\n",
    "    recommendations = []\n",
    "    for idx in indices[0][1:]:  # Skip the book itself\n",
    "        if idx >= len(books):\n",
    "            continue\n",
    "        recommended_book_id = books.iloc[idx][\"book_id\"]\n",
    "\n",
    "        # Here, get the rating for the original book if available or some other logic for ratings\n",
    "        rating = predicted_ratings.get(book_id, None)\n",
    "\n",
    "        # Add the recommended book as a dictionary with the same rating\n",
    "        recommendations.append({\n",
    "            'book_id': recommended_book_id,\n",
    "            'rating': rating  # Keep the same predicted rating as the original\n",
    "        })\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass all recs through hdbscan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if no hdbscan then this function is as normal and same metrics as for nmf normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_NMF(nmf_model, interactions, user_id, books_read, books, min_rating=None, n_recommendations=5):\n",
    "    all_books = interactions['book_id'].unique()\n",
    "\n",
    "    user_predictions = [\n",
    "        (book_id, nmf_model.predict(uid=user_id, iid=book_id).est) for book_id in all_books\n",
    "    ]\n",
    "    \n",
    "    # Sort by predicted rating in descending order\n",
    "    top_books = sorted(user_predictions, key=lambda x: x[1], reverse=True)[:n_recommendations]\n",
    "\n",
    "    # Denormalize the ratings\n",
    "    denormed_recommendations = [\n",
    "        {'book_id': book_id, 'predicted_rating': denormalize_rating([rating], min_rating)[0]}\n",
    "        for book_id, rating in top_books\n",
    "    ]\n",
    "\n",
    "    return denormed_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_GAT(user_id, unread_book_ids, all_embeddings, user_id_to_index, book_id_to_index, books_df, min_rating=None, top_n=5):\n",
    "    user_index = user_id_to_index.get(user_id)\n",
    "    if user_index is None:\n",
    "        raise ValueError(f\"User ID {user_id} not found in index mappings.\")\n",
    "\n",
    "    user_embedding = all_embeddings[user_index]\n",
    "\n",
    "    predictions = []\n",
    "    for book_id in unread_book_ids:\n",
    "        book_index = book_id_to_index.get(book_id)\n",
    "        if book_index is None:\n",
    "            continue\n",
    "\n",
    "        book_embedding = all_embeddings[book_index]\n",
    "        predicted_rating = np.expm1(np.dot(user_embedding, book_embedding))  # Denormalize if needed\n",
    "\n",
    "        # Denormalize the predicted rating\n",
    "        denormed_rating = denormalize_rating([predicted_rating], min_rating)[0]\n",
    "\n",
    "        predictions.append({\n",
    "            \"book_id\": book_id,\n",
    "            \"predicted_rating\": denormed_rating\n",
    "        })\n",
    "\n",
    "    # Sort by predicted rating in descending order\n",
    "    sorted_books = sorted(predictions, key=lambda x: x[\"predicted_rating\"], reverse=True)\n",
    "\n",
    "    # Return top_n recommendations, each with book_id and predicted_rating\n",
    "    return sorted_books[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_recommendations_weighted(gat_recs, nmf_recs, final_size=5, gat_weight=0.6, nmf_weight=0.4):\n",
    "    combined, seen_books = [], set()\n",
    "\n",
    "    # Create a list of all recommendations with their respective weights\n",
    "    weighted_recs = []\n",
    "\n",
    "    # Add GAT recommendations with weighted scores\n",
    "    for rec in gat_recs:\n",
    "        if rec['book_id'] not in seen_books:\n",
    "            weighted_recs.append({'book_id': rec['book_id'], 'score': gat_weight * rec['predicted_rating']})\n",
    "            seen_books.add(rec['book_id'])\n",
    "\n",
    "    # Add NMF recommendations with weighted scores\n",
    "    for rec in nmf_recs:\n",
    "        if rec['book_id'] not in seen_books:\n",
    "            weighted_recs.append({'book_id': rec['book_id'], 'score': nmf_weight * rec['predicted_rating']})\n",
    "            seen_books.add(rec['book_id'])\n",
    "\n",
    "    # Sort all recommendations by score in descending order\n",
    "    weighted_recs.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    # Select the top recommendations until reaching final_size\n",
    "    for rec in weighted_recs:\n",
    "        if len(combined) < final_size:\n",
    "            combined.append(rec)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books, interactions, read, reviews, umap_embeddings, faiss_index, book_id_to_index, user_id_to_index_gat, book_id_to_index_gat, all_embeddings = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by user_id and calculate the number of books and reviews\n",
    "user_stats = read.groupby('user_id')['book_id'].nunique()\n",
    "user_reviews_count = reviews.groupby('user_id').size()\n",
    "\n",
    "# Merge the stats into a single DataFrame\n",
    "user_data = pd.DataFrame({\n",
    "    'num_books': user_stats,\n",
    "    'num_reviews': user_reviews_count\n",
    "}).fillna(0)\n",
    "\n",
    "# Define conditions for case_2 and case_3\n",
    "# case_2_users = user_data[(user_data['num_books'] >= 10) & (user_data['num_reviews'] < 5)].index\n",
    "case_3_users = user_data[(user_data['num_books'] >= 10) & (user_data['num_reviews'] >= 5)].index\n",
    "\n",
    "# Filter out users not in interactions and user_id_to_index_gat\n",
    "# valid_case_2_users = [user_id for user_id in case_2_users if user_id in interactions.index]\n",
    "valid_case_3_users = [user_id for user_id in case_3_users if user_id in reviews.index and user_id in user_id_to_index_gat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(valid_case_2_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_case_3_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_2, test_2 = train_test_split(valid_case_2_users)\n",
    "train_3, test_3 = train_test_split(valid_case_3_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_case_3_users(case_3_users, interactions, books, read, reviews,\n",
    "                          umap_embeddings, faiss_index, book_id_to_index,\n",
    "                          user_id_to_index_gat, book_id_to_index_gat, all_embeddings,\n",
    "                          final_size=5, gat_weight=0.7, nmf_weight=0.3):\n",
    "    \n",
    "    best_nmf = joblib.load('../Pickle/best_nmf_model.pkl')\n",
    "    results = []\n",
    "\n",
    "    for user_id in case_3_users:\n",
    "        user_books_read = read[read['user_id'] == user_id]\n",
    "        user_reviews = reviews[reviews['user_id'] == user_id]\n",
    "\n",
    "        if user_books_read.empty or user_reviews.empty:\n",
    "            continue \n",
    "\n",
    "        # Validate user in GAT index mapping\n",
    "        if user_id not in user_id_to_index_gat:\n",
    "            print(f\"Skipping user {user_id}: not in GAT user index.\")\n",
    "            continue\n",
    "\n",
    "        # Get books the user hasn't read yet\n",
    "        all_books = interactions['book_id'].unique()\n",
    "        unread_books = list(set(all_books) - set(user_books_read['book_id']))\n",
    "\n",
    "        # Get NMF recommendations\n",
    "        nmf_recs = recommend_books_NMF(\n",
    "            nmf_model=best_nmf,\n",
    "            interactions=interactions,\n",
    "            user_id=user_id,\n",
    "            books_read=user_books_read['book_id'],\n",
    "            books=books,\n",
    "            n_recommendations=final_size * 2\n",
    "        )\n",
    "\n",
    "        # Get GAT recommendations\n",
    "        gat_recs = recommend_books_GAT(\n",
    "            user_id=user_id,\n",
    "            unread_book_ids=unread_books,\n",
    "            all_embeddings=all_embeddings,\n",
    "            user_id_to_index=user_id_to_index_gat,\n",
    "            book_id_to_index=book_id_to_index_gat,\n",
    "            books_df=books,\n",
    "            top_n=final_size * 2\n",
    "        )\n",
    "\n",
    "        # Merge recommendations with weighted approach\n",
    "        merged_recs = merge_recommendations_weighted(\n",
    "            gat_recs=gat_recs,\n",
    "            nmf_recs=nmf_recs,\n",
    "            final_size=final_size,\n",
    "            gat_weight=gat_weight,\n",
    "            nmf_weight=nmf_weight\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'user_id': user_id,\n",
    "            'recommended_books': merged_recs  # Contains book_id + weighted score\n",
    "        })\n",
    "\n",
    "        print(f\"User {user_id} hybrid weighted recommendations: {merged_recs}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Execute the evaluation with case_3_users\n",
    "hybrid_results = evaluate_case_3_users(\n",
    "    case_3_users=case_3_users,\n",
    "    interactions=interactions,\n",
    "    books=books,\n",
    "    read=read,\n",
    "    reviews=reviews,\n",
    "    umap_embeddings=umap_embeddings,\n",
    "    faiss_index=faiss_index,\n",
    "    book_id_to_index=book_id_to_index,\n",
    "    user_id_to_index_gat=user_id_to_index_gat,\n",
    "    book_id_to_index_gat=book_id_to_index_gat,\n",
    "    all_embeddings=all_embeddings,\n",
    "    gat_weight=0.7,  # Assign weight to GAT model\n",
    "    nmf_weight=0.3   # Assign weight to NMF model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_at_k(results, ground_truth, k=5):\n",
    "    precisions = []\n",
    "    for user_result in results:\n",
    "        user_id = user_result['user_id']\n",
    "        recommended_ids = [rec['book_id'] for rec in user_result['recommended_books'][:k]]\n",
    "\n",
    "        true_books = set(ground_truth.get(user_id, []))\n",
    "\n",
    "        hits = len(set(recommended_ids) & true_books)\n",
    "        precision = hits / k\n",
    "        precisions.append(precision)\n",
    "\n",
    "    avg_precision = np.mean(precisions)\n",
    "    print(f\"Precision@{k}: {avg_precision:.4f}\")\n",
    "    return avg_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_case_3_reads = reviews[reviews['user_id'].isin(valid_case_3_users)]\n",
    "\n",
    "valid_case_3_user_read_books = valid_case_3_reads.groupby('user_id')['book_id'].apply(set).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = eval_at_k(hybrid_results, valid_case_3_user_read_books, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_case_2_users(case_2_users, interactions, books, read):\n",
    "#     best_nmf = joblib.load('../Pickle/best_nmf_model.pkl')\n",
    "#     results = []\n",
    "\n",
    "#     predicted_ratings = {} \n",
    "\n",
    "#     for user_id in case_2_users:\n",
    "#         user_books_read = read[read['user_id'] == user_id]\n",
    "#         if user_books_read.empty:\n",
    "#             continue  # Skip if the user hasn't read any books\n",
    "\n",
    "#         user_books = user_books_read['book_id'].tolist()  # List of books user has read\n",
    "\n",
    "#         recs = recommend_books_NMF(\n",
    "#             nmf_model=best_nmf,\n",
    "#             interactions=interactions,\n",
    "#             user_id=user_id,\n",
    "#             books_read=user_books,\n",
    "#             books=books\n",
    "#         )\n",
    "\n",
    "#         nmf_recommended_book_ids = [rec['book_id'] for rec in recs]\n",
    "#         nmf_predicted_ratings = {rec['book_id']: rec['predicted_rating'] for rec in recs}\n",
    "\n",
    "#         predicted_ratings.update(nmf_predicted_ratings)\n",
    "\n",
    "#         final_recommended_books = {\n",
    "#             book_id: predicted_ratings.get(book_id, 0) for book_id in nmf_recommended_book_ids\n",
    "#         }\n",
    "\n",
    "#         results.append({\n",
    "#             'user_id': user_id,\n",
    "#             'recommended_books': [\n",
    "#                 {'book_id': book_id, 'predicted_rating': predicted_rating}\n",
    "#                 for book_id, predicted_rating in final_recommended_books.items()\n",
    "#             ][:5]  # Limit to final 5 recommendations\n",
    "#         })\n",
    "\n",
    "#         print(f\"User {user_id} recommended books and predicted ratings: {list(final_recommended_books.items())[:5]}\")\n",
    "\n",
    "#     return results\n",
    "\n",
    "# nmf_results = evaluate_case_2_users(case_2_users, interactions, books, read)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
