{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load books in chunks\n",
    "    books_list = []\n",
    "    with open('../Pickle/books.pkl', 'rb') as file:\n",
    "        while True:\n",
    "            try:\n",
    "                books_list.append(pickle.load(file))\n",
    "            except EOFError:\n",
    "                break\n",
    "    books = pd.concat(books_list, ignore_index=True).drop_duplicates(subset='title', keep='first')\n",
    "\n",
    "    # Load other datasets\n",
    "    interactions = pd.read_pickle('../Pickle/interactions.pkl')\n",
    "    read = pd.read_pickle('../Pickle/read.pkl')\n",
    "    reviews = pd.read_pickle('../Pickle/reviews.pkl')\n",
    "    \n",
    "    # Load embeddings and indexes\n",
    "    umap_embeddings = pd.read_pickle('../Pickle/umap_embeddings.pkl')\n",
    "    faiss_index = faiss.read_index('../Pickle/faiss_index.bin')\n",
    "    book_id_to_index = pd.read_pickle('../Pickle/book_id_to_index.pkl')\n",
    "    user_id_to_index_gat = pd.read_pickle('../Pickle/user_id_to_index_gat.pkl')\n",
    "    book_id_to_index_gat = pd.read_pickle('../Pickle/book_id_to_index_gat.pkl')\n",
    "    all_embeddings = pd.read_pickle('../Pickle/gat_embeddings.pkl')\n",
    "    clustered_books = pd.read_pickle('../Pickle/clustered_books.pkl')\n",
    "    clusters = pd.read_pickle('../Pickle/clusters.pkl')\n",
    "\n",
    "    # Filter datasets to valid book ids\n",
    "    valid_book_ids = set(books['book_id'])\n",
    "    read = read[(read['is_read'] == 1) & (read['book_id'].isin(valid_book_ids))]\n",
    "    interactions = interactions[interactions['book_id'].isin(valid_book_ids)]\n",
    "    reviews = reviews[reviews['book_id'].isin(valid_book_ids)]\n",
    "\n",
    "    return (\n",
    "        books, interactions, read, reviews,\n",
    "        umap_embeddings, faiss_index, book_id_to_index,\n",
    "        user_id_to_index_gat, book_id_to_index_gat, all_embeddings, clustered_books, clusters\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_rating(log_scaled_ratings, min_rating):\n",
    "    log_scaled_ratings = np.asarray(log_scaled_ratings, dtype=float)\n",
    "    original_ratings = np.expm1(log_scaled_ratings)\n",
    "    if min_rating:\n",
    "        original_ratings += min_rating\n",
    "    return np.clip(original_ratings, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gat_model():\n",
    "    from Gatv2Conv import GATModel\n",
    "    model = GATModel(\n",
    "        in_channels=32,\n",
    "        hidden_channels=25,\n",
    "        out_channels=1,\n",
    "        num_heads=25,\n",
    "        edge_feature_dim=386\n",
    "    )\n",
    "    model.load_state_dict(torch.load('../RecSysJupyter/gat_model.pth'))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_HDBSCAN(book_id, book_id_to_index, soft_clusters, umap_embeddings, clustered_books, faiss_index, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend books based on soft clustering (probabilistic membership vectors) using the HDBSCAN clustering results.\n",
    "    \n",
    "    This function recommends books by first checking if the input book belongs to a cluster with significant \n",
    "    soft membership. If the book has strong membership in one or more clusters, recommendations are made based \n",
    "    on the weighted distances between the book's embedding and those of the books in the same cluster(s). \n",
    "    If the book does not have a strong membership in any cluster, or if it's an outlier, a global search using \n",
    "    FAISS is performed to find similar books.\n",
    "\n",
    "    Args:\n",
    "    book_id (int): The ID of the book for which recommendations are to be made.\n",
    "    book_id_to_index (dict): A mapping of book IDs to their corresponding index in the embedding and clustering arrays.\n",
    "    soft_clusters (numpy.ndarray): The soft clustering (membership vectors), where each element contains the \n",
    "                                    membership probabilities for each cluster for a given book.\n",
    "    umap_embeddings (numpy.ndarray): The UMAP embeddings for all books.\n",
    "    clustered_books (pandas.DataFrame): A DataFrame containing the books, with at least a column `book_id` \n",
    "                                        containing the book IDs.\n",
    "    faiss_index (faiss.Index): The FAISS index for performing global nearest neighbor search.\n",
    "    top_n (int, optional): The number of recommended books to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tuples containing recommended book IDs and their corresponding similarity scores, sorted by similarity.\n",
    "    \"\"\"\n",
    "    # Check if the book ID exists\n",
    "    if book_id not in book_id_to_index:\n",
    "        print(f\"Book ID {book_id} not found.\")\n",
    "        return []\n",
    "    \n",
    "    # Get the index and soft membership of the input book\n",
    "    book_idx = book_id_to_index[book_id]\n",
    "    book_soft_membership = soft_clusters[book_idx]\n",
    "\n",
    "    # Get the embedding for the book\n",
    "    query_embedding = umap_embeddings[book_idx].reshape(1, -1).astype('float32')\n",
    "\n",
    "    # If the book is not an outlier (it will have a soft membership in multiple clusters)\n",
    "    if np.max(book_soft_membership) > 0:  # Check if the book has any strong membership\n",
    "        # Get the indices of books with significant membership in the same clusters\n",
    "        same_cluster_indices = np.where(book_soft_membership > 0.1)[0]  # Threshold for significant membership (e.g., 0.1)\n",
    "\n",
    "        # If there are other books in the same clusters\n",
    "        if len(same_cluster_indices) > 0:\n",
    "            # Compute weighted distances to all books based on soft membership\n",
    "            weighted_distances = []\n",
    "            for idx in same_cluster_indices:\n",
    "                soft_membership = soft_clusters[idx]\n",
    "                weighted_distance = np.sum(soft_membership * np.linalg.norm(umap_embeddings[idx] - query_embedding, axis=1))\n",
    "                weighted_distances.append(weighted_distance)\n",
    "\n",
    "            # Get top_n closest books based on weighted distances\n",
    "            top_indices = np.argsort(weighted_distances)[:top_n]\n",
    "\n",
    "            # Map back to book IDs and return similarity scores\n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                similar_book_idx = same_cluster_indices[idx]\n",
    "                similar_book_id = clustered_books.iloc[similar_book_idx]['book_id']\n",
    "                similarity_score = 1 / (1 + weighted_distances[idx])  # Convert distance to similarity\n",
    "                results.append((similar_book_id, similarity_score))\n",
    "\n",
    "            return results\n",
    "\n",
    "    # If outlier or no significant membership in soft clusters, perform global FAISS search\n",
    "    print(\"Book is an outlier or has no significant cluster neighbors, using global search\")\n",
    "\n",
    "    D, I = faiss_index.search(query_embedding, top_n + 1)  # +1 because it includes itself as the closest neighbor\n",
    "    results = []\n",
    "    count = 0\n",
    "    for idx in I[0]:\n",
    "        if idx == book_idx:\n",
    "            continue  # Skip the query book itself\n",
    "        similar_book_id = clustered_books.iloc[idx]['book_id']\n",
    "        distance = D[0, count]\n",
    "        similarity_score = 1 / (1 + distance)\n",
    "        results.append((similar_book_id, similarity_score))\n",
    "        count += 1\n",
    "        if len(results) == top_n:\n",
    "            break\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_nmf(\n",
    "    nmf_model, \n",
    "    interactions, \n",
    "    user_id, \n",
    "    books_read, \n",
    "    books, \n",
    "    min_rating, \n",
    "    n_recommendations=5, \n",
    "    top_n_factors=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Recommend books to a user using a trained NMF (Non-negative Matrix Factorization) model for collaborative filtering.\n",
    "    This function predicts ratings for books that the user hasn't already read and provides explanations for the \n",
    "    recommendations based on the latent factors learned by the NMF model.\n",
    "    \n",
    "    The function predicts the ratings for candidate books that the user has not interacted with and ranks them \n",
    "    by predicted rating. It also explains the recommendations by providing contributions from the top latent factors \n",
    "    that influence the predicted rating.\n",
    "\n",
    "    Args:\n",
    "    nmf_model (surprise.NMF): The trained NMF model used to predict ratings.\n",
    "    interactions (pandas.DataFrame): A DataFrame containing user-item interactions (e.g., user ratings for books).\n",
    "    user_id (int): The ID of the user for whom recommendations are being generated.\n",
    "    books_read (list of int): List of book IDs that the user has already read.\n",
    "    books (pandas.DataFrame): A DataFrame containing the book details, including 'book_id' and 'title'.\n",
    "    min_rating (float, optional): The minimum rating used for denormalization (if any). Default is None.\n",
    "    n_recommendations (int, optional): The number of recommendations to return. Default is 5.\n",
    "    top_n_factors (int, optional): The number of top latent factors to use for explaining the recommendation. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, each containing a recommended book's ID, title, predicted rating, and an explanation \n",
    "          of the recommendation based on the latent factors.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the unique book ids from interactions\n",
    "    all_books = interactions['book_id'].unique()\n",
    "    \n",
    "    # Filter out books the user has already read (optional, if books_read is provided)\n",
    "    candidate_books = [book_id for book_id in all_books if book_id not in books_read]\n",
    "\n",
    "    # Generate predicted ratings for each candidate book\n",
    "    user_predictions = [\n",
    "        (book_id, nmf_model.predict(uid=user_id, iid=book_id).est)\n",
    "        for book_id in candidate_books\n",
    "    ]\n",
    "    \n",
    "    # Sort by predicted rating (highest first)\n",
    "    top_books = sorted(user_predictions, key=lambda x: x[1], reverse=True)[:n_recommendations]\n",
    "    \n",
    "    # Get Surprise internal user/item mappings\n",
    "    user_inner_id = nmf_model.trainset.to_inner_uid(user_id)\n",
    "    pu = nmf_model.pu\n",
    "    qi = nmf_model.qi\n",
    "    \n",
    "    user_factors = pu[user_inner_id]\n",
    "\n",
    "    recommendations_with_explanations = []\n",
    "\n",
    "    for book_id, raw_pred_rating in top_books:\n",
    "        # Get internal item index for the model\n",
    "        try:\n",
    "            item_inner_id = nmf_model.trainset.to_inner_iid(book_id)\n",
    "        except ValueError:\n",
    "            continue  # Item not in the model\n",
    "        \n",
    "        item_factors = qi[item_inner_id]\n",
    "\n",
    "        # Contributions from each latent factor\n",
    "        contributions = user_factors * item_factors\n",
    "        predicted_rating = contributions.sum()\n",
    "\n",
    "        # Get indices of top contributing latent factors\n",
    "        top_factors_idx = np.argsort(np.abs(contributions))[::-1][:top_n_factors]\n",
    "\n",
    "        # Book title lookup\n",
    "        book_title = books.loc[books['book_id'] == book_id, 'title'].values[0]\n",
    "\n",
    "        # Format the factor explanations\n",
    "        explanations = []\n",
    "        for rank, i in enumerate(top_factors_idx, 1):\n",
    "            explanation = {\n",
    "                'latent_factor': int(i + 1),\n",
    "                'user_affinity': round(user_factors[i], 3),\n",
    "                'item_relevance': round(item_factors[i], 3),\n",
    "                'contribution': round(contributions[i], 3)\n",
    "            }\n",
    "            explanations.append(explanation)\n",
    "        \n",
    "            denormed_rating = denormalize_rating([predicted_rating], min_rating)[0]\n",
    "\n",
    "        # Append the final recommendation and explanation\n",
    "        recommendations_with_explanations.append({\n",
    "            'book_id': book_id,\n",
    "            'title': book_title,\n",
    "            'predicted_rating': round(denormed_rating, 2),\n",
    "            'top_latent_factors': explanations\n",
    "        })\n",
    "\n",
    "    return recommendations_with_explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_GAT(user_id, unread_book_ids, all_embeddings, user_id_to_index, book_id_to_index, books_df, min_rating, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend books to a user using a Graph Attention Network (GAT)-based collaborative filtering approach.\n",
    "    This function predicts the ratings for books that the user has not read yet based on user and book embeddings \n",
    "    and returns the top N recommendations.\n",
    "\n",
    "    Args:\n",
    "    user_id (int): The ID of the user for whom recommendations are being generated.\n",
    "    unread_book_ids (list of int): List of book IDs that the user has not read.\n",
    "    all_embeddings (numpy.ndarray): An array containing the embeddings for all users and books.\n",
    "    user_id_to_index (dict): A dictionary mapping user IDs to indices in the embedding matrix.\n",
    "    book_id_to_index (dict): A dictionary mapping book IDs to indices in the embedding matrix.\n",
    "    books_df (pandas.DataFrame): A DataFrame containing book information (e.g., 'book_id' and 'title').\n",
    "    min_rating (float, optional): The minimum rating used for denormalization (if any). Default is None.\n",
    "    top_n (int, optional): The number of recommendations to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of the top N recommended books, each containing the book's ID and the predicted rating.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the user's index in the embedding matrix\n",
    "    user_index = user_id_to_index.get(user_id)\n",
    "    if user_index is None:\n",
    "        raise ValueError(f\"User ID {user_id} not found in index mappings.\")\n",
    "\n",
    "    # Extract the user's embedding\n",
    "    user_embedding = all_embeddings[user_index]\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # Predict ratings for all unread books\n",
    "    for book_id in unread_book_ids:\n",
    "        book_index = book_id_to_index.get(book_id)\n",
    "        if book_index is None:\n",
    "            continue  # Skip books not in the book_id_to_index\n",
    "\n",
    "        # Extract the book's embedding\n",
    "        book_embedding = all_embeddings[book_index]\n",
    "\n",
    "        # Compute the predicted rating as the dot product between the user's and book's embedding\n",
    "        predicted_rating = np.expm1(np.dot(user_embedding, book_embedding))  # Denormalize if needed\n",
    "\n",
    "        denormed_rating = denormalize_rating([predicted_rating], min_rating)[0] if min_rating else predicted_rating\n",
    "\n",
    "        predictions.append({\n",
    "            \"book_id\": book_id,\n",
    "            \"predicted_rating\": denormed_rating\n",
    "        })\n",
    "\n",
    "    # Sort the predictions by predicted rating in descending order\n",
    "    sorted_books = sorted(predictions, key=lambda x: x[\"predicted_rating\"], reverse=True)\n",
    "\n",
    "    # Return the top_n recommendations\n",
    "    return sorted_books[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_recommendations_weighted(gat_recs, nmf_recs, final_size=5, gat_weight=0.6, nmf_weight=0.4):\n",
    "    \"\"\"\n",
    "    Merges recommendations from two different models (GAT and NMF) by weighting their scores and selecting the top recommendations.\n",
    "    This function combines recommendations from both models, assigns weights to each model's predictions,\n",
    "    and returns a final list of top recommendations based on the weighted scores.\n",
    "\n",
    "    Args:\n",
    "    gat_recs (list of dict): List of GAT recommendations, where each recommendation is a dictionary \n",
    "                              containing 'book_id' and 'predicted_rating'.\n",
    "    nmf_recs (list of dict): List of NMF recommendations, where each recommendation is a dictionary \n",
    "                              containing 'book_id' and 'predicted_rating'.\n",
    "    final_size (int, optional): The number of top recommendations to return. Default is 5.\n",
    "    gat_weight (float, optional): The weight assigned to the GAT model's recommendations. Default is 0.6.\n",
    "    nmf_weight (float, optional): The weight assigned to the NMF model's recommendations. Default is 0.4.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing the top 'final_size' recommendations, each with 'book_id' and the combined score.\n",
    "    \n",
    "    Notes:\n",
    "    - Recommendations from both models are weighted based on the specified `gat_weight` and `nmf_weight`.\n",
    "    - Duplicates (books already recommended by one model) are removed, and the final recommendations are selected based on \n",
    "      the weighted scores.\n",
    "    - The function sorts recommendations in descending order by their combined weighted score and returns the top `final_size` recommendations.\n",
    "    \"\"\"\n",
    "    \n",
    "    combined, seen_books = [], set()\n",
    "\n",
    "    # Create a list of all recommendations with their respective weights\n",
    "    weighted_recs = []\n",
    "\n",
    "    # Add GAT recommendations with weighted scores\n",
    "    for rec in gat_recs:\n",
    "        if rec['book_id'] not in seen_books:\n",
    "            weighted_recs.append({'book_id': rec['book_id'], 'score': gat_weight * rec['predicted_rating']})\n",
    "            seen_books.add(rec['book_id'])\n",
    "\n",
    "    # Add NMF recommendations with weighted scores\n",
    "    for rec in nmf_recs:\n",
    "        if rec['book_id'] not in seen_books:\n",
    "            weighted_recs.append({'book_id': rec['book_id'], 'score': nmf_weight * rec['predicted_rating']})\n",
    "            seen_books.add(rec['book_id'])\n",
    "\n",
    "    # Sort all recommendations by score in descending order\n",
    "    weighted_recs.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    # Select the top recommendations until reaching final_size\n",
    "    for rec in weighted_recs:\n",
    "        if len(combined) < final_size:\n",
    "            combined.append(rec)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books, interactions, read, reviews, umap_embeddings, faiss_index, book_id_to_index, user_id_to_index_gat, book_id_to_index_gat, all_embeddings, clustered_books, clusters = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_stats(user_id, interactions, reviews):\n",
    "    \"\"\"\n",
    "    Get the statistics about a user's reading and reviewing activity.\n",
    "\n",
    "    Args:\n",
    "    user_id (int): The user ID for whom to get the statistics.\n",
    "    interactions (DataFrame): The interactions dataframe containing user-book interaction data.\n",
    "    reviews (DataFrame): The reviews dataframe containing review data for users.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the number of books the user has read (books_read) and the number of reviews they've written (reviews_written).\n",
    "    \"\"\"\n",
    "    # Get the number of books read by the user from the interactions dataframe\n",
    "    books_read = interactions[interactions['user_id'] == user_id]['book_id'].nunique()\n",
    "\n",
    "    # Get the number of reviews written by the user from the reviews dataframe\n",
    "    reviews_written = reviews[reviews['user_id'] == user_id].shape[0]\n",
    "\n",
    "    return books_read, reviews_written\n",
    "\n",
    "\n",
    "def recommend_for_user(user_id, interactions, reviews, umap_embeddings, faiss_index, book_id_to_index, clustered_books, clusters, nmf_model, all_embeddings, user_id_to_index_gat, book_id_to_index_gat, books, min_rating=None, top_n=5, final_size=5, gat_weight=0.6, nmf_weight=0.4):\n",
    "    \"\"\"\n",
    "    Recommends books for a user based on their reading and review activity.\n",
    "\n",
    "    This function first analyzes the user's activity to determine an appropriate recommendation strategy. \n",
    "    Based on the number of books read and reviews written, it selects one of the following recommendation methods:\n",
    "    - If the user has read fewer than 10 books, it uses HDBSCAN-based recommendations.\n",
    "    - If the user has read more than 10 books but written fewer than 6 reviews, it uses NMF-based recommendations.\n",
    "    - If the user has read more than 10 books and written more than 5 reviews, it combines NMF and GAT-based recommendations using weighted merging.\n",
    "\n",
    "    Args:\n",
    "    user_id (int): The user ID for whom to generate recommendations.\n",
    "    interactions (DataFrame): The interactions dataframe containing user-book interaction data.\n",
    "    reviews (DataFrame): The reviews dataframe containing user review data.\n",
    "    umap_embeddings (ndarray): UMAP embeddings of books used for HDBSCAN-based recommendations.\n",
    "    faiss_index (Index): FAISS index for performing efficient nearest neighbor search.\n",
    "    book_id_to_index (dict): A dictionary mapping book IDs to their index in the embeddings.\n",
    "    clustered_books (DataFrame): DataFrame containing clustered books.\n",
    "    clusters (ndarray): An array of cluster assignments for each book.\n",
    "    nmf_model (NMF model): Trained NMF model for recommendation generation.\n",
    "    all_embeddings (ndarray): Embeddings of books for the GAT model.\n",
    "    user_id_to_index_gat (dict): Dictionary mapping user IDs to indices in the GAT model.\n",
    "    book_id_to_index_gat (dict): Dictionary mapping book IDs to indices in the GAT model.\n",
    "    books (DataFrame): DataFrame containing book information, used for title lookup in recommendations.\n",
    "    min_rating (float, optional): The minimum rating for denormalization. Default is None.\n",
    "    top_n (int, optional): The number of top recommendations to return. Default is 5.\n",
    "    final_size (int, optional): The final number of top recommendations to return after merging. Default is 5.\n",
    "    gat_weight (float, optional): Weight for the GAT model's recommendations. Default is 0.6.\n",
    "    nmf_weight (float, optional): Weight for the NMF model's recommendations. Default is 0.4.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing the final recommendations for the user, each with 'book_id' and 'predicted_rating'.\n",
    "    \n",
    "    Notes:\n",
    "    - The function adapts the recommendation strategy based on the user's activity (books read and reviews written).\n",
    "    - The final recommendation set is a combination of models and weighted based on the user's activity.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get user statistics (books read and reviews written)\n",
    "    books_read, reviews_written = get_user_stats(user_id, interactions, reviews)\n",
    "\n",
    "    # Determine recommendation strategy\n",
    "    if books_read < 10:\n",
    "        # Use HDBSCAN-based recommendation if user has read fewer than 10 books\n",
    "        print(f\"User {user_id}: Less than 10 books read, using HDBSCAN.\")\n",
    "        unread_books = interactions[interactions['user_id'] == user_id]['book_id'].unique()\n",
    "        recommendations = recommend_HDBSCAN(user_id, book_id_to_index, clusters, umap_embeddings, clustered_books, faiss_index, top_n)\n",
    "\n",
    "    elif books_read > 10 and reviews_written <= 5:\n",
    "        # Use NMF-based recommendation if user has read more than 10 books but written fewer than 6 reviews\n",
    "        print(f\"User {user_id}: More than 10 books read but less than 6 reviews, using NMF.\")\n",
    "        unread_books = interactions[interactions['user_id'] == user_id]['book_id'].unique()\n",
    "        recommendations = recommend_nmf(nmf_model, interactions, user_id, unread_books, books, min_rating, top_n)\n",
    "\n",
    "    elif books_read > 10 and reviews_written > 5:\n",
    "        # Use the combined NMF + GAT recommendation if user has read more than 10 books and written more than 5 reviews\n",
    "        print(f\"User {user_id}: More than 10 books read and more than 5 reviews, using combined NMF + GAT.\")\n",
    "        unread_books = interactions[interactions['user_id'] == user_id]['book_id'].unique()\n",
    "        \n",
    "        # Get NMF recommendations\n",
    "        nmf_recs = recommend_nmf(nmf_model, interactions, user_id, unread_books, books, min_rating, top_n)\n",
    "        \n",
    "        # Get GAT recommendations\n",
    "        gat_recs = recommend_GAT(user_id, unread_books, all_embeddings, user_id_to_index_gat, book_id_to_index_gat, books, min_rating, top_n)\n",
    "        \n",
    "        # Merge NMF and GAT recommendations\n",
    "        recommendations = merge_recommendations_weighted(gat_recs, nmf_recs, final_size, gat_weight, nmf_weight)\n",
    "\n",
    "    return recommendations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
