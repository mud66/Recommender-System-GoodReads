{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "read = pd.read_pickle('Pickle/read.pkl')\n",
    "books = pd.read_pickle('Pickle/books.pkl')\n",
    "reviews = pd.read_pickle('Pickle/reviews.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.read_pickle('Pickle/review_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:07<00:00, 141515.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare embeddings\n",
    "embeddings_df.set_index('review_id', inplace=True)\n",
    "def get_review_embedding(review_id): \n",
    "    try: return embeddings_df.at[review_id, 'embeddings'] \n",
    "    except KeyError: return None # Return None if the review_id is not found \n",
    "reviews['embeddings'] = reviews['review_id'].progress_apply(get_review_embedding) \n",
    "reviews = reviews.dropna(subset=['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_counts = reviews['user_id'].value_counts()\n",
    "valid_users = review_counts[review_counts >= 3].index\n",
    "reviews = reviews[reviews['user_id'].isin(valid_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique user IDs from all relevant DataFrames\n",
    "interaction_user_ids = set(read['user_id'].unique())\n",
    "review_user_ids = set(reviews['user_id'].unique())\n",
    "read_user_ids = set(read['user_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common user IDs\n",
    "common_user_ids = interaction_user_ids.intersection(review_user_ids).intersection(read_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_review_embeddings = reviews.groupby('user_id')['embeddings'].apply(lambda x: np.mean(np.vstack(x.dropna()), axis=0)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_review_embeddings.set_index('user_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_embeddings(reviews_df, base_weight=0.6, min_weight=0.6):\n",
    "    # Create a copy of the DataFrame\n",
    "    reviews_df = reviews_df.copy()\n",
    "\n",
    "    # Normalize the number of votes\n",
    "    if reviews_df['n_votes'].max() > 0:  # Ensure no division by zero\n",
    "        reviews_df['n_votes_normalized'] = reviews_df['n_votes'] / reviews_df['n_votes'].max()\n",
    "    else:\n",
    "        reviews_df['n_votes_normalized'] = 0\n",
    "\n",
    "    # Calculate weights with a base weight and ensure min weight is applied\n",
    "    reviews_df['weight'] = base_weight + reviews_df['n_votes_normalized']\n",
    "    reviews_df['weight'] = reviews_df['weight'].apply(lambda x: max(x, min_weight))\n",
    "\n",
    "    # Apply weights to embeddings\n",
    "    reviews_df['embeddings'] = reviews_df.apply(lambda row: row['embeddings'] * row['weight'], axis=1)\n",
    "\n",
    "    return reviews_df\n",
    "\n",
    "# Apply the function\n",
    "reviews = calculate_weighted_embeddings(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_reviews, test_reviews = train_test_split(reviews[reviews['user_id'].isin(common_user_ids)], test_size=0.2, random_state=42)\n",
    "train_users = train_reviews['user_id'].unique()\n",
    "test_users = test_reviews['user_id'].unique()\n",
    "train_read = read[read['user_id'].isin(train_users)]\n",
    "test_read = read[read['user_id'].isin(test_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading history\n",
    "user_books_read = read.groupby('user_id')['book_id'].apply(list).reset_index()\n",
    "user_books_read.set_index('user_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 533907/533907 [01:34<00:00, 5666.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Most commonly read genres using filtered_genres\n",
    "user_genres = read.merge(books[['book_id', 'filtered_genres']], on='book_id', how='left')\n",
    "def most_common_genres(books_read, n=3):\n",
    "    genres = books_read['filtered_genres'].value_counts().index.tolist()[:n]\n",
    "    return genres\n",
    "\n",
    "user_most_common_genres = user_genres.groupby('user_id').progress_apply(most_common_genres).reset_index()\n",
    "user_most_common_genres.columns = ['user_id', 'most_common_genres']\n",
    "user_most_common_genres.set_index('user_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine user features\n",
    "user_features = pd.concat([user_review_embeddings, user_books_read, user_most_common_genres], axis=1)\n",
    "user_features = user_features.dropna(subset=['embeddings', 'book_id', 'most_common_genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features into a single vector\n",
    "mlb = MultiLabelBinarizer()\n",
    "user_genre_features = pd.DataFrame(mlb.fit_transform(user_features['most_common_genres']), index=user_features.index)\n",
    "combined_features = user_features.apply(lambda row: np.concatenate([row['embeddings'], user_genre_features.loc[row.name]]), axis=1)\n",
    "user_features['combined'] = combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Jaccard similarity between sets of books read\n",
    "def jaccard_similarity(list1, list2):\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Create a matrix to store Jaccard similarities\n",
    "num_users = len(user_features)\n",
    "jaccard_similarity_matrix = np.zeros((num_users, num_users))\n",
    "\n",
    "for i, user1 in enumerate(user_features.index):\n",
    "    for j, user2 in enumerate(user_features.index):\n",
    "        if i <= j:  # Calculate only once for each pair\n",
    "            jaccard_sim = jaccard_similarity(user_features.at[user1, 'book_id'], user_features.at[user2, 'book_id'])\n",
    "            jaccard_similarity_matrix[i, j] = jaccard_similarity_matrix[j, i] = jaccard_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate user similarity using the combined features\n",
    "cosine_sim = cosine_similarity(np.vstack(user_features['combined']))\n",
    "combined_similarity = (cosine_sim + jaccard_similarity_matrix) / 2\n",
    "user_ids = user_features.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books(user_id, user_similarity, user_ids, books, train_read, num_recommendations=20, include_read=False):\n",
    "    if user_id not in user_ids:\n",
    "        print(f\"User {user_id} not found in user ids.\")\n",
    "        return pd.DataFrame(columns=['book_id', 'title'])\n",
    "    \n",
    "    try:\n",
    "        # Find the index of the user_id in user_ids list without np.where\n",
    "        user_index = user_ids.index(user_id)\n",
    "    except ValueError:\n",
    "        print(f\"User ID {user_id} not found in user IDs list.\")\n",
    "        return pd.DataFrame(columns=['book_id', 'title'])\n",
    "    \n",
    "    similar_user_indices = user_similarity[user_index].argsort()[-(num_recommendations + 20):-1][::-1]\n",
    "    similar_user_ids = [user_ids[i] for i in similar_user_indices]\n",
    "    valid_similar_user_ids = [uid for uid in similar_user_ids if uid in train_read['user_id'].unique()]\n",
    "\n",
    "    if len(valid_similar_user_ids) == 0:\n",
    "        print(f\"No valid similar users found for user {user_id}.\")\n",
    "        return pd.DataFrame(columns=['book_id', 'title'])\n",
    "\n",
    "    similar_users_books = train_read[train_read['user_id'].isin(valid_similar_user_ids) & (train_read['is_read'] == 1)]['book_id'].unique()\n",
    "\n",
    "    if not include_read:\n",
    "        user_books = train_read[(train_read['user_id'] == user_id) & (train_read['is_read'] == 1)]['book_id'].unique()\n",
    "        recommended_books = [book_id for book_id in similar_users_books if book_id not in user_books]\n",
    "    else:\n",
    "        recommended_books = similar_users_books\n",
    "\n",
    "    if len(recommended_books) == 0:\n",
    "        print(f\"No new books to recommend for user {user_id}.\")\n",
    "        return pd.DataFrame(columns=['book_id', 'title'])\n",
    "\n",
    "    recommended_books_df = books[books['book_id'].isin(recommended_books)].head(num_recommendations)\n",
    "    return recommended_books_df[['book_id', 'title']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(test_users, user_similarity, user_ids, books, test_read, k=5):\n",
    "    def precision_at_k(y_true, y_pred, k):\n",
    "        y_true_set = set(y_true)\n",
    "        y_pred_k = y_pred[:k]\n",
    "        return len(set(y_pred_k) & y_true_set) / k\n",
    "\n",
    "    def recall_at_k(y_true, y_pred, k):\n",
    "        y_true_set = set(y_true)\n",
    "        y_pred_k = y_pred[:k]\n",
    "        return len(set(y_pred_k) & y_true_set) / len(y_true_set)\n",
    "\n",
    "    def ndcg_at_k(y_true, y_pred, k):\n",
    "        def dcg(relevance_scores):\n",
    "            return sum((2**rel - 1) / np.log2(idx + 2) for idx, rel in enumerate(relevance_scores))\n",
    "\n",
    "        y_true_set = set(y_true)\n",
    "        y_pred_k = y_pred[:k]\n",
    "        relevance_scores = [1 if item in y_true_set else 0 for item in y_pred_k]\n",
    "        ideal_relevance_scores = [1] * min(len(y_true), k) + [0] * (k - min(len(y_true), k))\n",
    "        return dcg(relevance_scores) / dcg(ideal_relevance_scores)\n",
    "\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for user_id in tqdm(test_users):\n",
    "        if user_id not in user_ids:\n",
    "            continue\n",
    "        \n",
    "        # Actual books read by the user, filtered to common user IDs\n",
    "        actual_books = set(read[(read['user_id'] == user_id) & (read['is_read'] == 1)]['book_id'])        \n",
    "\n",
    "        # Get top K recommended books\n",
    "        recommended_books = recommend_books(user_id, user_similarity, user_ids, books, test_read, num_recommendations=k, include_read=True)['book_id'].tolist()\n",
    "\n",
    "        if len(recommended_books) == 0:\n",
    "            continue\n",
    "\n",
    "        precision_scores.append(precision_at_k(actual_books, recommended_books, k))\n",
    "        recall_scores.append(recall_at_k(actual_books, recommended_books, k))\n",
    "        ndcg_scores.append(ndcg_at_k(actual_books, recommended_books, k))\n",
    "\n",
    "    precision_avg = np.mean(precision_scores)\n",
    "    recall_avg = np.mean(recall_scores)\n",
    "    ndcg_avg = np.mean(ndcg_scores)\n",
    "\n",
    "    return precision_avg, recall_avg, ndcg_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10605/10605 [08:56<00:00, 19.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@K: 0.1308, Recall@K: 0.2739, NDCG@K: 0.2314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage for evaluation:\n",
    "precision, recall, ndcg = evaluate_model(test_users, combined_similarity, user_ids, books, test_read, k=10)\n",
    "print(f\"Precision@K: {precision:.4f}, Recall@K: {recall:.4f}, NDCG@K: {ndcg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
