{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, Trainset, AlgoBase, accuracy\n",
    "from tqdm import tqdm\n",
    "from surprise import AlgoBase, Trainset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_pickle('Pickle/interactions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions[['user_id', 'book_id', 'rating']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(interactions, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the global mean rating\n",
    "global_mean = train_df['rating'].mean()\n",
    "\n",
    "# Calculate user bias with regularization\n",
    "lambda_reg = 10\n",
    "user_sum_ratings = train_df.groupby('user_id')['rating'].sum()\n",
    "user_count_ratings = train_df.groupby('user_id')['rating'].count()\n",
    "user_bias = (user_sum_ratings - user_count_ratings * global_mean) / (user_count_ratings + lambda_reg)\n",
    "\n",
    "# Map user bias back to the original dataframe\n",
    "train_df['user_bias'] = train_df['user_id'].map(user_bias)\n",
    "# Calculate item bias with regularization\n",
    "item_sum_ratings = train_df.groupby('book_id')['rating'].sum()\n",
    "item_count_ratings = train_df.groupby('book_id')['rating'].count()\n",
    "item_bias = (item_sum_ratings - item_count_ratings * global_mean) / (item_count_ratings + lambda_reg)\n",
    "\n",
    "# Map item bias back to the original dataframe\n",
    "train_df['item_bias'] = train_df['book_id'].map(item_bias)\n",
    "# Normalize ratings\n",
    "train_df['normalised_rating'] = train_df['rating'] - train_df['user_bias'] - train_df['item_bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert standardized train_df to surprise dataset\n",
    "reader = Reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.load_from_df(train_df[['user_id', 'book_id', 'normalised_rating']], reader)\n",
    "# Convert test_df to surprise dataset without normalization\n",
    "test_data = Dataset.load_from_df(test_df[['user_id', 'book_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full trainset and testset\n",
    "trainset = train_data.build_full_trainset()\n",
    "testset = test_data.construct_testset([(uid, iid, r, {}) for uid, iid, r in test_df[['user_id', 'book_id', 'rating']].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomizedSVD(AlgoBase):\n",
    "    def __init__(self, n_factors=100, n_iter=5, random_state=None, power_iteration_normalizer='QR'):\n",
    "        super().__init__()\n",
    "        self.n_factors = n_factors\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.power_iteration_normalizer = power_iteration_normalizer\n",
    "\n",
    "    def fit(self, trainset: Trainset):\n",
    "        super().fit(trainset)\n",
    "\n",
    "        num_users = trainset.n_users\n",
    "        num_items = trainset.n_items\n",
    "        user_item_matrix = np.zeros((num_users, num_items))\n",
    "\n",
    "        for user_id, iid, rating in trainset.all_ratings():\n",
    "            user_item_matrix[int(user_id), int(iid)] = rating\n",
    "\n",
    "        # Apply SVD directly to the user-item matrix\n",
    "        U, Sigma, VT = randomized_svd(user_item_matrix, \n",
    "                                      n_components=self.n_factors, \n",
    "                                      n_iter=self.n_iter, \n",
    "                                      random_state=self.random_state,\n",
    "                                      power_iteration_normalizer=self.power_iteration_normalizer)\n",
    "\n",
    "        self.user_factors = U.dot(np.diag(Sigma))\n",
    "        self.item_factors = VT.T\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        if self.trainset.knows_user(u) and self.trainset.knows_item(i):\n",
    "            return np.dot(self.user_factors[u, :], self.item_factors[i, :])\n",
    "        else:\n",
    "            return self.trainset.global_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parameter grid\n",
    "param_grid = { \n",
    "    'n_factors': [80], \n",
    "    'n_iter': [19], \n",
    "    'random_state': [42], \n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gs = GridSearchCV(RandomizedSVD, param_grid, measures=['rmse'], cv=3)\n",
    "gs.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = gs.best_params['rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_factors': 80, 'n_iter': 19, 'random_state': 42}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RandomizedSVD at 0x21109fb45f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_randomized_svd = RandomizedSVD(**best_params)\n",
    "best_randomized_svd.fit(train_data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_randomized_svd.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse bias terms\n",
    "def reverse_bias_terms(uid, iid, est, user_bias, item_bias, global_mean):\n",
    "    user_b = user_bias.get(uid, 0)  # Default to 0 if the user/item is not in the training data\n",
    "    item_b = item_bias.get(iid, 0)\n",
    "    unbiased_prediction = est - user_b - item_b + global_mean\n",
    "    return unbiased_prediction\n",
    "\n",
    "# Rescale predictions by reversing bias terms\n",
    "def unbiased_predictions(predictions, user_bias, item_bias, global_mean):\n",
    "    adjusted_predictions = []\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        # Calculate the unbiased prediction\n",
    "        unbiased_prediction = reverse_bias_terms(uid, iid, est, user_bias, item_bias, global_mean)\n",
    "        # Clip the rating to the original scale (e.g., 1 to 5)\n",
    "        unbiased_prediction = min(5, max(1, unbiased_prediction))\n",
    "        adjusted_predictions.append((uid, iid, true_r, unbiased_prediction, _))\n",
    "    return adjusted_predictions\n",
    "\n",
    "# Rescale predictions\n",
    "adjusted_predictions = unbiased_predictions(predictions, user_bias.to_dict(), item_bias.to_dict(), train_df['rating'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.7743\n",
      "MAE:  1.2896\n"
     ]
    }
   ],
   "source": [
    "rmse = accuracy.rmse(adjusted_predictions)\n",
    "mae = accuracy.mae(adjusted_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_ndcg_at_k(predictions, k=10, threshold=4.0):\n",
    "    \"\"\"Return precision, recall, and nDCG at k metrics for each user.\"\"\"\n",
    "    \n",
    "    # Helper function to calculate DCG and nDCG\n",
    "    def dcg_at_k(scores, k):\n",
    "        return sum([rel / log2(idx + 2) for idx, rel in enumerate(scores[:k])])\n",
    "\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    ndcgs = dict()\n",
    "    \n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        \n",
    "        # Precision@K\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        \n",
    "        # Recall@K\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "        \n",
    "        # nDCG@K\n",
    "        actual = [true_r for (_, true_r) in user_ratings]\n",
    "        ideal = sorted(actual, reverse=True)\n",
    "        \n",
    "        idcg = dcg_at_k(ideal, k)\n",
    "        dcg = dcg_at_k([rel for (est, rel) in user_ratings], k)\n",
    "        \n",
    "        ndcgs[uid] = dcg / idcg if idcg > 0 else 0\n",
    "    \n",
    "    precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "    recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "    ndcg = sum(ndcg for ndcg in ndcgs.values()) / len(ndcgs)\n",
    "    \n",
    "    return precision, recall, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Precision: 0.633800543662929, Adjusted Recall: 0.3563289427633184, Adjusted nDCG: 0.7594018263183041\n"
     ]
    }
   ],
   "source": [
    "precision, recall, ndcg = precision_recall_ndcg_at_k(adjusted_predictions, k=10, threshold=4.0)\n",
    "print(f'Adjusted Precision: {precision}, Adjusted Recall: {recall}, Adjusted nDCG: {ndcg}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
