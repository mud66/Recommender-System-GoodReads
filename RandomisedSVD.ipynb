{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, Trainset, AlgoBase, accuracy\n",
    "from tqdm import tqdm\n",
    "from surprise import AlgoBase, Trainset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from custom_svd import RandomizedSVD\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_pickle('Pickle/interactions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions[['user_id', 'book_id', 'rating', 'is_read']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "\n",
    "# # Separate lower ratings by their respective values\n",
    "# ratings_0 = interactions[interactions['rating'] == 0]\n",
    "# ratings_1 = interactions[interactions['rating'] == 1]\n",
    "# ratings_2 = interactions[interactions['rating'] == 2]\n",
    "# higher_ratings = interactions[interactions['rating'] > 3]\n",
    "\n",
    "# # Calculate the number of samples needed for each lower rating\n",
    "# higher_count = len(higher_ratings)\n",
    "\n",
    "# # Define sampling factors\n",
    "# factor_0 = 0.2\n",
    "# factor_1 = 0.2\n",
    "# factor_2 = 0.2\n",
    "\n",
    "# # Ensure n_samples are within valid limits\n",
    "# n_samples_0 = min(int(higher_count * factor_0), len(ratings_0))\n",
    "# n_samples_1 = min(int(higher_count * factor_1), len(ratings_1))\n",
    "# n_samples_2 = min(int(higher_count * factor_2), len(ratings_2))\n",
    "\n",
    "# # Apply oversampling\n",
    "# ratings_0_oversampled = resample(ratings_0, replace=True, n_samples=n_samples_0, random_state=42)\n",
    "# ratings_1_oversampled = resample(ratings_1, replace=True, n_samples=n_samples_1, random_state=42)\n",
    "# ratings_2_oversampled = resample(ratings_2, replace=True, n_samples=n_samples_2, random_state=42)\n",
    "\n",
    "# # Combine the datasets\n",
    "# interactions = pd.concat([higher_ratings, ratings_0_oversampled, ratings_1_oversampled, ratings_2_oversampled])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(interactions, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the global mean rating\n",
    "global_mean = train_df['rating'].mean()\n",
    "\n",
    "# Calculate user bias with regularization\n",
    "lambda_reg = 10\n",
    "user_sum_ratings = train_df.groupby('user_id')['rating'].sum()\n",
    "user_count_ratings = train_df.groupby('user_id')['rating'].count()\n",
    "user_bias = (user_sum_ratings - user_count_ratings * global_mean) / (user_count_ratings + lambda_reg)\n",
    "\n",
    "# Map user bias back to the original dataframe\n",
    "train_df['user_bias'] = train_df['user_id'].map(user_bias)\n",
    "# Calculate item bias with regularization\n",
    "item_sum_ratings = train_df.groupby('book_id')['rating'].sum()\n",
    "item_count_ratings = train_df.groupby('book_id')['rating'].count()\n",
    "item_bias = (item_sum_ratings - item_count_ratings * global_mean) / (item_count_ratings + lambda_reg)\n",
    "\n",
    "# Map item bias back to the original dataframe\n",
    "train_df['item_bias'] = train_df['book_id'].map(item_bias)\n",
    "# Normalize ratings\n",
    "train_df['normalised_rating'] = train_df['rating'] - train_df['user_bias'] - train_df['item_bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert standardized train_df to surprise dataset\n",
    "reader = Reader(rating_scale=(train_df['rating'].min(), train_df['rating'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.load_from_df(train_df[['user_id', 'book_id', 'normalised_rating']], reader)\n",
    "# Convert test_df to surprise dataset without normalization\n",
    "test_data = Dataset.load_from_df(test_df[['user_id', 'book_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full trainset and testset\n",
    "trainset = train_data.build_full_trainset()\n",
    "testset = test_data.construct_testset([(uid, iid, r, {}) for uid, iid, r in test_df[['user_id', 'book_id', 'rating']].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parameter grid\n",
    "param_grid = { \n",
    "    'n_factors':[60], \n",
    "    'n_iter': [18], \n",
    "    'random_state': [42], \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(RandomizedSVD, param_grid, measures=['rmse'], cv=2)\n",
    "gs.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = gs.best_params['rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_factors': 60, 'n_iter': 18, 'random_state': 42}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<custom_svd.RandomizedSVD at 0x1c65bf6e360>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_randomized_svd = RandomizedSVD(**best_params)\n",
    "best_randomized_svd.fit(train_data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_randomized_svd.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse bias terms\n",
    "def reverse_bias_terms(uid, iid, est, user_bias, item_bias, global_mean):\n",
    "    user_b = user_bias.get(uid, 0)  # Default to 0 if the user/item is not in the training data\n",
    "    item_b = item_bias.get(iid, 0)\n",
    "    unbiased_prediction = est - user_b - item_b + global_mean\n",
    "    return unbiased_prediction\n",
    "\n",
    "# Rescale predictions by reversing bias terms\n",
    "def unbiased_predictions(predictions, user_bias, item_bias, global_mean):\n",
    "    adjusted_predictions = []\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        # Calculate the unbiased prediction\n",
    "        unbiased_prediction = reverse_bias_terms(uid, iid, est, user_bias, item_bias, global_mean)\n",
    "        # Clip the rating to the original scale (e.g., 1 to 5)\n",
    "        unbiased_prediction = min(5, max(1, unbiased_prediction))\n",
    "        adjusted_predictions.append((uid, iid, true_r, unbiased_prediction, _))\n",
    "    return adjusted_predictions\n",
    "\n",
    "# Rescale predictions\n",
    "adjusted_predictions = unbiased_predictions(predictions, user_bias.to_dict(), item_bias.to_dict(), train_df['rating'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_ndcg_at_k(predictions, k, threshold):\n",
    "    \"\"\"Return precision, recall, and nDCG at k metrics for each user.\"\"\"\n",
    "    \n",
    "    # Helper function to calculate DCG and nDCG\n",
    "    def dcg_at_k(scores, k):\n",
    "        return sum([rel / log2(idx + 2) for idx, rel in enumerate(scores[:k])])\n",
    "\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    ndcgs = dict()\n",
    "    \n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        \n",
    "        # Precision@K\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        \n",
    "        # Recall@K\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "        \n",
    "        # nDCG@K\n",
    "        actual = [true_r for (_, true_r) in user_ratings]\n",
    "        ideal = sorted(actual, reverse=True)\n",
    "        \n",
    "        idcg = dcg_at_k(ideal, k)\n",
    "        dcg = dcg_at_k([rel for (est, rel) in user_ratings], k)\n",
    "        \n",
    "        ndcgs[uid] = dcg / idcg if idcg > 0 else 0\n",
    "    \n",
    "    precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "    recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "    ndcg = sum(ndcg for ndcg in ndcgs.values()) / len(ndcgs)\n",
    "    \n",
    "    return precision, recall, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Precision: 0.9041972072553173, Adjusted Recall: 0.38721653211444157, Adjusted nDCG: 0.7693620295201644\n"
     ]
    }
   ],
   "source": [
    "precision, recall, ndcg = precision_recall_ndcg_at_k(adjusted_predictions, k=10, threshold=2)\n",
    "print(f'Adjusted Precision: {precision}, Adjusted Recall: {recall}, Adjusted nDCG: {ndcg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_filename = 'Pickle/svd_model.pkl'\n",
    "with open(model_filename, 'wb') as model_file:\n",
    "    pickle.dump(best_randomized_svd, model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases_filename = 'Pickle/biases.pkl'\n",
    "\n",
    "# Save biases\n",
    "biases = {\n",
    "    'user_bias': user_bias,\n",
    "    'item_bias': item_bias,\n",
    "    'global_mean': global_mean\n",
    "}\n",
    "\n",
    "with open(biases_filename, 'wb') as biases_file:\n",
    "    pickle.dump(biases, biases_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
