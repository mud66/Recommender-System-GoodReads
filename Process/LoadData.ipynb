{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "DIR = '../Data'\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'goodreads_interactions.csv')\n",
    "read = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'book_id_map.csv')\n",
    "book_map = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'goodreads_book_authors.json.gz')\n",
    "authors = pd.read_json(file_path, compression='gzip', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'user_id_map.csv')\n",
    "user_map = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'goodreads_books.json.gz')\n",
    "\n",
    "chunk_size = 1000\n",
    "num_chunks = 1000\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size, compression='gzip')\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for i, chunk in tqdm(enumerate(chunks), total=num_chunks):\n",
    "    for _, row in chunk.iterrows():\n",
    "        df_list.append(row)\n",
    "    if i >= num_chunks:\n",
    "        break\n",
    "\n",
    "books = pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'goodreads_book_genres_initial.json.gz')\n",
    "\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size, compression='gzip')\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for i, chunk in tqdm(enumerate(chunks), total=num_chunks):\n",
    "    for _, row in chunk.iterrows():\n",
    "        df_list.append(row)\n",
    "    if i >= num_chunks:\n",
    "        break\n",
    "\n",
    "genres = pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'goodreads_reviews_dedup.json.gz')\n",
    "\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size, compression='gzip')\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for i, chunk in tqdm(enumerate(chunks), total=num_chunks):\n",
    "    for _, row in chunk.iterrows():\n",
    "        df_list.append(row)\n",
    "    if i >= num_chunks:\n",
    "        break\n",
    "\n",
    "reviews = pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'goodreads_interactions_dedup.json.gz')\n",
    "\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size, compression='gzip')\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for i, chunk in tqdm(enumerate(chunks), total=num_chunks):\n",
    "    for _, row in chunk.iterrows():\n",
    "        df_list.append(row)\n",
    "    if i >= num_chunks:\n",
    "        break\n",
    "\n",
    "interactions = pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_rows(df, column_name):\n",
    "   \n",
    "    df_cleaned = df.dropna(subset=[column_name])\n",
    "    return df_cleaned\n",
    "\n",
    "books = drop_empty_rows(books, 'description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only keep needed rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions[['user_id', 'book_id', 'review_id', 'is_read', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews[['user_id', 'book_id', 'review_id', 'rating', 'review_text', 'n_votes', 'n_comments']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine list of genre names with the book description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = genres.sort_values(by='book_id')\n",
    "books = books.sort_values(by='book_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter genres\n",
    "def filter_genres(genre_dict): return [genre for genre, value in genre_dict.items() if value is not None]\n",
    "\n",
    "# Apply the function to the genres column\n",
    "genres['filtered_genres'] = genres['genres'].progress_apply(filter_genres)\n",
    "\n",
    "# Convert the list of filtered genres to a comma-separated string\n",
    "genres['filtered_genres'] = genres['filtered_genres'].progress_apply(lambda x: ', '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.merge(books, genres[['book_id', 'filtered_genres']], on='book_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books.dropna(subset = ['filtered_genres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing popular genres column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_shelves = ['to-read', 'read', 'currently-reading', 'default', 'owned', 'unread', 'my-library']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "def expand_popular_shelves(shelves_list):\n",
    "    expanded_shelves = []\n",
    "    for shelf in shelves_list:\n",
    "        count = int(shelf['count'])\n",
    "        name = shelf['name']\n",
    "        if name not in exclude_shelves:\n",
    "            expanded_shelves.extend([name] * count)\n",
    "    return ' '.join(expanded_shelves)\n",
    "\n",
    "books['expanded_shelves'] = books['popular_shelves'].progress_apply(expand_popular_shelves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions[interactions['is_read'] != False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge dataframes to get consistent user and book ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.merge(interactions, user_map, on='user_id', how='left')\n",
    "interactions = pd.merge(interactions, book_map, on='book_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions.drop(columns=['user_id', 'book_id'], inplace=True)\n",
    "interactions.rename(columns={'user_id_csv': 'user_id', 'book_id_csv': 'book_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.merge(reviews, user_map, on='user_id', how='left')\n",
    "reviews = pd.merge(reviews, book_map, on='book_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.drop(columns=['user_id', 'book_id'], inplace=True)\n",
    "reviews.rename(columns={'user_id_csv': 'user_id', 'book_id_csv': 'book_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books[['language_code', 'description', 'authors', 'book_id', 'title', 'expanded_shelves', 'average_rating', 'title_without_series', 'filtered_genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map authors and author ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.loc[:, 'authors'] = books['authors'].progress_apply(lambda x: [author for author in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_name_mapping = authors.set_index('author_id')['name'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['authors'] = books['authors'].progress_apply(\n",
    "    lambda x: [author_name_mapping[int(author_id['author_id'])] if isinstance(author_id, dict) else author_name_mapping[int(author_id)] for author_id in x]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read.to_pickle('../Pickle/read.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions.to_pickle('../Pickle/interactions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/reviews.pkl', 'wb') as file: \n",
    "    pickle.dump(reviews, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure the progress_apply method from tqdm is used\n",
    "tqdm.pandas()\n",
    "\n",
    "# Split the DataFrame into chunks and save each chunk to a pickle file with a progress bar\n",
    "chunk_size = 10000  # Adjust the chunk size as needed\n",
    "num_chunks = len(books) // chunk_size + 1\n",
    "\n",
    "# Initialize the progress bar for rows\n",
    "progress_bar = tqdm(total=len(books))\n",
    "\n",
    "# Open the pickle file once before the loop\n",
    "with open('../Pickle/books.pkl', 'wb') as file:\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size\n",
    "        chunk = books.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Update progress bar for each row\n",
    "        for _, row in chunk.iterrows():\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # Append each chunk to the pickle file\n",
    "        if i == 0:\n",
    "            # For the first chunk, use \"wb\" (write binary) mode\n",
    "            pickle.dump(chunk, file)\n",
    "        else:\n",
    "            # For subsequent chunks, use \"ab\" (append binary) mode\n",
    "            pickle.dump(chunk, file)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
