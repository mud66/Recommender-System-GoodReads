{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "DIR = 'Data'\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'goodreads_interactions.csv')\n",
    "read = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'book_id_map.csv')\n",
    "book_map = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'goodreads_book_authors.json.gz')\n",
    "authors = pd.read_json(file_path, compression='gzip', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'user_id_map.csv')\n",
    "user_map = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'goodreads_books.json.gz')\n",
    "\n",
    "chunk_size = 1000\n",
    "num_chunks = 1000\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size, compression='gzip')\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if i >= num_chunks:\n",
    "        break\n",
    "    df_list.append(chunk)\n",
    "\n",
    "books = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'goodreads_book_genres_initial.json.gz')\n",
    "\n",
    "chunk_size = 1000\n",
    "num_chunks = 1000\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size, compression='gzip')\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if i >= num_chunks:\n",
    "        break\n",
    "    df_list.append(chunk)\n",
    "\n",
    "genres = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'goodreads_reviews_dedup.json.gz')\n",
    "\n",
    "chunk_size = 1000\n",
    "num_chunks = 1000\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size, compression='gzip')\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if i >= num_chunks:\n",
    "        break\n",
    "    df_list.append(chunk)\n",
    "\n",
    "reviews = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DIR, 'goodreads_interactions_dedup.json.gz')\n",
    "\n",
    "chunk_size = 1000\n",
    "num_chunks = 1000\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size, compression='gzip')\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if i >= num_chunks:\n",
    "        break\n",
    "    df_list.append(chunk)\n",
    "\n",
    "interactions = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_rows(df, column_name):\n",
    "   \n",
    "    df_cleaned = df.dropna(subset=[column_name])\n",
    "    return df_cleaned\n",
    "\n",
    "books = drop_empty_rows(books, 'description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only keep needed rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions[['user_id', 'book_id', 'review_id', 'is_read', 'rating']]\n",
    "books = books[['language_code', 'description', 'authors', 'book_id', 'title', 'similar_books', 'image_url', 'url', 'popular_shelves', 'average_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews[['user_id', 'book_id', 'review_id', 'rating', 'review_text', 'n_votes', 'n_comments']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine list of genre names with the book description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = genres.sort_values(by='book_id')\n",
    "books = books.sort_values(by='book_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [01:02<00:00, 16049.21it/s]\n",
      "100%|██████████| 1000000/1000000 [00:08<00:00, 122007.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to filter genres\n",
    "def filter_genres(genre_dict): return [genre for genre, value in genre_dict.items() if value is not None]\n",
    "\n",
    "# Apply the function to the genres column\n",
    "genres['filtered_genres'] = genres['genres'].progress_apply(filter_genres)\n",
    "\n",
    "# Convert the list of filtered genres to a comma-separated string\n",
    "genres['filtered_genres'] = genres['filtered_genres'].progress_apply(lambda x: ', '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.merge(books, genres[['book_id', 'filtered_genres']], on='book_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing popular genres column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_shelves = ['to-read', 'read', 'currently-reading', 'default', 'owned', 'unread', 'my-library']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [18:23<00:00, 905.94it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "def expand_popular_shelves(shelves_list):\n",
    "    expanded_shelves = []\n",
    "    for shelf in shelves_list:\n",
    "        count = int(shelf['count'])\n",
    "        name = shelf['name']\n",
    "        if name not in exclude_shelves:\n",
    "            expanded_shelves.extend([name] * count)\n",
    "    return ' '.join(expanded_shelves)\n",
    "\n",
    "books['expanded_shelves'] = books['popular_shelves'].progress_apply(expand_popular_shelves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions[interactions['is_read'] != False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge dataframes to get consistent user and book ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.merge(interactions, user_map, on='user_id', how='left')\n",
    "interactions = pd.merge(interactions, book_map, on='book_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions.drop(columns=['user_id', 'book_id'], inplace=True)\n",
    "interactions.rename(columns={'user_id_csv': 'user_id', 'book_id_csv': 'book_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.merge(reviews, user_map, on='user_id', how='left')\n",
    "reviews = pd.merge(reviews, book_map, on='book_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.drop(columns=['user_id', 'book_id'], inplace=True)\n",
    "reviews.rename(columns={'user_id_csv': 'user_id', 'book_id_csv': 'book_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map authors and author ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [08:08<00:00, 2047.62it/s]\n"
     ]
    }
   ],
   "source": [
    "books['authors'] = books['authors'].progress_apply(lambda x: [d['author_id'] for d in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_name_mapping = authors.set_index('author_id')['name'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [04:19<00:00, 3859.22it/s]\n"
     ]
    }
   ],
   "source": [
    "books['authors'] = books['authors'].progress_apply(lambda x: [author_name_mapping[int(author_id)] for author_id in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "read.to_pickle('Pickle/read.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions.to_pickle('Pickle/interactions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language_code\n",
       "         449587\n",
       "eng      299752\n",
       "en-US     38781\n",
       "en-GB     24779\n",
       "spa       23188\n",
       "          ...  \n",
       "ast           1\n",
       "und           1\n",
       "crh           1\n",
       "chb           1\n",
       "kok           1\n",
       "Name: count, Length: 176, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books['language_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books[books['language_code'].isin(['en-US', 'en-GB', 'eng'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books[['description', 'authors', 'book_id', 'title', 'url', 'average_rating', 'expanded_shelves', 'filtered_genres']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Pickle/books.pkl', 'wb') as file: \n",
    "    pickle.dump(books, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Pickle/reviews.pkl', 'wb') as file: \n",
    "    pickle.dump(reviews, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
